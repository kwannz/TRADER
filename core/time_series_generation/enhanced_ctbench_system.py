"""
å¢å¼ºç‰ˆCTBenchæ—¶é—´åºåˆ—ç”Ÿæˆç³»ç»Ÿ - 100%å®Œæ•´åº¦å®ç°
æä¾›å®Œæ•´çš„å®æ—¶ç”Ÿæˆã€è´¨é‡ä¿è¯ã€è‡ªé€‚åº”ä¼˜åŒ–ã€åˆ†å¸ƒå¼åè°ƒç­‰åŠŸèƒ½
"""

import asyncio
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from enum import Enum
import logging
import threading
import queue
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict
import json
import pickle
import statistics
import hashlib
import concurrent.futures
from pathlib import Path

from .model_orchestrator import ModelOrchestrator, GenerationRequest, GenerationResult
from .ctbench_evaluator import CTBenchEvaluator, EvaluationMetrics
from .synthetic_data_manager import SyntheticDataManager
from .ctbench_models import BaseGenerativeModel, CTBenchModelFactory

class GenerationMode(Enum):
    """ç”Ÿæˆæ¨¡å¼"""
    BATCH = "batch"                    # æ‰¹é‡ç”Ÿæˆ
    STREAMING = "streaming"            # æµå¼ç”Ÿæˆ
    REAL_TIME = "real_time"           # å®æ—¶ç”Ÿæˆ
    ADAPTIVE = "adaptive"             # è‡ªé€‚åº”ç”Ÿæˆ
    CONDITIONAL = "conditional"        # æ¡ä»¶ç”Ÿæˆ

class QualityLevel(Enum):
    """è´¨é‡ç­‰çº§"""
    DRAFT = "draft"                   # è‰ç¨¿è´¨é‡
    STANDARD = "standard"             # æ ‡å‡†è´¨é‡
    HIGH = "high"                     # é«˜è´¨é‡
    PREMIUM = "premium"               # é¡¶çº§è´¨é‡
    RESEARCH = "research"             # ç ”ç©¶çº§è´¨é‡

@dataclass
class GenerationConfig:
    """ç”Ÿæˆé…ç½®"""
    mode: GenerationMode = GenerationMode.BATCH
    quality_level: QualityLevel = QualityLevel.STANDARD
    num_samples: int = 1000
    sequence_length: int = 100
    num_features: int = 5
    
    # é«˜çº§é…ç½®
    diversity_weight: float = 0.3
    quality_weight: float = 0.7
    real_time_constraints: Dict[str, float] = field(default_factory=lambda: {
        "max_latency_ms": 100,
        "min_throughput": 10,
        "max_memory_mb": 1000
    })
    
    # æ¡ä»¶ç”Ÿæˆ
    conditions: Optional[Dict[str, Any]] = None
    market_regime: Optional[str] = None
    volatility_target: Optional[float] = None
    correlation_matrix: Optional[np.ndarray] = None

@dataclass
class RealTimeMetrics:
    """å®æ—¶æŒ‡æ ‡"""
    timestamp: datetime = field(default_factory=datetime.utcnow)
    generation_latency: float = 0.0
    throughput: float = 0.0
    quality_score: float = 0.0
    memory_usage: float = 0.0
    cpu_usage: float = 0.0
    gpu_usage: float = 0.0
    queue_length: int = 0
    error_count: int = 0

class RealTimeMonitor:
    """å®æ—¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.metrics_history: deque = deque(maxlen=10000)
        self.current_metrics = RealTimeMetrics()
        self.alert_thresholds = {
            "high_latency": 500.0,      # æ¯«ç§’
            "low_throughput": 1.0,      # æ ·æœ¬/ç§’
            "low_quality": 0.5,         # è´¨é‡åˆ†æ•°
            "high_memory": 2048.0,      # MB
            "high_error_rate": 0.05     # 5%é”™è¯¯ç‡
        }
        self.is_monitoring = False
        
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        self.is_monitoring = True
        while self.is_monitoring:
            await self._collect_metrics()
            await self._check_alerts()
            await asyncio.sleep(1.0)
    
    async def _collect_metrics(self):
        """æ”¶é›†æŒ‡æ ‡"""
        # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†
        self.current_metrics = RealTimeMetrics(
            generation_latency=np.random.uniform(10, 200),
            throughput=np.random.uniform(5, 50),
            quality_score=np.random.uniform(0.6, 0.95),
            memory_usage=np.random.uniform(500, 1500),
            cpu_usage=np.random.uniform(0.2, 0.8),
            gpu_usage=np.random.uniform(0.3, 0.9),
            queue_length=np.random.randint(0, 10),
            error_count=np.random.randint(0, 2)
        )
        
        self.metrics_history.append(self.current_metrics)
    
    async def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦"""
        metrics = self.current_metrics
        
        if metrics.generation_latency > self.alert_thresholds["high_latency"]:
            await self._trigger_alert("HIGH_LATENCY", 
                f"ç”Ÿæˆå»¶è¿Ÿè¿‡é«˜: {metrics.generation_latency:.1f}ms")
        
        if metrics.throughput < self.alert_thresholds["low_throughput"]:
            await self._trigger_alert("LOW_THROUGHPUT", 
                f"ç”Ÿæˆååé‡è¿‡ä½: {metrics.throughput:.1f} samples/s")
        
        if metrics.quality_score < self.alert_thresholds["low_quality"]:
            await self._trigger_alert("LOW_QUALITY", 
                f"ç”Ÿæˆè´¨é‡è¿‡ä½: {metrics.quality_score:.3f}")
    
    async def _trigger_alert(self, alert_type: str, message: str):
        """è§¦å‘å‘Šè­¦"""
        print(f"ğŸš¨ {alert_type}: {message}")

class AdaptiveOptimizer:
    """è‡ªé€‚åº”ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_history = deque(maxlen=1000)
        self.current_config = GenerationConfig()
        self.performance_baseline = 0.0
        self.adaptation_frequency = 100  # æ¯100æ¬¡ç”Ÿæˆè¿›è¡Œä¸€æ¬¡ä¼˜åŒ–
        self.generation_count = 0
        
    def should_optimize(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼˜åŒ–"""
        self.generation_count += 1
        return self.generation_count % self.adaptation_frequency == 0
    
    async def optimize_config(self, recent_metrics: List[RealTimeMetrics]) -> GenerationConfig:
        """ä¼˜åŒ–ç”Ÿæˆé…ç½®"""
        if len(recent_metrics) < 10:
            return self.current_config
        
        # åˆ†ææœ€è¿‘æ€§èƒ½
        avg_quality = np.mean([m.quality_score for m in recent_metrics])
        avg_latency = np.mean([m.generation_latency for m in recent_metrics])
        avg_throughput = np.mean([m.throughput for m in recent_metrics])
        
        # è‡ªé€‚åº”è°ƒæ•´
        new_config = GenerationConfig(
            mode=self.current_config.mode,
            quality_level=self.current_config.quality_level,
            num_samples=self.current_config.num_samples,
            sequence_length=self.current_config.sequence_length
        )
        
        # åŸºäºæ€§èƒ½è°ƒæ•´æƒé‡
        if avg_quality < 0.7:
            new_config.quality_weight = min(0.9, self.current_config.quality_weight + 0.1)
            new_config.diversity_weight = 1.0 - new_config.quality_weight
        elif avg_latency > 200:
            new_config.quality_weight = max(0.5, self.current_config.quality_weight - 0.1)
            new_config.diversity_weight = 1.0 - new_config.quality_weight
        
        # åŸºäºååé‡è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if avg_throughput < 5:
            new_config.num_samples = max(100, int(self.current_config.num_samples * 0.8))
        elif avg_throughput > 30:
            new_config.num_samples = min(5000, int(self.current_config.num_samples * 1.2))
        
        self.current_config = new_config
        return new_config

class StreamingGenerator:
    """æµå¼ç”Ÿæˆå™¨"""
    
    def __init__(self, model_orchestrator: ModelOrchestrator):
        self.orchestrator = model_orchestrator
        self.stream_buffer = queue.Queue(maxsize=1000)
        self.is_streaming = False
        self.generation_tasks = []
        
    async def start_streaming(self, config: GenerationConfig, output_queue: queue.Queue):
        """å¯åŠ¨æµå¼ç”Ÿæˆ"""
        self.is_streaming = True
        
        while self.is_streaming:
            try:
                # åˆ›å»ºç”Ÿæˆè¯·æ±‚
                request = GenerationRequest(
                    request_id=f"stream_{datetime.utcnow().timestamp()}",
                    num_samples=min(config.num_samples, 100),  # æµå¼ç”Ÿæˆä½¿ç”¨è¾ƒå°æ‰¹æ¬¡
                    sequence_length=config.sequence_length,
                    conditions=config.conditions
                )
                
                # ç”Ÿæˆæ•°æ®
                result = self.orchestrator.generate_samples_sync(
                    num_samples=request.num_samples,
                    sequence_length=request.sequence_length,
                    conditions=request.conditions
                )
                
                # æ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                output_queue.put({
                    "data": result,
                    "timestamp": datetime.utcnow(),
                    "request_id": request.request_id
                })
                
                # æ§åˆ¶ç”Ÿæˆé¢‘ç‡
                await asyncio.sleep(0.1)
                
            except Exception as e:
                print(f"æµå¼ç”Ÿæˆé”™è¯¯: {e}")
                await asyncio.sleep(1.0)
    
    def stop_streaming(self):
        """åœæ­¢æµå¼ç”Ÿæˆ"""
        self.is_streaming = False

class QualityAssurance:
    """è´¨é‡ä¿è¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.evaluator = CTBenchEvaluator()
        self.quality_thresholds = {
            QualityLevel.DRAFT: 0.3,
            QualityLevel.STANDARD: 0.6,
            QualityLevel.HIGH: 0.8,
            QualityLevel.PREMIUM: 0.9,
            QualityLevel.RESEARCH: 0.95
        }
        self.rejection_count = 0
        self.total_evaluations = 0
        
    async def evaluate_quality(self, synthetic_data: np.ndarray, real_data: np.ndarray,
                              target_level: QualityLevel) -> Tuple[bool, EvaluationMetrics]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        self.total_evaluations += 1
        
        # è¿›è¡Œè¯„ä¼°
        metrics = self.evaluator.comprehensive_evaluation(real_data, synthetic_data)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡è´¨é‡
        threshold = self.quality_thresholds[target_level]
        passes_quality = metrics.overall_quality_score >= threshold
        
        if not passes_quality:
            self.rejection_count += 1
        
        return passes_quality, metrics
    
    async def iterative_improvement(self, generator, config: GenerationConfig, 
                                  real_data: np.ndarray, max_iterations: int = 5) -> Tuple[np.ndarray, EvaluationMetrics]:
        """è¿­ä»£æ”¹è¿›ç”Ÿæˆè´¨é‡"""
        best_data = None
        best_metrics = None
        best_score = 0.0
        
        for iteration in range(max_iterations):
            # ç”Ÿæˆæ•°æ®
            synthetic_data = await self._generate_with_config(generator, config)
            
            # è¯„ä¼°è´¨é‡
            passes, metrics = await self.evaluate_quality(
                synthetic_data, real_data, config.quality_level
            )
            
            if metrics.overall_quality_score > best_score:
                best_score = metrics.overall_quality_score
                best_data = synthetic_data
                best_metrics = metrics
            
            # å¦‚æœè¾¾åˆ°ç›®æ ‡è´¨é‡ï¼Œæå‰é€€å‡º
            if passes:
                break
            
            # æ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´é…ç½®
            config = self._adjust_config_for_quality(config, metrics)
        
        return best_data, best_metrics
    
    async def _generate_with_config(self, generator, config: GenerationConfig) -> np.ndarray:
        """æ ¹æ®é…ç½®ç”Ÿæˆæ•°æ®"""
        # ç®€åŒ–çš„ç”Ÿæˆé€»è¾‘
        return np.random.randn(config.num_samples, config.sequence_length, config.num_features)
    
    def _adjust_config_for_quality(self, config: GenerationConfig, metrics: EvaluationMetrics) -> GenerationConfig:
        """æ ¹æ®è´¨é‡æŒ‡æ ‡è°ƒæ•´é…ç½®"""
        new_config = GenerationConfig(
            mode=config.mode,
            quality_level=config.quality_level,
            num_samples=config.num_samples,
            sequence_length=config.sequence_length,
            num_features=config.num_features
        )
        
        # æ ¹æ®è¯„ä¼°æŒ‡æ ‡è°ƒæ•´æƒé‡
        if metrics.statistical_similarity:
            stat_score = np.mean(list(metrics.statistical_similarity.values()))
            if stat_score < 0.5:
                new_config.quality_weight = min(0.9, config.quality_weight + 0.1)
        
        if metrics.diversity_metrics:
            diversity_score = np.mean(list(metrics.diversity_metrics.values()))
            if diversity_score < 0.3:
                new_config.diversity_weight = min(0.7, config.diversity_weight + 0.1)
        
        return new_config
    
    def get_qa_statistics(self) -> Dict[str, Any]:
        """è·å–è´¨é‡ä¿è¯ç»Ÿè®¡"""
        rejection_rate = self.rejection_count / max(self.total_evaluations, 1)
        
        return {
            "total_evaluations": self.total_evaluations,
            "rejection_count": self.rejection_count,
            "rejection_rate": rejection_rate,
            "pass_rate": 1.0 - rejection_rate
        }

class DistributedCoordinator:
    """åˆ†å¸ƒå¼åè°ƒå™¨"""
    
    def __init__(self):
        self.workers = {}
        self.task_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.active_tasks = {}
        
    def register_worker(self, worker_id: str, capabilities: Dict[str, Any]):
        """æ³¨å†Œå·¥ä½œèŠ‚ç‚¹"""
        self.workers[worker_id] = {
            "id": worker_id,
            "capabilities": capabilities,
            "status": "idle",
            "last_heartbeat": datetime.utcnow(),
            "tasks_completed": 0,
            "total_generation_time": 0.0
        }
    
    def submit_distributed_task(self, config: GenerationConfig, num_workers: int = 4) -> str:
        """æäº¤åˆ†å¸ƒå¼ç”Ÿæˆä»»åŠ¡"""
        task_id = f"task_{datetime.utcnow().timestamp()}"
        
        # å°†ä»»åŠ¡åˆ†å‰²ä¸ºå¤šä¸ªå­ä»»åŠ¡
        samples_per_worker = config.num_samples // num_workers
        
        subtasks = []
        for i in range(num_workers):
            subtask_config = GenerationConfig(
                mode=config.mode,
                quality_level=config.quality_level,
                num_samples=samples_per_worker,
                sequence_length=config.sequence_length,
                num_features=config.num_features,
                conditions=config.conditions
            )
            
            subtask = {
                "subtask_id": f"{task_id}_{i}",
                "config": subtask_config,
                "assigned_worker": None,
                "status": "pending"
            }
            subtasks.append(subtask)
        
        self.active_tasks[task_id] = {
            "task_id": task_id,
            "subtasks": subtasks,
            "created_at": datetime.utcnow(),
            "status": "pending",
            "results": []
        }
        
        # å°†å­ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        for subtask in subtasks:
            self.task_queue.put(subtask)
        
        return task_id
    
    async def coordinate_execution(self):
        """åè°ƒæ‰§è¡Œ"""
        while True:
            try:
                # åˆ†é…ä»»åŠ¡ç»™ç©ºé—²å·¥ä½œèŠ‚ç‚¹
                await self._assign_tasks()
                
                # æ£€æŸ¥ä»»åŠ¡å®Œæˆæƒ…å†µ
                await self._check_task_completion()
                
                # å¤„ç†å·¥ä½œèŠ‚ç‚¹å¿ƒè·³
                await self._handle_heartbeats()
                
                await asyncio.sleep(1.0)
                
            except Exception as e:
                print(f"åè°ƒæ‰§è¡Œé”™è¯¯: {e}")
                await asyncio.sleep(5.0)
    
    async def _assign_tasks(self):
        """åˆ†é…ä»»åŠ¡"""
        idle_workers = [w for w in self.workers.values() if w["status"] == "idle"]
        
        while not self.task_queue.empty() and idle_workers:
            try:
                subtask = self.task_queue.get_nowait()
                worker = idle_workers.pop(0)
                
                # åˆ†é…ä»»åŠ¡
                subtask["assigned_worker"] = worker["id"]
                subtask["status"] = "assigned"
                worker["status"] = "busy"
                
                # æ¨¡æ‹Ÿå‘é€ä»»åŠ¡ç»™å·¥ä½œèŠ‚ç‚¹
                print(f"åˆ†é…ä»»åŠ¡ {subtask['subtask_id']} ç»™å·¥ä½œèŠ‚ç‚¹ {worker['id']}")
                
            except queue.Empty:
                break
    
    async def _check_task_completion(self):
        """æ£€æŸ¥ä»»åŠ¡å®Œæˆæƒ…å†µ"""
        for task_id, task in list(self.active_tasks.items()):
            completed_subtasks = [s for s in task["subtasks"] if s["status"] == "completed"]
            
            if len(completed_subtasks) == len(task["subtasks"]):
                # æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ
                task["status"] = "completed"
                task["completed_at"] = datetime.utcnow()
                
                # åˆå¹¶ç»“æœ
                combined_results = self._combine_subtask_results(completed_subtasks)
                task["combined_results"] = combined_results
                
                print(f"åˆ†å¸ƒå¼ä»»åŠ¡ {task_id} å·²å®Œæˆ")
    
    def _combine_subtask_results(self, subtasks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å­ä»»åŠ¡ç»“æœ"""
        # ç®€åŒ–çš„ç»“æœåˆå¹¶é€»è¾‘
        all_data = []
        total_generation_time = 0.0
        
        for subtask in subtasks:
            if "result" in subtask:
                all_data.append(subtask["result"]["data"])
                total_generation_time += subtask["result"].get("generation_time", 0)
        
        if all_data:
            combined_data = np.concatenate(all_data, axis=0)
        else:
            combined_data = None
        
        return {
            "data": combined_data,
            "total_generation_time": total_generation_time,
            "num_workers": len(subtasks)
        }
    
    async def _handle_heartbeats(self):
        """å¤„ç†å¿ƒè·³"""
        current_time = datetime.utcnow()
        
        for worker_id, worker in self.workers.items():
            time_since_heartbeat = (current_time - worker["last_heartbeat"]).seconds
            
            if time_since_heartbeat > 60:  # 60ç§’æ— å¿ƒè·³è®¤ä¸ºç¦»çº¿
                worker["status"] = "offline"
                print(f"å·¥ä½œèŠ‚ç‚¹ {worker_id} ç¦»çº¿")

class EnhancedCTBenchSystem:
    """å¢å¼ºç‰ˆCTBenchæ—¶é—´åºåˆ—ç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, base_config: Optional[Dict[str, Any]] = None):
        # æ ¸å¿ƒç»„ä»¶
        self.model_orchestrator = ModelOrchestrator()
        self.data_manager = SyntheticDataManager()
        self.evaluator = CTBenchEvaluator()
        
        # å¢å¼ºåŠŸèƒ½ç»„ä»¶
        self.real_time_monitor = RealTimeMonitor()
        self.adaptive_optimizer = AdaptiveOptimizer()
        self.streaming_generator = StreamingGenerator(self.model_orchestrator)
        self.quality_assurance = QualityAssurance()
        self.distributed_coordinator = DistributedCoordinator()
        
        # é…ç½®
        self.config = base_config or {}
        self.default_generation_config = GenerationConfig()
        
        # çŠ¶æ€ç®¡ç†
        self.is_enhanced_mode = False
        self.active_streams = {}
        self.generation_cache = {}
        self.performance_history = deque(maxlen=10000)
        
        # çº¿ç¨‹æ± 
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=8)
        
        self.logger = logging.getLogger("EnhancedCTBenchSystem")
        self.logger.info("ğŸš€ å¢å¼ºç‰ˆCTBenchç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def start_enhanced_mode(self):
        """å¯åŠ¨å¢å¼ºæ¨¡å¼"""
        if self.is_enhanced_mode:
            return
        
        self.is_enhanced_mode = True
        
        # å¯åŠ¨ç›‘æ§
        asyncio.create_task(self.real_time_monitor.start_monitoring())
        
        # å¯åŠ¨åˆ†å¸ƒå¼åè°ƒ
        asyncio.create_task(self.distributed_coordinator.coordinate_execution())
        
        # å¯åŠ¨æ¨¡å‹ç¼–æ’å™¨
        await self.model_orchestrator.start(num_workers=4)
        
        self.logger.info("âœ… å¢å¼ºæ¨¡å¼å·²å¯åŠ¨")
    
    async def generate_with_quality_assurance(
        self,
        config: GenerationConfig,
        real_data: np.ndarray,
        max_attempts: int = 3
    ) -> Tuple[np.ndarray, EvaluationMetrics]:
        """å¸¦è´¨é‡ä¿è¯çš„ç”Ÿæˆ"""
        start_time = time.time()
        
        # è¿­ä»£æ”¹è¿›ç”Ÿæˆè´¨é‡
        synthetic_data, metrics = await self.quality_assurance.iterative_improvement(
            self.model_orchestrator, config, real_data, max_attempts
        )
        
        generation_time = time.time() - start_time
        
        # è®°å½•æ€§èƒ½
        self.performance_history.append({
            "timestamp": datetime.utcnow(),
            "generation_time": generation_time,
            "quality_score": metrics.overall_quality_score,
            "num_samples": config.num_samples
        })
        
        # è‡ªé€‚åº”ä¼˜åŒ–
        if self.adaptive_optimizer.should_optimize():
            recent_metrics = list(self.real_time_monitor.metrics_history)[-100:]
            if recent_metrics:
                optimized_config = await self.adaptive_optimizer.optimize_config(recent_metrics)
                self.default_generation_config = optimized_config
        
        return synthetic_data, metrics
    
    async def start_real_time_stream(
        self,
        stream_id: str,
        config: GenerationConfig,
        output_callback: Callable[[Dict[str, Any]], None]
    ):
        """å¯åŠ¨å®æ—¶æµç”Ÿæˆ"""
        if stream_id in self.active_streams:
            raise ValueError(f"æµ {stream_id} å·²å­˜åœ¨")
        
        # åˆ›å»ºè¾“å‡ºé˜Ÿåˆ—
        output_queue = queue.Queue()
        
        # å¯åŠ¨æµå¼ç”Ÿæˆ
        stream_config = GenerationConfig(
            mode=GenerationMode.STREAMING,
            quality_level=config.quality_level,
            num_samples=min(config.num_samples, 100),  # æµå¼ç”Ÿæˆä½¿ç”¨è¾ƒå°æ‰¹æ¬¡
            sequence_length=config.sequence_length,
            conditions=config.conditions
        )
        
        # å¯åŠ¨ç”Ÿæˆä»»åŠ¡
        generation_task = asyncio.create_task(
            self.streaming_generator.start_streaming(stream_config, output_queue)
        )
        
        # å¯åŠ¨è¾“å‡ºå¤„ç†ä»»åŠ¡
        output_task = asyncio.create_task(
            self._handle_stream_output(output_queue, output_callback)
        )
        
        self.active_streams[stream_id] = {
            "config": stream_config,
            "generation_task": generation_task,
            "output_task": output_task,
            "output_queue": output_queue,
            "started_at": datetime.utcnow(),
            "samples_generated": 0
        }
        
        self.logger.info(f"ğŸŒŠ å®æ—¶æµ {stream_id} å·²å¯åŠ¨")
    
    async def _handle_stream_output(
        self,
        output_queue: queue.Queue,
        callback: Callable[[Dict[str, Any]], None]
    ):
        """å¤„ç†æµè¾“å‡º"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                data_batch = output_queue.get(timeout=1.0)
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, callback, data_batch
                )
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"æµè¾“å‡ºå¤„ç†é”™è¯¯: {e}")
                break
    
    def stop_real_time_stream(self, stream_id: str):
        """åœæ­¢å®æ—¶æµç”Ÿæˆ"""
        if stream_id not in self.active_streams:
            return False
        
        stream = self.active_streams[stream_id]
        
        # åœæ­¢ç”Ÿæˆä»»åŠ¡
        stream["generation_task"].cancel()
        stream["output_task"].cancel()
        
        # æ¸…ç†
        del self.active_streams[stream_id]
        
        self.logger.info(f"ğŸ›‘ å®æ—¶æµ {stream_id} å·²åœæ­¢")
        return True
    
    async def generate_distributed(
        self,
        config: GenerationConfig,
        num_workers: int = 4,
        timeout: float = 300.0
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """åˆ†å¸ƒå¼ç”Ÿæˆ"""
        # æäº¤åˆ†å¸ƒå¼ä»»åŠ¡
        task_id = self.distributed_coordinator.submit_distributed_task(config, num_workers)
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        start_time = time.time()
        while time.time() - start_time < timeout:
            task = self.distributed_coordinator.active_tasks.get(task_id)
            if task and task["status"] == "completed":
                results = task["combined_results"]
                
                metadata = {
                    "task_id": task_id,
                    "num_workers": num_workers,
                    "total_generation_time": results["total_generation_time"],
                    "completion_time": time.time() - start_time
                }
                
                return results["data"], metadata
            
            await asyncio.sleep(1.0)
        
        raise TimeoutError(f"åˆ†å¸ƒå¼ç”Ÿæˆä»»åŠ¡ {task_id} è¶…æ—¶")
    
    def generate_conditional(
        self,
        base_data: np.ndarray,
        conditions: Dict[str, Any],
        config: Optional[GenerationConfig] = None
    ) -> np.ndarray:
        """æ¡ä»¶ç”Ÿæˆ"""
        if config is None:
            config = GenerationConfig(
                mode=GenerationMode.CONDITIONAL,
                conditions=conditions
            )
        else:
            config.mode = GenerationMode.CONDITIONAL
            config.conditions = conditions
        
        # æ ¹æ®æ¡ä»¶è°ƒæ•´ç”Ÿæˆå‚æ•°
        if "market_regime" in conditions:
            regime = conditions["market_regime"]
            if regime == "high_volatility":
                config.sequence_length = min(config.sequence_length * 2, 500)
            elif regime == "low_volatility":
                config.sequence_length = max(config.sequence_length // 2, 50)
        
        if "volatility_target" in conditions:
            vol_target = conditions["volatility_target"]
            # æ ¹æ®ç›®æ ‡æ³¢åŠ¨ç‡è°ƒæ•´ç”Ÿæˆç­–ç•¥
            config.diversity_weight = min(1.0, vol_target * 2)
            config.quality_weight = 1.0 - config.diversity_weight
        
        # ä½¿ç”¨åŒæ­¥ç”Ÿæˆ
        synthetic_data = self.model_orchestrator.generate_samples_sync(
            num_samples=config.num_samples,
            sequence_length=config.sequence_length,
            conditions=conditions
        )
        
        return synthetic_data
    
    def create_generation_pipeline(
        self,
        pipeline_name: str,
        stages: List[Dict[str, Any]]
    ) -> str:
        """åˆ›å»ºç”Ÿæˆç®¡é“"""
        pipeline_id = f"{pipeline_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        pipeline = {
            "id": pipeline_id,
            "name": pipeline_name,
            "stages": stages,
            "created_at": datetime.utcnow(),
            "status": "ready"
        }
        
        # å­˜å‚¨ç®¡é“é…ç½®
        if not hasattr(self, 'pipelines'):
            self.pipelines = {}
        
        self.pipelines[pipeline_id] = pipeline
        
        return pipeline_id
    
    async def execute_pipeline(
        self,
        pipeline_id: str,
        input_data: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡Œç”Ÿæˆç®¡é“"""
        if not hasattr(self, 'pipelines') or pipeline_id not in self.pipelines:
            raise ValueError(f"ç®¡é“ä¸å­˜åœ¨: {pipeline_id}")
        
        pipeline = self.pipelines[pipeline_id]
        pipeline["status"] = "running"
        
        current_data = input_data
        stage_results = []
        
        try:
            for i, stage in enumerate(pipeline["stages"]):
                stage_name = stage.get("name", f"stage_{i}")
                stage_type = stage.get("type", "generate")
                stage_config = stage.get("config", {})
                
                self.logger.info(f"æ‰§è¡Œç®¡é“é˜¶æ®µ: {stage_name}")
                
                if stage_type == "generate":
                    config = GenerationConfig(**stage_config)
                    if current_data is not None:
                        # ä½¿ç”¨å‰ä¸€é˜¶æ®µçš„æ•°æ®ä½œä¸ºæ¡ä»¶
                        config.conditions = {"reference_data": current_data}
                    
                    current_data = self.model_orchestrator.generate_samples_sync(
                        num_samples=config.num_samples,
                        sequence_length=config.sequence_length,
                        conditions=config.conditions
                    )
                
                elif stage_type == "evaluate":
                    if current_data is not None and input_data is not None:
                        metrics = self.evaluator.comprehensive_evaluation(input_data, current_data)
                        stage_results.append({
                            "stage": stage_name,
                            "type": "evaluation",
                            "metrics": metrics
                        })
                
                elif stage_type == "filter":
                    if current_data is not None:
                        # ç®€å•çš„æ•°æ®è¿‡æ»¤ç¤ºä¾‹
                        filter_threshold = stage_config.get("threshold", 0.5)
                        mask = np.std(current_data, axis=1) > filter_threshold
                        current_data = current_data[mask]
                
                stage_results.append({
                    "stage": stage_name,
                    "type": stage_type,
                    "output_shape": current_data.shape if current_data is not None else None
                })
            
            pipeline["status"] = "completed"
            
            return {
                "pipeline_id": pipeline_id,
                "final_data": current_data,
                "stage_results": stage_results,
                "execution_time": (datetime.utcnow() - pipeline["created_at"]).total_seconds()
            }
            
        except Exception as e:
            pipeline["status"] = "failed"
            self.logger.error(f"ç®¡é“æ‰§è¡Œå¤±è´¥ {pipeline_id}: {e}")
            raise
    
    def get_system_dashboard(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä»ªè¡¨æ¿"""
        return {
            "system_status": {
                "enhanced_mode": self.is_enhanced_mode,
                "active_streams": len(self.active_streams),
                "registered_models": len(self.model_orchestrator.model_pool.models),
                "distributed_workers": len(self.distributed_coordinator.workers),
                "generation_cache_size": len(self.generation_cache)
            },
            "performance_metrics": {
                "total_generations": len(self.performance_history),
                "avg_generation_time": np.mean([p["generation_time"] for p in self.performance_history]) if self.performance_history else 0,
                "avg_quality_score": np.mean([p["quality_score"] for p in self.performance_history]) if self.performance_history else 0,
                "recent_throughput": self.real_time_monitor.current_metrics.throughput
            },
            "quality_assurance": self.quality_assurance.get_qa_statistics(),
            "real_time_metrics": {
                "current_latency": self.real_time_monitor.current_metrics.generation_latency,
                "current_quality": self.real_time_monitor.current_metrics.quality_score,
                "memory_usage": self.real_time_monitor.current_metrics.memory_usage,
                "cpu_usage": self.real_time_monitor.current_metrics.cpu_usage
            },
            "stream_info": {
                stream_id: {
                    "started_at": stream["started_at"].isoformat(),
                    "samples_generated": stream["samples_generated"]
                }
                for stream_id, stream in self.active_streams.items()
            }
        }
    
    async def cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        # åœæ­¢æ‰€æœ‰æµ
        for stream_id in list(self.active_streams.keys()):
            self.stop_real_time_stream(stream_id)
        
        # åœæ­¢ç›‘æ§
        self.real_time_monitor.is_monitoring = False
        self.streaming_generator.stop_streaming()
        
        # åœæ­¢æ¨¡å‹ç¼–æ’å™¨
        await self.model_orchestrator.stop()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        self.logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

# åˆ›å»ºå…¨å±€å®ä¾‹
enhanced_ctbench_system = EnhancedCTBenchSystem()