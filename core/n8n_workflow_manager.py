"""
N8Nå·¥ä½œæµç®¡ç†å™¨
å®ç°ä¸n8nå¹³å°çš„é›†æˆï¼Œè‡ªåŠ¨åŒ–äº¤æ˜“æµç¨‹å’Œæ•°æ®å¤„ç†
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.unified_logger import get_logger, LogCategory

logger = get_logger()

class WorkflowStatus(Enum):
    """å·¥ä½œæµçŠ¶æ€"""
    IDLE = "idle"
    RUNNING = "running"
    SUCCESS = "success"
    ERROR = "error"
    WAITING = "waiting"
    CANCELED = "canceled"

class WorkflowTrigger(Enum):
    """å·¥ä½œæµè§¦å‘å™¨ç±»å‹"""
    MANUAL = "manual"
    WEBHOOK = "webhook"
    CRON = "cron"
    DATA_CHANGE = "data_change"
    PRICE_ALERT = "price_alert"
    MARKET_EVENT = "market_event"

@dataclass
class WorkflowExecution:
    """å·¥ä½œæµæ‰§è¡Œè®°å½•"""
    execution_id: str
    workflow_id: str
    status: WorkflowStatus
    trigger_type: WorkflowTrigger
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    input_data: Optional[Dict] = None
    output_data: Optional[Dict] = None
    error_message: Optional[str] = None
    retry_count: int = 0

@dataclass
class WorkflowDefinition:
    """å·¥ä½œæµå®šä¹‰"""
    workflow_id: str
    name: str
    description: str
    trigger_type: WorkflowTrigger
    enabled: bool = True
    schedule: Optional[str] = None  # Cronè¡¨è¾¾å¼
    retry_attempts: int = 3
    timeout_seconds: int = 300
    webhook_path: Optional[str] = None
    nodes: List[Dict] = field(default_factory=list)

class N8NClient:
    """N8N APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:5678", api_key: Optional[str] = None):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        headers = {}
        if self.api_key:
            headers["X-N8N-API-KEY"] = self.api_key
        
        self.session = aiohttp.ClientSession(
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_workflows(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·¥ä½œæµ"""
        try:
            async with self.session.get(f"{self.base_url}/api/v1/workflows") as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('data', [])
                else:
                    logger.error(f"è·å–å·¥ä½œæµå¤±è´¥: HTTP {response.status}")
                    return []
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå¼‚å¸¸: {e}")
            return []
    
    async def get_workflow(self, workflow_id: str) -> Optional[Dict]:
        """è·å–å•ä¸ªå·¥ä½œæµè¯¦æƒ…"""
        try:
            async with self.session.get(f"{self.base_url}/api/v1/workflows/{workflow_id}") as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logger.error(f"è·å–å·¥ä½œæµ {workflow_id} å¤±è´¥: HTTP {response.status}")
                    return None
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµ {workflow_id} å¼‚å¸¸: {e}")
            return None
    
    async def execute_workflow(self, workflow_id: str, input_data: Optional[Dict] = None) -> Optional[str]:
        """æ‰‹åŠ¨æ‰§è¡Œå·¥ä½œæµ"""
        try:
            payload = {"data": input_data} if input_data else {}
            
            async with self.session.post(
                f"{self.base_url}/api/v1/workflows/{workflow_id}/execute",
                json=payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get('executionId')
                else:
                    logger.error(f"æ‰§è¡Œå·¥ä½œæµ {workflow_id} å¤±è´¥: HTTP {response.status}")
                    return None
        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥ä½œæµ {workflow_id} å¼‚å¸¸: {e}")
            return None
    
    async def get_execution_status(self, execution_id: str) -> Optional[Dict]:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        try:
            async with self.session.get(f"{self.base_url}/api/v1/executions/{execution_id}") as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logger.error(f"è·å–æ‰§è¡ŒçŠ¶æ€ {execution_id} å¤±è´¥: HTTP {response.status}")
                    return None
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡ŒçŠ¶æ€ {execution_id} å¼‚å¸¸: {e}")
            return None
    
    async def cancel_execution(self, execution_id: str) -> bool:
        """å–æ¶ˆæ‰§è¡Œ"""
        try:
            async with self.session.delete(f"{self.base_url}/api/v1/executions/{execution_id}/stop") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"å–æ¶ˆæ‰§è¡Œ {execution_id} å¼‚å¸¸: {e}")
            return False
    
    async def create_workflow(self, workflow_data: Dict) -> Optional[str]:
        """åˆ›å»ºæ–°å·¥ä½œæµ"""
        try:
            async with self.session.post(f"{self.base_url}/api/v1/workflows", json=workflow_data) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get('id')
                else:
                    logger.error(f"åˆ›å»ºå·¥ä½œæµå¤±è´¥: HTTP {response.status}")
                    return None
        except Exception as e:
            logger.error(f"åˆ›å»ºå·¥ä½œæµå¼‚å¸¸: {e}")
            return None
    
    async def update_workflow(self, workflow_id: str, workflow_data: Dict) -> bool:
        """æ›´æ–°å·¥ä½œæµ"""
        try:
            async with self.session.put(f"{self.base_url}/api/v1/workflows/{workflow_id}", json=workflow_data) as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"æ›´æ–°å·¥ä½œæµ {workflow_id} å¼‚å¸¸: {e}")
            return False
    
    async def activate_workflow(self, workflow_id: str) -> bool:
        """æ¿€æ´»å·¥ä½œæµ"""
        try:
            async with self.session.post(f"{self.base_url}/api/v1/workflows/{workflow_id}/activate") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"æ¿€æ´»å·¥ä½œæµ {workflow_id} å¼‚å¸¸: {e}")
            return False
    
    async def deactivate_workflow(self, workflow_id: str) -> bool:
        """åœç”¨å·¥ä½œæµ"""
        try:
            async with self.session.post(f"{self.base_url}/api/v1/workflows/{workflow_id}/deactivate") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"åœç”¨å·¥ä½œæµ {workflow_id} å¼‚å¸¸: {e}")
            return False

class WorkflowErrorHandler:
    """å·¥ä½œæµé”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self):
        self.error_counts = {}
        self.error_history = []
        self.max_history = 1000
        
    def record_error(self, workflow_id: str, execution_id: str, error_message: str, error_data: Dict = None):
        """è®°å½•é”™è¯¯"""
        error_record = {
            "timestamp": datetime.utcnow(),
            "workflow_id": workflow_id,
            "execution_id": execution_id,
            "error_message": error_message,
            "error_data": error_data or {},
            "severity": self._classify_error_severity(error_message)
        }
        
        # è®°å½•åˆ°å†å²
        self.error_history.append(error_record)
        if len(self.error_history) > self.max_history:
            self.error_history.pop(0)
        
        # æ›´æ–°é”™è¯¯è®¡æ•°
        self.error_counts[workflow_id] = self.error_counts.get(workflow_id, 0) + 1
        
        # è®°å½•æ—¥å¿—
        severity = error_record["severity"]
        if severity == "critical":
            logger.critical(f"ğŸš¨ å·¥ä½œæµä¸¥é‡é”™è¯¯ [{workflow_id}:{execution_id}]: {error_message}")
        elif severity == "high":
            logger.error(f"âŒ å·¥ä½œæµé«˜ä¼˜å…ˆçº§é”™è¯¯ [{workflow_id}:{execution_id}]: {error_message}")
        elif severity == "medium":
            logger.warning(f"âš ï¸ å·¥ä½œæµä¸­ç­‰é”™è¯¯ [{workflow_id}:{execution_id}]: {error_message}")
        else:
            logger.info(f"â„¹ï¸ å·¥ä½œæµä¸€èˆ¬é”™è¯¯ [{workflow_id}:{execution_id}]: {error_message}")
    
    def _classify_error_severity(self, error_message: str) -> str:
        """é”™è¯¯ä¸¥é‡ç¨‹åº¦åˆ†ç±»"""
        error_lower = error_message.lower()
        
        if any(keyword in error_lower for keyword in ["timeout", "connection", "network", "database"]):
            return "high"
        elif any(keyword in error_lower for keyword in ["authentication", "permission", "unauthorized"]):
            return "critical"
        elif any(keyword in error_lower for keyword in ["validation", "format", "parse"]):
            return "medium"
        else:
            return "low"
    
    def get_error_stats(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯ç»Ÿè®¡"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        recent_errors = [e for e in self.error_history if e["timestamp"] >= cutoff_time]
        
        severity_counts = {}
        workflow_errors = {}
        
        for error in recent_errors:
            # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
            severity = error["severity"]
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
            # æŒ‰å·¥ä½œæµç»Ÿè®¡
            workflow_id = error["workflow_id"]
            if workflow_id not in workflow_errors:
                workflow_errors[workflow_id] = {"count": 0, "last_error": None}
            workflow_errors[workflow_id]["count"] += 1
            workflow_errors[workflow_id]["last_error"] = error["error_message"]
        
        return {
            "time_range_hours": hours,
            "total_errors": len(recent_errors),
            "severity_breakdown": severity_counts,
            "workflow_breakdown": workflow_errors,
            "error_rate": len(recent_errors) / max(hours, 1)  # é”™è¯¯/å°æ—¶
        }

class N8NWorkflowManager:
    """N8Nå·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self, n8n_base_url: str = "http://localhost:5678", api_key: Optional[str] = None):
        self.n8n_base_url = n8n_base_url
        self.api_key = api_key
        self.workflows: Dict[str, WorkflowDefinition] = {}
        self.executions: Dict[str, WorkflowExecution] = {}
        self.error_handler = WorkflowErrorHandler()
        
        # ç›‘æ§ä»»åŠ¡
        self.monitoring_task = None
        self.monitoring_enabled = False
        self.monitoring_interval = 30  # ç§’
        
    async def initialize(self):
        """åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–N8Nå·¥ä½œæµç®¡ç†å™¨...")
            
            # æµ‹è¯•N8Nè¿æ¥
            await self._test_n8n_connection()
            
            # åŠ è½½ç°æœ‰å·¥ä½œæµ
            await self._load_workflows()
            
            # åˆå§‹åŒ–é¢„å®šä¹‰å·¥ä½œæµ
            await self._initialize_predefined_workflows()
            
            # å¯åŠ¨ç›‘æ§
            self.monitoring_enabled = True
            self.monitoring_task = asyncio.create_task(self._monitoring_loop())
            
            logger.info(f"âœ… N8Nå·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½{len(self.workflows)}ä¸ªå·¥ä½œæµ")
            
        except Exception as e:
            logger.error(f"âŒ N8Nå·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _test_n8n_connection(self):
        """æµ‹è¯•N8Nè¿æ¥"""
        try:
            async with N8NClient(self.n8n_base_url, self.api_key) as client:
                workflows = await client.get_workflows()
                logger.info(f"âœ… N8Nè¿æ¥æˆåŠŸï¼Œå‘ç°{len(workflows)}ä¸ªç°æœ‰å·¥ä½œæµ")
        except Exception as e:
            logger.warning(f"âš ï¸ N8Nè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦å¯åŠ¨N8NæœåŠ¡: {e}")
    
    async def _load_workflows(self):
        """åŠ è½½ç°æœ‰å·¥ä½œæµ"""
        try:
            async with N8NClient(self.n8n_base_url, self.api_key) as client:
                workflows = await client.get_workflows()
                
                for workflow_data in workflows:
                    workflow_def = WorkflowDefinition(
                        workflow_id=workflow_data.get('id', ''),
                        name=workflow_data.get('name', ''),
                        description=workflow_data.get('description', ''),
                        trigger_type=WorkflowTrigger.MANUAL,  # é»˜è®¤æ‰‹åŠ¨è§¦å‘
                        enabled=workflow_data.get('active', False),
                        nodes=workflow_data.get('nodes', [])
                    )
                    
                    self.workflows[workflow_def.workflow_id] = workflow_def
                    
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰å·¥ä½œæµå¤±è´¥: {e}")
    
    async def _initialize_predefined_workflows(self):
        """åˆå§‹åŒ–é¢„å®šä¹‰å·¥ä½œæµ"""
        # å®šä¹‰äº¤æ˜“ç³»ç»Ÿå¸¸ç”¨å·¥ä½œæµæ¨¡æ¿
        predefined_workflows = [
            {
                "name": "ä»·æ ¼ç›‘æ§å’Œå‘Šè­¦",
                "description": "ç›‘æ§ç‰¹å®šäº¤æ˜“å¯¹ä»·æ ¼å˜åŒ–å¹¶å‘é€å‘Šè­¦",
                "trigger_type": WorkflowTrigger.CRON,
                "schedule": "*/5 * * * *",  # æ¯5åˆ†é’Ÿæ‰§è¡Œ
                "nodes": self._create_price_monitoring_nodes()
            },
            {
                "name": "æ•°æ®è´¨é‡æ£€æŸ¥",
                "description": "å®šæœŸæ£€æŸ¥æ•°æ®è´¨é‡å¹¶ç”ŸæˆæŠ¥å‘Š",
                "trigger_type": WorkflowTrigger.CRON,
                "schedule": "0 */1 * * *",  # æ¯å°æ—¶æ‰§è¡Œ
                "nodes": self._create_data_quality_nodes()
            },
            {
                "name": "ç³»ç»Ÿå¥åº·æ£€æŸ¥",
                "description": "ç›‘æ§ç³»ç»Ÿå„ç»„ä»¶å¥åº·çŠ¶æ€",
                "trigger_type": WorkflowTrigger.CRON,
                "schedule": "*/10 * * * *",  # æ¯10åˆ†é’Ÿæ‰§è¡Œ
                "nodes": self._create_health_check_nodes()
            },
            {
                "name": "äº¤æ˜“ä¿¡å·å¤„ç†",
                "description": "å¤„ç†AIç”Ÿæˆçš„äº¤æ˜“ä¿¡å·",
                "trigger_type": WorkflowTrigger.WEBHOOK,
                "webhook_path": "/webhook/trading-signal",
                "nodes": self._create_trading_signal_nodes()
            },
            {
                "name": "é£é™©ç®¡ç†",
                "description": "å®æ—¶é£é™©è¯„ä¼°å’Œç®¡ç†",
                "trigger_type": WorkflowTrigger.DATA_CHANGE,
                "nodes": self._create_risk_management_nodes()
            }
        ]
        
        # åˆ›å»ºé¢„å®šä¹‰å·¥ä½œæµ
        for workflow_template in predefined_workflows:
            try:
                workflow_id = await self._create_workflow_from_template(workflow_template)
                if workflow_id:
                    logger.info(f"âœ… åˆ›å»ºé¢„å®šä¹‰å·¥ä½œæµ: {workflow_template['name']}")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºé¢„å®šä¹‰å·¥ä½œæµå¤±è´¥ {workflow_template['name']}: {e}")
    
    def _create_price_monitoring_nodes(self) -> List[Dict]:
        """åˆ›å»ºä»·æ ¼ç›‘æ§èŠ‚ç‚¹"""
        return [
            {
                "name": "è·å–ä»·æ ¼æ•°æ®",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/data/latest-prices",
                    "options": {"timeout": 10000}
                },
                "position": [250, 300]
            },
            {
                "name": "ä»·æ ¼å˜åŒ–åˆ¤æ–­",
                "type": "n8n-nodes-base.function",
                "parameters": {
                    "functionCode": """
                    const data = items[0].json;
                    const btcPrice = parseFloat(data.prices['BTC/USDT'].binance?.price || 0);
                    const ethPrice = parseFloat(data.prices['ETH/USDT'].binance?.price || 0);
                    
                    // ä»·æ ¼å‘Šè­¦é˜ˆå€¼
                    const alerts = [];
                    if (btcPrice > 50000) alerts.push({coin: 'BTC', price: btcPrice, type: 'high'});
                    if (btcPrice < 40000) alerts.push({coin: 'BTC', price: btcPrice, type: 'low'});
                    if (ethPrice > 4000) alerts.push({coin: 'ETH', price: ethPrice, type: 'high'});
                    if (ethPrice < 3000) alerts.push({coin: 'ETH', price: ethPrice, type: 'low'});
                    
                    return alerts.map(alert => ({json: alert}));
                    """
                },
                "position": [450, 300]
            },
            {
                "name": "å‘é€å‘Šè­¦",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/alerts/price",
                    "method": "POST",
                    "options": {"timeout": 5000}
                },
                "position": [650, 300]
            }
        ]
    
    def _create_data_quality_nodes(self) -> List[Dict]:
        """åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥èŠ‚ç‚¹"""
        return [
            {
                "name": "è·å–æ•°æ®è´¨é‡æŠ¥å‘Š",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/data/quality-report",
                    "options": {"timeout": 30000}
                },
                "position": [250, 300]
            },
            {
                "name": "åˆ†æè´¨é‡æŒ‡æ ‡",
                "type": "n8n-nodes-base.function",
                "parameters": {
                    "functionCode": """
                    const report = items[0].json;
                    const issues = [];
                    
                    // æ£€æŸ¥æ•°æ®è´¨é‡åˆ†æ•°
                    if (report.overall_score < 80) {
                        issues.push({
                            type: 'low_quality_score',
                            score: report.overall_score,
                            severity: 'medium'
                        });
                    }
                    
                    // æ£€æŸ¥é”™è¯¯ç‡
                    const errorRate = report.error_rate || 0;
                    if (errorRate > 5) {
                        issues.push({
                            type: 'high_error_rate',
                            rate: errorRate,
                            severity: 'high'
                        });
                    }
                    
                    return issues.map(issue => ({json: issue}));
                    """
                },
                "position": [450, 300]
            },
            {
                "name": "è®°å½•è´¨é‡é—®é¢˜",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/logs/data-quality",
                    "method": "POST"
                },
                "position": [650, 300]
            }
        ]
    
    def _create_health_check_nodes(self) -> List[Dict]:
        """åˆ›å»ºå¥åº·æ£€æŸ¥èŠ‚ç‚¹"""
        return [
            {
                "name": "ç³»ç»Ÿå¥åº·æ£€æŸ¥",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/ha/health",
                    "options": {"timeout": 15000}
                },
                "position": [250, 300]
            },
            {
                "name": "å¥åº·çŠ¶æ€è¯„ä¼°",
                "type": "n8n-nodes-base.function",
                "parameters": {
                    "functionCode": """
                    const health = items[0].json.data;
                    const alerts = [];
                    
                    // æ£€æŸ¥æ•´ä½“çŠ¶æ€
                    if (health.overall_status !== 'healthy') {
                        alerts.push({
                            type: 'system_unhealthy',
                            status: health.overall_status,
                            severity: 'critical'
                        });
                    }
                    
                    // æ£€æŸ¥æœåŠ¡çŠ¶æ€
                    for (const [service, info] of Object.entries(health.services)) {
                        if (info.availability < 80) {
                            alerts.push({
                                type: 'service_degraded',
                                service: service,
                                availability: info.availability,
                                severity: 'high'
                            });
                        }
                    }
                    
                    return alerts.map(alert => ({json: alert}));
                    """
                },
                "position": [450, 300]
            },
            {
                "name": "ç³»ç»Ÿå‘Šè­¦",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/alerts/system",
                    "method": "POST"
                },
                "position": [650, 300]
            }
        ]
    
    def _create_trading_signal_nodes(self) -> List[Dict]:
        """åˆ›å»ºäº¤æ˜“ä¿¡å·å¤„ç†èŠ‚ç‚¹"""
        return [
            {
                "name": "æ¥æ”¶äº¤æ˜“ä¿¡å·",
                "type": "n8n-nodes-base.webhook",
                "parameters": {
                    "path": "trading-signal",
                    "httpMethod": "POST"
                },
                "position": [250, 300]
            },
            {
                "name": "ä¿¡å·éªŒè¯",
                "type": "n8n-nodes-base.function",
                "parameters": {
                    "functionCode": """
                    const signal = items[0].json;
                    
                    // éªŒè¯ä¿¡å·æ ¼å¼
                    const required_fields = ['symbol', 'action', 'confidence', 'timestamp'];
                    const missing_fields = required_fields.filter(field => !signal[field]);
                    
                    if (missing_fields.length > 0) {
                        throw new Error(`ç¼ºå°‘å¿…è¦å­—æ®µ: ${missing_fields.join(', ')}`);
                    }
                    
                    // éªŒè¯ç½®ä¿¡åº¦
                    if (signal.confidence < 0.7) {
                        throw new Error(`ä¿¡å·ç½®ä¿¡åº¦è¿‡ä½: ${signal.confidence}`);
                    }
                    
                    return items;
                    """
                },
                "position": [450, 300]
            },
            {
                "name": "æ‰§è¡Œäº¤æ˜“å†³ç­–",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/trading/execute-signal",
                    "method": "POST"
                },
                "position": [650, 300]
            }
        ]
    
    def _create_risk_management_nodes(self) -> List[Dict]:
        """åˆ›å»ºé£é™©ç®¡ç†èŠ‚ç‚¹"""
        return [
            {
                "name": "è·å–æŒä»“ä¿¡æ¯",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/trading/positions",
                    "options": {"timeout": 10000}
                },
                "position": [250, 300]
            },
            {
                "name": "é£é™©è¯„ä¼°",
                "type": "n8n-nodes-base.function",
                "parameters": {
                    "functionCode": """
                    const positions = items[0].json.data;
                    const risks = [];
                    
                    let totalExposure = 0;
                    for (const position of positions) {
                        totalExposure += Math.abs(position.value || 0);
                        
                        // å•ä¸€æŒä»“é£é™©
                        const exposure_ratio = Math.abs(position.value) / position.account_balance;
                        if (exposure_ratio > 0.1) {  // å•ä»“è¶…è¿‡10%
                            risks.push({
                                type: 'high_single_exposure',
                                symbol: position.symbol,
                                ratio: exposure_ratio,
                                severity: 'medium'
                            });
                        }
                    }
                    
                    // æ€»ä½“é£é™©
                    const total_ratio = totalExposure / positions[0]?.account_balance || 0;
                    if (total_ratio > 0.5) {  // æ€»é£é™©è¶…è¿‡50%
                        risks.push({
                            type: 'high_total_exposure',
                            ratio: total_ratio,
                            severity: 'high'
                        });
                    }
                    
                    return risks.map(risk => ({json: risk}));
                    """
                },
                "position": [450, 300]
            },
            {
                "name": "é£é™©å‘Šè­¦",
                "type": "n8n-nodes-base.httpRequest",
                "parameters": {
                    "url": "http://localhost:8000/api/v1/alerts/risk",
                    "method": "POST"
                },
                "position": [650, 300]
            }
        ]
    
    async def _create_workflow_from_template(self, template: Dict) -> Optional[str]:
        """ä»æ¨¡æ¿åˆ›å»ºå·¥ä½œæµ"""
        workflow_data = {
            "name": template["name"],
            "description": template["description"],
            "active": True,
            "nodes": template["nodes"],
            "connections": self._generate_node_connections(template["nodes"])
        }
        
        try:
            async with N8NClient(self.n8n_base_url, self.api_key) as client:
                workflow_id = await client.create_workflow(workflow_data)
                
                if workflow_id:
                    # åˆ›å»ºå·¥ä½œæµå®šä¹‰
                    workflow_def = WorkflowDefinition(
                        workflow_id=workflow_id,
                        name=template["name"],
                        description=template["description"],
                        trigger_type=template["trigger_type"],
                        enabled=True,
                        schedule=template.get("schedule"),
                        webhook_path=template.get("webhook_path"),
                        nodes=template["nodes"]
                    )
                    
                    self.workflows[workflow_id] = workflow_def
                    return workflow_id
                    
        except Exception as e:
            logger.error(f"ä»æ¨¡æ¿åˆ›å»ºå·¥ä½œæµå¤±è´¥: {e}")
            return None
    
    def _generate_node_connections(self, nodes: List[Dict]) -> Dict:
        """ç”ŸæˆèŠ‚ç‚¹è¿æ¥å…³ç³»"""
        connections = {}
        
        # ç®€å•çš„é¡ºåºè¿æ¥
        for i in range(len(nodes) - 1):
            current_node = nodes[i]["name"]
            next_node = nodes[i + 1]["name"]
            
            connections[current_node] = {
                "main": [
                    [
                        {
                            "node": next_node,
                            "type": "main",
                            "index": 0
                        }
                    ]
                ]
            }
        
        return connections
    
    async def execute_workflow(self, workflow_id: str, input_data: Optional[Dict] = None) -> Optional[str]:
        """æ‰§è¡Œå·¥ä½œæµ"""
        try:
            if workflow_id not in self.workflows:
                logger.error(f"å·¥ä½œæµä¸å­˜åœ¨: {workflow_id}")
                return None
            
            workflow_def = self.workflows[workflow_id]
            
            # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å¯ç”¨
            if not workflow_def.enabled:
                logger.warning(f"å·¥ä½œæµå·²ç¦ç”¨: {workflow_id}")
                return None
            
            # æ‰§è¡Œå·¥ä½œæµ
            async with N8NClient(self.n8n_base_url, self.api_key) as client:
                execution_id = await client.execute_workflow(workflow_id, input_data)
                
                if execution_id:
                    # è®°å½•æ‰§è¡Œ
                    execution = WorkflowExecution(
                        execution_id=execution_id,
                        workflow_id=workflow_id,
                        status=WorkflowStatus.RUNNING,
                        trigger_type=WorkflowTrigger.MANUAL,
                        start_time=datetime.utcnow(),
                        input_data=input_data
                    )
                    
                    self.executions[execution_id] = execution
                    logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå¯åŠ¨: {workflow_def.name} [{execution_id}]")
                    
                return execution_id
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥ä½œæµå¤±è´¥ {workflow_id}: {e}")
            self.error_handler.record_error(workflow_id, "", str(e))
            return None
    
    async def get_execution_status(self, execution_id: str) -> Optional[WorkflowExecution]:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        if execution_id not in self.executions:
            return None
        
        execution = self.executions[execution_id]
        
        try:
            # ä»N8Nè·å–æœ€æ–°çŠ¶æ€
            async with N8NClient(self.n8n_base_url, self.api_key) as client:
                status_data = await client.get_execution_status(execution_id)
                
                if status_data:
                    # æ›´æ–°æ‰§è¡Œè®°å½•
                    execution.status = WorkflowStatus(status_data.get('status', 'running'))
                    if status_data.get('finishedAt'):
                        execution.end_time = datetime.fromisoformat(status_data['finishedAt'])
                        execution.duration = (execution.end_time - execution.start_time).total_seconds()
                    
                    execution.output_data = status_data.get('data')
                    execution.error_message = status_data.get('error', {}).get('message')
                    
                    # å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•
                    if execution.status == WorkflowStatus.ERROR and execution.error_message:
                        self.error_handler.record_error(
                            execution.workflow_id, 
                            execution_id, 
                            execution.error_message
                        )
                
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡ŒçŠ¶æ€å¤±è´¥ {execution_id}: {e}")
        
        return execution
    
    async def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("ğŸ” å¯åŠ¨å·¥ä½œæµç›‘æ§å¾ªç¯")
        
        while self.monitoring_enabled:
            try:
                await asyncio.sleep(self.monitoring_interval)
                
                # æ£€æŸ¥è¿è¡Œä¸­çš„æ‰§è¡Œ
                running_executions = [
                    exec_id for exec_id, execution in self.executions.items()
                    if execution.status == WorkflowStatus.RUNNING
                ]
                
                for exec_id in running_executions:
                    await self.get_execution_status(exec_id)
                
                # æ¸…ç†æ—§çš„æ‰§è¡Œè®°å½•
                await self._cleanup_old_executions()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
        
        logger.info("ğŸ” å·¥ä½œæµç›‘æ§å¾ªç¯å·²åœæ­¢")
    
    async def _cleanup_old_executions(self):
        """æ¸…ç†æ—§çš„æ‰§è¡Œè®°å½•"""
        cutoff_time = datetime.utcnow() - timedelta(hours=24)
        
        old_executions = [
            exec_id for exec_id, execution in self.executions.items()
            if execution.start_time < cutoff_time and execution.status not in [WorkflowStatus.RUNNING]
        ]
        
        for exec_id in old_executions:
            del self.executions[exec_id]
        
        if old_executions:
            logger.info(f"ğŸ§¹ æ¸…ç†äº†{len(old_executions)}ä¸ªæ—§æ‰§è¡Œè®°å½•")
    
    def get_workflow_stats(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµç»Ÿè®¡"""
        total_executions = len(self.executions)
        status_counts = {}
        
        for execution in self.executions.values():
            status = execution.status.value
            status_counts[status] = status_counts.get(status, 0) + 1
        
        # æˆåŠŸç‡
        success_count = status_counts.get(WorkflowStatus.SUCCESS.value, 0)
        success_rate = (success_count / total_executions * 100) if total_executions > 0 else 0
        
        # é”™è¯¯ç»Ÿè®¡
        error_stats = self.error_handler.get_error_stats()
        
        return {
            "total_workflows": len(self.workflows),
            "enabled_workflows": len([w for w in self.workflows.values() if w.enabled]),
            "total_executions": total_executions,
            "execution_status": status_counts,
            "success_rate": success_rate,
            "error_statistics": error_stats,
            "monitoring_enabled": self.monitoring_enabled
        }
    
    async def shutdown(self):
        """å…³é—­å·¥ä½œæµç®¡ç†å™¨"""
        try:
            self.monitoring_enabled = False
            
            if self.monitoring_task:
                self.monitoring_task.cancel()
                await self.monitoring_task
            
            logger.info("âœ… N8Nå·¥ä½œæµç®¡ç†å™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­å·¥ä½œæµç®¡ç†å™¨å¤±è´¥: {e}")

# å…¨å±€å·¥ä½œæµç®¡ç†å™¨å®ä¾‹
n8n_workflow_manager = N8NWorkflowManager()