"""
Asynchronous Advantage Actor-Critic (A3C) å¼ºåŒ–å­¦ä¹ Agent
å®žçŽ°A3Cç®—æ³•ç”¨äºŽäº¤æ˜“å†³ç­–å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import torch.multiprocessing as mp
import numpy as np
import threading
import queue
from collections import namedtuple, deque
from typing import Dict, List, Optional, Any, Tuple, Callable
import logging
from dataclasses import dataclass
import json
import os
import time
import asyncio

# A3Cç»éªŒå…ƒç»„
A3CExperience = namedtuple('A3CExperience', ['state', 'action', 'reward', 'next_state', 'done', 'value'])

@dataclass
class A3CConfig:
    """A3Cé…ç½®"""
    # ç½‘ç»œç»“æž„
    hidden_dims: List[int] = None
    dropout_rate: float = 0.1
    activation: str = "relu"  # relu, tanh, leaky_relu
    
    # A3Cç‰¹å®šå‚æ•°
    learning_rate: float = 1e-3
    gamma: float = 0.99  # æŠ˜æ‰£å› å­
    entropy_coef: float = 0.01  # ç†µæŸå¤±ç³»æ•°
    value_loss_coef: float = 0.5  # ä»·å€¼æŸå¤±ç³»æ•°
    
    # è®­ç»ƒå‚æ•°
    n_step: int = 20  # n-step TD
    max_grad_norm: float = 40.0  # æ¢¯åº¦è£å‰ª
    
    # å¼‚æ­¥å‚æ•°
    num_workers: int = 4  # å·¥ä½œè¿›ç¨‹æ•°é‡
    update_frequency: int = 20  # æ›´æ–°é¢‘çŽ‡
    
    # æŽ¢ç´¢å‚æ•°
    epsilon_start: float = 1.0
    epsilon_end: float = 0.01
    epsilon_decay_steps: int = 1000000
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [256, 256, 128]

class A3CNetwork(nn.Module):
    """A3C Actor-Criticç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, config: A3CConfig):
        super(A3CNetwork, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ¿€æ´»å‡½æ•°
        if config.activation == "relu":
            self.activation = nn.ReLU()
        elif config.activation == "tanh":
            self.activation = nn.Tanh()
        elif config.activation == "leaky_relu":
            self.activation = nn.LeakyReLU()
        else:
            self.activation = nn.ReLU()
        
        # æž„å»ºå…±äº«å±‚
        self._build_shared_layers()
        
        # Actorå¤´éƒ¨ (ç­–ç•¥ç½‘ç»œ)
        self._build_actor_head()
        
        # Criticå¤´éƒ¨ (ä»·å€¼ç½‘ç»œ)
        self._build_critic_head()
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _build_shared_layers(self):
        """æž„å»ºå…±äº«å±‚"""
        layers = []
        prev_dim = self.state_dim
        
        for hidden_dim in self.config.hidden_dims[:-1]:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                self.activation,
                nn.Dropout(self.config.dropout_rate)
            ])
            prev_dim = hidden_dim
        
        self.shared_layers = nn.Sequential(*layers)
        self.shared_output_dim = prev_dim
    
    def _build_actor_head(self):
        """æž„å»ºActorå¤´éƒ¨"""
        final_hidden_dim = self.config.hidden_dims[-1]
        
        self.actor_head = nn.Sequential(
            nn.Linear(self.shared_output_dim, final_hidden_dim),
            self.activation,
            nn.Linear(final_hidden_dim, self.action_dim)
        )
    
    def _build_critic_head(self):
        """æž„å»ºCriticå¤´éƒ¨"""
        final_hidden_dim = self.config.hidden_dims[-1]
        
        self.critic_head = nn.Sequential(
            nn.Linear(self.shared_output_dim, final_hidden_dim),
            self.activation,
            nn.Linear(final_hidden_dim, 1)
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_layers(state)
        
        # Actorè¾“å‡º (åŠ¨ä½œæ¦‚çŽ‡logits)
        action_logits = self.actor_head(shared_features)
        
        # Criticè¾“å‡º (çŠ¶æ€ä»·å€¼)
        state_value = self.critic_head(shared_features).squeeze(-1)
        
        return action_logits, state_value
    
    def act(self, state: torch.Tensor) -> Tuple[int, float, float]:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        action_logits, value = self.forward(state)
        
        # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
        action_probs = F.softmax(action_logits, dim=-1)
        action_dist = Categorical(action_probs)
        
        # é‡‡æ ·åŠ¨ä½œ
        action = action_dist.sample()
        log_prob = action_dist.log_prob(action)
        
        return action.item(), log_prob.item(), value.item()
    
    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è¯„ä¼°çŠ¶æ€å’ŒåŠ¨ä½œ"""
        action_logits, value = self.forward(state)
        
        # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
        action_probs = F.softmax(action_logits, dim=-1)
        action_dist = Categorical(action_probs)
        
        # è®¡ç®—logæ¦‚çŽ‡å’Œç†µ
        log_prob = action_dist.log_prob(action)
        entropy = action_dist.entropy()
        
        return log_prob, value, entropy

class GlobalNetwork:
    """å…¨å±€ç½‘ç»œ (å‚æ•°æœåŠ¡å™¨)"""
    
    def __init__(self, state_dim: int, action_dim: int, config: A3CConfig):
        self.network = A3CNetwork(state_dim, action_dim, config)
        self.optimizer = optim.Adam(self.network.parameters(), lr=config.learning_rate)
        self.lock = threading.Lock()
        
        # å…¨å±€ç»Ÿè®¡
        self.global_step = 0
        self.global_episode = 0
        self.total_rewards = []
        
    def update(self, gradients: List[torch.Tensor]):
        """æ›´æ–°å…¨å±€ç½‘ç»œ"""
        with self.lock:
            # åº”ç”¨æ¢¯åº¦
            for param, grad in zip(self.network.parameters(), gradients):
                if param.grad is None:
                    param.grad = grad
                else:
                    param.grad += grad
            
            # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            self.global_step += 1
    
    def get_parameters(self) -> Dict[str, torch.Tensor]:
        """èŽ·å–å…¨å±€å‚æ•°"""
        with self.lock:
            return {name: param.clone() for name, param in self.network.state_dict().items()}
    
    def add_reward(self, reward: float):
        """æ·»åŠ episodeå¥–åŠ±"""
        with self.lock:
            self.total_rewards.append(reward)
            if len(self.total_rewards) > 1000:
                self.total_rewards = self.total_rewards[-500:]
            self.global_episode += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "global_step": self.global_step,
                "global_episode": self.global_episode,
                "avg_reward": np.mean(self.total_rewards) if self.total_rewards else 0,
                "recent_avg_reward": np.mean(self.total_rewards[-100:]) if len(self.total_rewards) >= 100 else np.mean(self.total_rewards) if self.total_rewards else 0
            }

class A3CWorker:
    """A3Cå·¥ä½œè¿›ç¨‹"""
    
    def __init__(self, worker_id: int, global_network: GlobalNetwork, 
                 env_factory: Callable, config: A3CConfig):
        self.worker_id = worker_id
        self.global_network = global_network
        self.env_factory = env_factory
        self.config = config
        
        # æœ¬åœ°ç½‘ç»œ
        self.local_network = A3CNetwork(
            global_network.network.state_dim,
            global_network.network.action_dim,
            config
        )
        
        # ç»éªŒç¼“å†²
        self.experience_buffer = []
        
        # ç»Ÿè®¡
        self.local_step = 0
        self.local_episode = 0
        self.episode_rewards = []
        
        self.logger = logging.getLogger(f"A3CWorker-{worker_id}")
        
    def run(self, max_episodes: int = 1000):
        """è¿è¡Œå·¥ä½œè¿›ç¨‹"""
        env = self.env_factory()
        
        for episode in range(max_episodes):
            self._run_episode(env)
            
            if episode % 100 == 0:
                stats = self.global_network.get_stats()
                self.logger.info(f"Worker {self.worker_id}: Episode {episode}, "
                               f"Global Step: {stats['global_step']}, "
                               f"Avg Reward: {stats['avg_reward']:.2f}")
    
    def _run_episode(self, env):
        """è¿è¡Œä¸€ä¸ªepisode"""
        # åŒæ­¥æœ¬åœ°ç½‘ç»œä¸Žå…¨å±€ç½‘ç»œ
        self._sync_with_global()
        
        state = env.reset()
        episode_reward = 0
        self.experience_buffer.clear()
        
        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, value = self._select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            experience = A3CExperience(state, action, reward, next_state, done, value)
            self.experience_buffer.append(experience)
            
            episode_reward += reward
            self.local_step += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if len(self.experience_buffer) >= self.config.n_step or done:
                self._update_global_network(next_state, done)
                self.experience_buffer.clear()
            
            state = next_state
            
            if done:
                break
        
        # è®°å½•episodeå¥–åŠ±
        self.episode_rewards.append(episode_reward)
        self.global_network.add_reward(episode_reward)
        self.local_episode += 1
    
    def _select_action(self, state: np.ndarray) -> Tuple[int, float, float]:
        """é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action, log_prob, value = self.local_network.act(state_tensor)
            return action, log_prob, value
    
    def _sync_with_global(self):
        """ä¸Žå…¨å±€ç½‘ç»œåŒæ­¥"""
        global_params = self.global_network.get_parameters()
        self.local_network.load_state_dict(global_params)
    
    def _update_global_network(self, next_state: np.ndarray, done: bool):
        """æ›´æ–°å…¨å±€ç½‘ç»œ"""
        if not self.experience_buffer:
            return
        
        # è®¡ç®—å›žæŠ¥
        returns = self._compute_returns(next_state, done)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = self._compute_gradients(returns)
        
        # æ›´æ–°å…¨å±€ç½‘ç»œ
        self.global_network.update(gradients)
    
    def _compute_returns(self, next_state: np.ndarray, done: bool) -> List[float]:
        """è®¡ç®—n-stepå›žæŠ¥"""
        returns = []
        
        # è®¡ç®—bootstrapä»·å€¼
        if done:
            R = 0.0
        else:
            with torch.no_grad():
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                _, next_value = self.local_network.forward(next_state_tensor)
                R = next_value.item()
        
        # åå‘è®¡ç®—å›žæŠ¥
        for experience in reversed(self.experience_buffer):
            R = experience.reward + self.config.gamma * R
            returns.append(R)
        
        returns.reverse()
        return returns
    
    def _compute_gradients(self, returns: List[float]) -> List[torch.Tensor]:
        """è®¡ç®—æ¢¯åº¦"""
        # å‡†å¤‡æ•°æ®
        states = torch.FloatTensor([exp.state for exp in self.experience_buffer])
        actions = torch.LongTensor([exp.action for exp in self.experience_buffer])
        returns_tensor = torch.FloatTensor(returns)
        values = torch.FloatTensor([exp.value for exp in self.experience_buffer])
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages = returns_tensor - values
        
        # å‰å‘ä¼ æ’­
        log_probs, current_values, entropy = self.local_network.evaluate(states, actions)
        
        # è®¡ç®—æŸå¤±
        # ActoræŸå¤± (ç­–ç•¥æ¢¯åº¦)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # CriticæŸå¤± (ä»·å€¼å‡½æ•°)
        critic_loss = F.mse_loss(current_values, returns_tensor)
        
        # ç†µæŸå¤± (é¼“åŠ±æŽ¢ç´¢)
        entropy_loss = -entropy.mean()
        
        # æ€»æŸå¤±
        total_loss = (actor_loss + 
                     self.config.value_loss_coef * critic_loss + 
                     self.config.entropy_coef * entropy_loss)
        
        # è®¡ç®—æ¢¯åº¦
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.local_network.parameters(), self.config.max_grad_norm)
        
        # èŽ·å–æ¢¯åº¦
        gradients = [param.grad.clone() for param in self.local_network.parameters()]
        
        # æ¸…é›¶æœ¬åœ°æ¢¯åº¦
        self.local_network.zero_grad()
        
        return gradients

class A3CAgent:
    """A3Cä¸»Agent"""
    
    def __init__(self, state_dim: int, action_dim: int, config: A3CConfig = None):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config or A3CConfig()
        
        # å…¨å±€ç½‘ç»œ
        self.global_network = GlobalNetwork(state_dim, action_dim, self.config)
        
        # å·¥ä½œè¿›ç¨‹æ± 
        self.workers = []
        self.worker_threads = []
        
        # è®­ç»ƒçŠ¶æ€
        self.is_training = False
        
        self.logger = logging.getLogger("A3CAgent")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """é€‰æ‹©åŠ¨ä½œ (ç”¨äºŽè¯„ä¼°)"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_logits, _ = self.global_network.network.forward(state_tensor)
            
            if training:
                # ä½¿ç”¨ç­–ç•¥åˆ†å¸ƒé‡‡æ ·
                action_probs = F.softmax(action_logits, dim=-1)
                action_dist = Categorical(action_probs)
                action = action_dist.sample()
            else:
                # ä½¿ç”¨è´ªå©ªç­–ç•¥
                action = action_logits.argmax()
            
            return action.item()
    
    async def train(self, env_factory: Callable, num_episodes: int = 1000,
                   progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """è®­ç»ƒA3C"""
        try:
            self.logger.info("ðŸš€ å¼€å§‹A3Cè®­ç»ƒ...")
            self.is_training = True
            
            # åˆ›å»ºå·¥ä½œè¿›ç¨‹
            self._create_workers(env_factory, num_episodes)
            
            # å¯åŠ¨å·¥ä½œçº¿ç¨‹
            for worker in self.workers:
                thread = threading.Thread(target=worker.run, args=(num_episodes,))
                thread.start()
                self.worker_threads.append(thread)
            
            # ç›‘æŽ§è®­ç»ƒè¿›ç¨‹
            await self._monitor_training(progress_callback)
            
            # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
            for thread in self.worker_threads:
                thread.join()
            
            self.is_training = False
            
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
            final_stats = self.global_network.get_stats()
            
            self.logger.info("âœ… A3Cè®­ç»ƒå®Œæˆ")
            return final_stats
            
        except Exception as e:
            self.logger.error(f"âŒ A3Cè®­ç»ƒå¤±è´¥: {e}")
            self.is_training = False
            raise
    
    def _create_workers(self, env_factory: Callable, num_episodes: int):
        """åˆ›å»ºå·¥ä½œè¿›ç¨‹"""
        self.workers = []
        for i in range(self.config.num_workers):
            worker = A3CWorker(i, self.global_network, env_factory, self.config)
            self.workers.append(worker)
        
        self.logger.info(f"åˆ›å»ºäº† {len(self.workers)} ä¸ªA3Cå·¥ä½œè¿›ç¨‹")
    
    async def _monitor_training(self, progress_callback: Optional[Callable]):
        """ç›‘æŽ§è®­ç»ƒè¿›ç¨‹"""
        last_episode = 0
        
        while self.is_training and any(thread.is_alive() for thread in self.worker_threads):
            await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
            
            stats = self.global_network.get_stats()
            current_episode = stats["global_episode"]
            
            # è°ƒç”¨è¿›åº¦å›žè°ƒ
            if progress_callback and current_episode > last_episode:
                await progress_callback(current_episode, {
                    "episode_reward": stats["avg_reward"],
                    "recent_avg_reward": stats["recent_avg_reward"],
                    "global_step": stats["global_step"]
                })
            
            last_episode = current_episode
    
    def get_training_stats(self) -> Dict[str, Any]:
        """èŽ·å–è®­ç»ƒç»Ÿè®¡"""
        return self.global_network.get_stats()
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡åž‹"""
        checkpoint = {
            "network_state_dict": self.global_network.network.state_dict(),
            "optimizer_state_dict": self.global_network.optimizer.state_dict(),
            "config": self.config.__dict__,
            "training_stats": self.get_training_stats()
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        torch.save(checkpoint, filepath)
        self.logger.info(f"A3Cæ¨¡åž‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡åž‹"""
        if not os.path.exists(filepath):
            self.logger.error(f"æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False
        
        try:
            checkpoint = torch.load(filepath, map_location="cpu")
            
            self.global_network.network.load_state_dict(checkpoint["network_state_dict"])
            self.global_network.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            
            self.logger.info(f"A3Cæ¨¡åž‹å·²ä»Ž {filepath} åŠ è½½")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½A3Cæ¨¡åž‹å¤±è´¥: {e}")
            return False
    
    def set_eval_mode(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        self.global_network.network.eval()
    
    def set_train_mode(self):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.global_network.network.train()
    
    def get_action_probabilities(self, state: np.ndarray) -> np.ndarray:
        """èŽ·å–åŠ¨ä½œæ¦‚çŽ‡åˆ†å¸ƒ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_logits, _ = self.global_network.network.forward(state_tensor)
            probabilities = F.softmax(action_logits, dim=-1)
            return probabilities.numpy()[0]
    
    def get_state_value(self, state: np.ndarray) -> float:
        """èŽ·å–çŠ¶æ€ä»·å€¼"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            _, value = self.global_network.network.forward(state_tensor)
            return value.item()

# è®­ç»ƒå·¥å…·å‡½æ•°
def train_a3c_agent(
    agent: A3CAgent,
    env_factory: Callable,
    num_episodes: int = 1000,
    eval_frequency: int = 100,
    save_frequency: int = 500,
    model_save_path: str = "models/a3c_trading_agent.pth"
) -> Dict[str, List]:
    """è®­ç»ƒA3C Agent"""
    
    logger = logging.getLogger("A3CTrainer")
    training_history = {
        "episode_rewards": [],
        "global_steps": [],
        "evaluation_scores": []
    }
    
    async def progress_callback(episode: int, metrics: Dict[str, Any]):
        training_history["episode_rewards"].append(metrics["episode_reward"])
        training_history["global_steps"].append(metrics["global_step"])
        
        # è¯„ä¼°
        if episode % eval_frequency == 0:
            eval_score = evaluate_a3c_agent(agent, env_factory(), num_episodes=5)
            training_history["evaluation_scores"].append(eval_score)
            
            logger.info(f"Episode {episode}: Avg Reward={metrics['episode_reward']:.2f}, "
                       f"Eval Score={eval_score:.2f}, Global Step={metrics['global_step']}")
        
        # ä¿å­˜æ¨¡åž‹
        if episode % save_frequency == 0 and episode > 0:
            agent.save_model(model_save_path.replace(".pth", f"_ep{episode}.pth"))
    
    # è¿è¡Œè®­ç»ƒ
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        final_stats = loop.run_until_complete(
            agent.train(env_factory, num_episodes, progress_callback)
        )
    finally:
        loop.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
    agent.save_model(model_save_path)
    
    return training_history

def evaluate_a3c_agent(agent: A3CAgent, env, num_episodes: int = 10) -> float:
    """è¯„ä¼°A3C Agentæ€§èƒ½"""
    agent.set_eval_mode()
    
    total_rewards = []
    
    for _ in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        
        while True:
            action = agent.select_action(state, training=False)
            state, reward, done, _ = env.step(action)
            episode_reward += reward
            
            if done:
                break
        
        total_rewards.append(episode_reward)
    
    agent.set_train_mode()
    
    return np.mean(total_rewards)