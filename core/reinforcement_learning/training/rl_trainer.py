"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
æä¾›ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹è®­ç»ƒæ¥å£å’Œç®¡ç†
"""

import asyncio
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Any, Callable
import logging
from datetime import datetime, timedelta
import json
import os
from dataclasses import dataclass, asdict
from pathlib import Path

from ..environments.trading_env import TradingEnvironment, create_trading_environment
from ..algorithms.dqn_agent import DQNAgent, DQNConfig

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # è®­ç»ƒå‚æ•°
    num_episodes: int = 1000
    max_steps_per_episode: int = 1000
    eval_frequency: int = 100
    save_frequency: int = 500
    
    # ç¯å¢ƒé…ç½®
    env_config: Dict[str, Any] = None
    
    # æ¨¡å‹é…ç½®
    model_config: Dict[str, Any] = None
    
    # æ•°æ®é…ç½®
    train_data_path: Optional[str] = None
    val_data_path: Optional[str] = None
    
    # ä¿å­˜è·¯å¾„
    model_save_dir: str = "models/rl_models"
    log_save_dir: str = "logs/rl_training"
    
    # æ—©åœå‚æ•°
    early_stopping_patience: int = 50
    early_stopping_min_delta: float = 0.001
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler: bool = True
    lr_step_size: int = 200
    lr_gamma: float = 0.9
    
    def __post_init__(self):
        if self.env_config is None:
            self.env_config = {}
        if self.model_config is None:
            self.model_config = {}

@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡"""
    episode: int = 0
    episode_reward: float = 0.0
    episode_length: int = 0
    training_loss: float = 0.0
    epsilon: float = 1.0
    eval_score: float = 0.0
    portfolio_return: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience: int = 50, min_delta: float = 0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = float('-inf')
        self.patience_counter = 0
        
    def __call__(self, score: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if score > self.best_score + self.min_delta:
            self.best_score = score
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        return self.patience_counter >= self.patience

class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.logger = logging.getLogger("RLTrainer")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.config.model_save_dir, exist_ok=True)
        os.makedirs(self.config.log_save_dir, exist_ok=True)
        
        # è®­ç»ƒå†å²
        self.training_history: List[TrainingMetrics] = []
        
        # æ—©åœæœºåˆ¶
        self.early_stopping = EarlyStopping(
            patience=self.config.early_stopping_patience,
            min_delta=self.config.early_stopping_min_delta
        )
        
        # æ¨¡å‹å’Œç¯å¢ƒ
        self.agent: Optional[DQNAgent] = None
        self.env: Optional[TradingEnvironment] = None
        self.eval_env: Optional[TradingEnvironment] = None
        
        # è®­ç»ƒçŠ¶æ€
        self.is_training = False
        self.should_stop = False
        
    async def setup_training(self):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        try:
            self.logger.info("è®¾ç½®è®­ç»ƒç¯å¢ƒ...")
            
            # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
            self.env = create_trading_environment(self.config.env_config)
            self.eval_env = create_trading_environment(self.config.env_config)
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            await self._load_training_data()
            
            # åˆ›å»ºAgent
            self._create_agent()
            
            self.logger.info("âœ… è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ è®¾ç½®è®­ç»ƒç¯å¢ƒå¤±è´¥: {e}")
            raise
    
    async def _load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        if self.config.train_data_path and os.path.exists(self.config.train_data_path):
            # ä»æ–‡ä»¶åŠ è½½
            data = pd.read_csv(self.config.train_data_path)
            prices = data['close'].values
            volumes = data.get('volume', pd.Series([1000000] * len(data))).values
            
            self.env.load_data(prices, volumes)
            self.eval_env.load_data(prices, volumes)
            
            self.logger.info(f"ä»æ–‡ä»¶åŠ è½½æ•°æ®: {len(prices)} ä¸ªæ•°æ®ç‚¹")
        else:
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            self._generate_synthetic_data()
    
    def _generate_synthetic_data(self):
        """ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®"""
        self.logger.info("ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
        np.random.seed(42)
        n_points = 10000
        
        # å‡ ä½•å¸ƒæœ—è¿åŠ¨
        dt = 1/252  # æ—¥é¢‘ç‡
        mu = 0.1  # å¹´åŒ–æ”¶ç›Šç‡
        sigma = 0.2  # å¹´åŒ–æ³¢åŠ¨ç‡
        
        price_changes = np.random.normal(
            (mu - 0.5 * sigma**2) * dt, 
            sigma * np.sqrt(dt), 
            n_points
        )
        
        # ç”Ÿæˆä»·æ ¼åºåˆ—
        initial_price = 100.0
        prices = [initial_price]
        
        for change in price_changes:
            prices.append(prices[-1] * np.exp(change))
        
        prices = np.array(prices)
        
        # ç”Ÿæˆæˆäº¤é‡æ•°æ®ï¼ˆä¸ä»·æ ¼å˜åŒ–ç›¸å…³ï¼‰
        price_returns = np.diff(prices) / prices[:-1]
        base_volume = 1000000
        volume_multipliers = 1 + 2 * np.abs(price_returns)  # ä»·æ ¼å˜åŒ–å¤§æ—¶æˆäº¤é‡å¤§
        volumes = np.concatenate([[base_volume], base_volume * volume_multipliers])
        
        # åŠ è½½åˆ°ç¯å¢ƒ
        self.env.load_data(prices, volumes)
        self.eval_env.load_data(prices, volumes)
        
        self.logger.info(f"ç”Ÿæˆåˆæˆæ•°æ®: {len(prices)} ä¸ªæ•°æ®ç‚¹")
    
    def _create_agent(self):
        """åˆ›å»ºå¼ºåŒ–å­¦ä¹ Agent"""
        # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.n
        
        # åˆ›å»ºDQNé…ç½®
        dqn_config = DQNConfig(**self.config.model_config)
        
        # åˆ›å»ºAgent
        self.agent = DQNAgent(state_dim, action_dim, dqn_config)
        
        self.logger.info(f"åˆ›å»ºDQN Agent: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
    async def train(self, progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """å¼€å§‹è®­ç»ƒ"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ...")
            
            if not self.agent or not self.env:
                await self.setup_training()
            
            self.is_training = True
            self.should_stop = False
            
            # å­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.config.lr_scheduler:
                scheduler = torch.optim.lr_scheduler.StepLR(
                    self.agent.optimizer,
                    step_size=self.config.lr_step_size,
                    gamma=self.config.lr_gamma
                )
            
            best_eval_score = float('-inf')
            
            for episode in range(self.config.num_episodes):
                if self.should_stop:
                    self.logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢è®­ç»ƒ")
                    break
                
                # è®­ç»ƒä¸€ä¸ªepisode
                metrics = await self._train_episode(episode)
                self.training_history.append(metrics)
                
                # è¯„ä¼°
                if episode % self.config.eval_frequency == 0:
                    eval_score = await self._evaluate_agent()
                    metrics.eval_score = eval_score
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if eval_score > best_eval_score:
                        best_eval_score = eval_score
                        best_model_path = os.path.join(self.config.model_save_dir, "best_model.pth")
                        self.agent.save_model(best_model_path)
                    
                    # æ—©åœæ£€æŸ¥
                    if self.early_stopping(eval_score):
                        self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨episode {episode}åœæ­¢è®­ç»ƒ")
                        break
                    
                    self.logger.info(
                        f"Episode {episode}: Reward={metrics.episode_reward:.2f}, "
                        f"Eval={eval_score:.2f}, Epsilon={metrics.epsilon:.3f}"
                    )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if episode % self.config.save_frequency == 0 and episode > 0:
                    checkpoint_path = os.path.join(
                        self.config.model_save_dir, 
                        f"checkpoint_ep{episode}.pth"
                    )
                    self.agent.save_model(checkpoint_path)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                if self.config.lr_scheduler:
                    scheduler.step()
                
                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    await progress_callback(episode, metrics)
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(self.config.model_save_dir, "final_model.pth")
            self.agent.save_model(final_model_path)
            
            # ä¿å­˜è®­ç»ƒå†å²
            await self._save_training_history()
            
            self.is_training = False
            
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
            final_stats = self._calculate_training_stats()
            
            self.logger.info("âœ… è®­ç»ƒå®Œæˆ")
            return final_stats
            
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            self.is_training = False
            raise
    
    async def _train_episode(self, episode: int) -> TrainingMetrics:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = self.env.reset()
        episode_reward = 0
        episode_length = 0
        total_loss = 0
        loss_count = 0
        
        for step in range(self.config.max_steps_per_episode):
            # é€‰æ‹©åŠ¨ä½œ
            action = self.agent.select_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            self.agent.store_experience(state, action, reward, next_state, done)
            
            # è®­ç»ƒ
            loss = self.agent.train_step()
            if loss is not None:
                total_loss += loss
                loss_count += 1
            
            state = next_state
            episode_reward += reward
            episode_length += 1
            
            if done:
                break
        
        # ç»“æŸepisode
        self.agent.end_episode(episode_reward)
        
        # è·å–ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡
        env_metrics = self.env.get_performance_metrics()
        
        # åˆ›å»ºè®­ç»ƒæŒ‡æ ‡
        metrics = TrainingMetrics(
            episode=episode,
            episode_reward=episode_reward,
            episode_length=episode_length,
            training_loss=total_loss / max(loss_count, 1),
            epsilon=self.agent.epsilon,
            portfolio_return=env_metrics.get('total_return', 0),
            sharpe_ratio=env_metrics.get('sharpe_ratio', 0),
            max_drawdown=env_metrics.get('max_drawdown', 0)
        )
        
        return metrics
    
    async def _evaluate_agent(self, num_episodes: int = 5) -> float:
        """è¯„ä¼°Agentæ€§èƒ½"""
        if not self.agent or not self.eval_env:
            return 0.0
        
        self.agent.set_eval_mode()
        
        total_rewards = []
        portfolio_returns = []
        
        for _ in range(num_episodes):
            state = self.eval_env.reset()
            episode_reward = 0
            
            while True:
                action = self.agent.select_action(state, training=False)
                state, reward, done, info = self.eval_env.step(action)
                episode_reward += reward
                
                if done:
                    break
            
            total_rewards.append(episode_reward)
            
            # è·å–æŠ•èµ„ç»„åˆè¡¨ç°
            metrics = self.eval_env.get_performance_metrics()
            portfolio_returns.append(metrics.get('total_return', 0))
        
        self.agent.set_train_mode()
        
        # ç»¼åˆè¯„åˆ†ï¼šå¹³å‡å¥–åŠ±å’ŒæŠ•èµ„ç»„åˆå›æŠ¥çš„åŠ æƒå¹³å‡
        avg_reward = np.mean(total_rewards)
        avg_return = np.mean(portfolio_returns)
        
        # ç»¼åˆè¯„åˆ† = 50% å¥–åŠ± + 50% æŠ•èµ„ç»„åˆå›æŠ¥
        composite_score = 0.5 * avg_reward + 0.5 * avg_return * 100  # æ”¾å¤§å›æŠ¥çš„æƒé‡
        
        return composite_score
    
    def _calculate_training_stats(self) -> Dict[str, Any]:
        """è®¡ç®—è®­ç»ƒç»Ÿè®¡"""
        if not self.training_history:
            return {}
        
        rewards = [m.episode_reward for m in self.training_history]
        losses = [m.training_loss for m in self.training_history if m.training_loss > 0]
        returns = [m.portfolio_return for m in self.training_history]
        sharpe_ratios = [m.sharpe_ratio for m in self.training_history if m.sharpe_ratio != 0]
        
        stats = {
            "total_episodes": len(self.training_history),
            "avg_episode_reward": np.mean(rewards),
            "avg_training_loss": np.mean(losses) if losses else 0,
            "avg_portfolio_return": np.mean(returns),
            "final_epsilon": self.training_history[-1].epsilon,
            "best_episode_reward": max(rewards),
            "best_portfolio_return": max(returns),
            "avg_sharpe_ratio": np.mean(sharpe_ratios) if sharpe_ratios else 0,
            "training_duration": (
                self.training_history[-1].timestamp - self.training_history[0].timestamp
            ).total_seconds() / 3600  # å°æ—¶
        }
        
        # æœ€è¿‘100ä¸ªepisodeçš„ç»Ÿè®¡
        if len(rewards) >= 100:
            recent_rewards = rewards[-100:]
            recent_returns = returns[-100:]
            
            stats.update({
                "recent_avg_reward": np.mean(recent_rewards),
                "recent_avg_return": np.mean(recent_returns),
                "recent_best_reward": max(recent_rewards),
                "recent_best_return": max(recent_returns)
            })
        
        return stats
    
    async def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        try:
            # ä¿å­˜ä¸ºJSON
            history_data = [asdict(metrics) for metrics in self.training_history]
            
            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            for entry in history_data:
                entry['timestamp'] = entry['timestamp'].isoformat()
            
            history_path = os.path.join(self.config.log_save_dir, "training_history.json")
            with open(history_path, 'w') as f:
                json.dump(history_data, f, indent=2)
            
            # ä¿å­˜ä¸ºCSV
            df = pd.DataFrame(history_data)
            csv_path = os.path.join(self.config.log_save_dir, "training_history.csv")
            df.to_csv(csv_path, index=False)
            
            # ç”Ÿæˆè®­ç»ƒå›¾è¡¨
            await self._generate_training_plots()
            
            self.logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ° {history_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e}")
    
    async def _generate_training_plots(self):
        """ç”Ÿæˆè®­ç»ƒå›¾è¡¨"""
        try:
            if len(self.training_history) < 2:
                return
            
            # å‡†å¤‡æ•°æ®
            episodes = [m.episode for m in self.training_history]
            rewards = [m.episode_reward for m in self.training_history]
            losses = [m.training_loss for m in self.training_history if m.training_loss > 0]
            returns = [m.portfolio_return for m in self.training_history]
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('RL Training Progress', fontsize=16)
            
            # å¥–åŠ±æ›²çº¿
            axes[0, 0].plot(episodes, rewards)
            axes[0, 0].set_title('Episode Rewards')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Reward')
            axes[0, 0].grid(True)
            
            # æŸå¤±æ›²çº¿
            if losses:
                loss_episodes = [m.episode for m in self.training_history if m.training_loss > 0]
                axes[0, 1].plot(loss_episodes, losses)
                axes[0, 1].set_title('Training Loss')
                axes[0, 1].set_xlabel('Episode')
                axes[0, 1].set_ylabel('Loss')
                axes[0, 1].grid(True)
            
            # æŠ•èµ„ç»„åˆå›æŠ¥
            axes[1, 0].plot(episodes, returns)
            axes[1, 0].set_title('Portfolio Returns')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Return')
            axes[1, 0].grid(True)
            
            # ç§»åŠ¨å¹³å‡å¥–åŠ±
            window_size = min(50, len(rewards) // 4)
            if window_size > 1:
                moving_avg = pd.Series(rewards).rolling(window=window_size).mean()
                axes[1, 1].plot(episodes, rewards, alpha=0.3, label='Raw Rewards')
                axes[1, 1].plot(episodes, moving_avg, label=f'{window_size}-Episode MA')
                axes[1, 1].set_title('Reward Moving Average')
                axes[1, 1].set_xlabel('Episode')
                axes[1, 1].set_ylabel('Reward')
                axes[1, 1].legend()
                axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_path = os.path.join(self.config.log_save_dir, "training_plots.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° {plot_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè®­ç»ƒå›¾è¡¨å¤±è´¥: {e}")
    
    async def stop_training(self):
        """åœæ­¢è®­ç»ƒ"""
        self.should_stop = True
        self.logger.info("å‘é€åœæ­¢è®­ç»ƒä¿¡å·")
    
    def get_training_progress(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒè¿›åº¦"""
        if not self.training_history:
            return {"progress": 0, "status": "not_started"}
        
        current_episode = len(self.training_history)
        progress_pct = (current_episode / self.config.num_episodes) * 100
        
        latest_metrics = self.training_history[-1]
        
        return {
            "progress": min(100, progress_pct),
            "current_episode": current_episode,
            "total_episodes": self.config.num_episodes,
            "status": "training" if self.is_training else "completed",
            "latest_reward": latest_metrics.episode_reward,
            "latest_return": latest_metrics.portfolio_return,
            "epsilon": latest_metrics.epsilon,
            "best_reward": max(m.episode_reward for m in self.training_history),
            "avg_recent_reward": np.mean([m.episode_reward for m in self.training_history[-10:]]) if len(self.training_history) >= 10 else latest_metrics.episode_reward
        }
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            if not self.agent:
                self.logger.error("Agentæœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ£€æŸ¥ç‚¹")
                return False
            
            success = self.agent.load_model(checkpoint_path)
            if success:
                self.logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
            return success
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False

# è®­ç»ƒä»»åŠ¡ç®¡ç†
class TrainingTaskManager:
    """è®­ç»ƒä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_trainers: Dict[str, RLTrainer] = {}
        self.logger = logging.getLogger("TrainingTaskManager")
    
    async def start_training_task(
        self, 
        task_id: str, 
        config: TrainingConfig,
        progress_callback: Optional[Callable] = None
    ) -> str:
        """å¯åŠ¨è®­ç»ƒä»»åŠ¡"""
        if task_id in self.active_trainers:
            self.logger.warning(f"è®­ç»ƒä»»åŠ¡ {task_id} å·²å­˜åœ¨")
            return task_id
        
        trainer = RLTrainer(config)
        self.active_trainers[task_id] = trainer
        
        # å¼‚æ­¥æ‰§è¡Œè®­ç»ƒ
        asyncio.create_task(self._run_training_task(task_id, trainer, progress_callback))
        
        self.logger.info(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡: {task_id}")
        return task_id
    
    async def _run_training_task(
        self, 
        task_id: str, 
        trainer: RLTrainer, 
        progress_callback: Optional[Callable] = None
    ):
        """è¿è¡Œè®­ç»ƒä»»åŠ¡"""
        try:
            await trainer.train(progress_callback)
            self.logger.info(f"è®­ç»ƒä»»åŠ¡å®Œæˆ: {task_id}")
        except Exception as e:
            self.logger.error(f"è®­ç»ƒä»»åŠ¡å¤±è´¥ {task_id}: {e}")
        finally:
            # ä»»åŠ¡å®Œæˆåæ¸…ç†
            self.active_trainers.pop(task_id, None)
    
    async def stop_training_task(self, task_id: str) -> bool:
        """åœæ­¢è®­ç»ƒä»»åŠ¡"""
        if task_id not in self.active_trainers:
            return False
        
        trainer = self.active_trainers[task_id]
        await trainer.stop_training()
        
        self.logger.info(f"åœæ­¢è®­ç»ƒä»»åŠ¡: {task_id}")
        return True
    
    def get_task_progress(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡è¿›åº¦"""
        if task_id not in self.active_trainers:
            return None
        
        trainer = self.active_trainers[task_id]
        return trainer.get_training_progress()
    
    def list_active_tasks(self) -> List[str]:
        """åˆ—å‡ºæ´»è·ƒä»»åŠ¡"""
        return list(self.active_trainers.keys())

# å…¨å±€è®­ç»ƒä»»åŠ¡ç®¡ç†å™¨
training_task_manager = TrainingTaskManager()