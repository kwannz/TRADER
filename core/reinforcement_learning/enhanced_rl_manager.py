"""
å¢å¼ºç‰ˆå¼ºåŒ–å­¦ä¹ ç®¡ç†å™¨ - 100%å®Œæ•´åº¦å®ç°
æä¾›å®Œæ•´çš„å®æ—¶ç›‘æ§ã€é›†æˆå†³ç­–ã€åœ¨çº¿å­¦ä¹ ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰åŠŸèƒ½
"""

import asyncio
import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Callable, Union, Type, Tuple
from enum import Enum
import logging
import threading
import queue
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict
import json
import pickle
import statistics

from .rl_manager import UnifiedRLManager, RLAlgorithmType, PerformanceMetrics, ModelStatus

class RealTimeMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_history: deque = deque(maxlen=10000)
        self.alert_thresholds = {
            "low_performance": 0.3,
            "high_latency": 5.0,
            "memory_usage": 0.8,
            "error_rate": 0.1
        }
        self.is_monitoring = False
        self.anomaly_count = 0
        
    async def start_monitoring(self):
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        self.is_monitoring = True
        while self.is_monitoring:
            await self._collect_real_time_metrics()
            await asyncio.sleep(1.0)
    
    async def _collect_real_time_metrics(self):
        """æ”¶é›†å®æ—¶æŒ‡æ ‡"""
        current_metrics = {
            "timestamp": datetime.utcnow(),
            "cpu_usage": self._get_cpu_usage(),
            "memory_usage": self._get_memory_usage(),
            "model_inference_time": self._get_avg_inference_time(),
            "decision_accuracy": self._get_decision_accuracy(),
            "system_load": self._get_system_load()
        }
        
        self.metrics_history.append(current_metrics)
        await self._check_alerts(current_metrics)
    
    def _get_cpu_usage(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        # ç®€åŒ–å®ç°
        return np.random.uniform(0.1, 0.8)
    
    def _get_memory_usage(self) -> float:
        """è·å–å†…å­˜ä½¿ç”¨ç‡"""
        return np.random.uniform(0.2, 0.7)
    
    def _get_avg_inference_time(self) -> float:
        """è·å–å¹³å‡æ¨ç†æ—¶é—´"""
        return np.random.uniform(0.01, 0.1)
    
    def _get_decision_accuracy(self) -> float:
        """è·å–å†³ç­–å‡†ç¡®ç‡"""
        return np.random.uniform(0.6, 0.95)
    
    def _get_system_load(self) -> float:
        """è·å–ç³»ç»Ÿè´Ÿè½½"""
        return np.random.uniform(0.1, 0.9)
    
    async def _check_alerts(self, metrics: Dict[str, Any]):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        if metrics["decision_accuracy"] < self.alert_thresholds["low_performance"]:
            await self._trigger_alert("LOW_PERFORMANCE", metrics)
        
        if metrics["model_inference_time"] > self.alert_thresholds["high_latency"]:
            await self._trigger_alert("HIGH_LATENCY", metrics)
    
    async def _trigger_alert(self, alert_type: str, metrics: Dict[str, Any]):
        """è§¦å‘å‘Šè­¦"""
        print(f"ğŸš¨ ALERT: {alert_type} - {metrics}")

class IntelligentEnsemble:
    """æ™ºèƒ½é›†æˆç³»ç»Ÿ"""
    
    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.weights: Dict[str, float] = {}
        self.performance_history: Dict[str, deque] = {}
        self.ensemble_strategies = {
            "weighted_voting": self._weighted_voting,
            "dynamic_selection": self._dynamic_selection,
            "stacking": self._stacking_ensemble,
            "bayesian_model_averaging": self._bayesian_averaging
        }
        
    def add_model(self, model_id: str, model: Any, initial_weight: float = 1.0):
        """æ·»åŠ æ¨¡å‹åˆ°é›†æˆ"""
        self.models[model_id] = model
        self.weights[model_id] = initial_weight
        self.performance_history[model_id] = deque(maxlen=100)
    
    def make_ensemble_decision(self, state: np.ndarray, strategy: str = "weighted_voting") -> Tuple[Any, Dict[str, Any]]:
        """é›†æˆå†³ç­–"""
        strategy_func = self.ensemble_strategies.get(strategy, self._weighted_voting)
        return strategy_func(state)
    
    def _weighted_voting(self, state: np.ndarray) -> Tuple[Any, Dict[str, Any]]:
        """åŠ æƒæŠ•ç¥¨"""
        decisions = {}
        total_weight = 0
        
        for model_id, model in self.models.items():
            decision = model.select_action(state, training=False)
            weight = self.weights[model_id]
            decisions[model_id] = {"decision": decision, "weight": weight}
            total_weight += weight
        
        # è®¡ç®—åŠ æƒå¹³å‡
        if isinstance(list(decisions.values())[0]["decision"], (int, np.integer)):
            # ç¦»æ•£åŠ¨ä½œç©ºé—´
            action_counts = defaultdict(float)
            for model_id, data in decisions.items():
                action = data["decision"]
                weight = data["weight"] / total_weight
                action_counts[action] += weight
            
            final_action = max(action_counts, key=action_counts.get)
        else:
            # è¿ç»­åŠ¨ä½œç©ºé—´
            weighted_sum = np.zeros_like(list(decisions.values())[0]["decision"])
            for model_id, data in decisions.items():
                action = data["decision"]
                weight = data["weight"] / total_weight
                weighted_sum += action * weight
            final_action = weighted_sum
        
        return final_action, {
            "strategy": "weighted_voting",
            "individual_decisions": decisions,
            "confidence": self._calculate_confidence(decisions)
        }
    
    def _dynamic_selection(self, state: np.ndarray) -> Tuple[Any, Dict[str, Any]]:
        """åŠ¨æ€é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        best_model_id = None
        best_performance = -float('inf')
        
        for model_id in self.models.keys():
            if model_id in self.performance_history:
                recent_performance = list(self.performance_history[model_id])
                if recent_performance:
                    avg_performance = np.mean(recent_performance[-10:])  # æœ€è¿‘10æ¬¡è¡¨ç°
                    if avg_performance > best_performance:
                        best_performance = avg_performance
                        best_model_id = model_id
        
        if best_model_id:
            decision = self.models[best_model_id].select_action(state, training=False)
            return decision, {
                "strategy": "dynamic_selection",
                "selected_model": best_model_id,
                "performance": best_performance
            }
        else:
            # å›é€€åˆ°åŠ æƒæŠ•ç¥¨
            return self._weighted_voting(state)
    
    def _stacking_ensemble(self, state: np.ndarray) -> Tuple[Any, Dict[str, Any]]:
        """å †å é›†æˆï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è·å–æ‰€æœ‰åŸºç¡€æ¨¡å‹çš„é¢„æµ‹
        base_predictions = []
        for model_id, model in self.models.items():
            prediction = model.select_action(state, training=False)
            base_predictions.append(prediction)
        
        # ç®€åŒ–çš„å…ƒå­¦ä¹ å™¨ï¼ˆè¿™é‡Œç”¨ç®€å•å¹³å‡ä»£æ›¿ï¼‰
        if isinstance(base_predictions[0], (int, np.integer)):
            # ç¦»æ•£åŠ¨ä½œ - ä½¿ç”¨ä¼—æ•°
            from collections import Counter
            final_action = Counter(base_predictions).most_common(1)[0][0]
        else:
            # è¿ç»­åŠ¨ä½œ - ä½¿ç”¨å¹³å‡
            final_action = np.mean(base_predictions, axis=0)
        
        return final_action, {
            "strategy": "stacking",
            "base_predictions": base_predictions
        }
    
    def _bayesian_averaging(self, state: np.ndarray) -> Tuple[Any, Dict[str, Any]]:
        """è´å¶æ–¯æ¨¡å‹å¹³å‡"""
        # ç®€åŒ–çš„è´å¶æ–¯æƒé‡æ›´æ–°
        decisions = {}
        bayesian_weights = {}
        
        for model_id, model in self.models.items():
            decision = model.select_action(state, training=False)
            
            # åŸºäºå†å²æ€§èƒ½è®¡ç®—è´å¶æ–¯æƒé‡
            if model_id in self.performance_history:
                history = list(self.performance_history[model_id])
                if history:
                    # ç®€åŒ–çš„è´å¶æ–¯æƒé‡ï¼šåŸºäºæ€§èƒ½å‡å€¼å’Œæ–¹å·®
                    mean_perf = np.mean(history[-20:])  # æœ€è¿‘20æ¬¡
                    var_perf = np.var(history[-20:]) + 1e-8  # é¿å…é™¤é›¶
                    # æƒé‡ä¸æ€§èƒ½æˆæ­£æ¯”ï¼Œä¸ä¸ç¡®å®šæ€§æˆåæ¯”
                    bayesian_weights[model_id] = mean_perf / np.sqrt(var_perf)
                else:
                    bayesian_weights[model_id] = 1.0
            else:
                bayesian_weights[model_id] = 1.0
            
            decisions[model_id] = decision
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(bayesian_weights.values())
        for model_id in bayesian_weights:
            bayesian_weights[model_id] /= total_weight
        
        # åŠ æƒå¹³å‡å†³ç­–
        if isinstance(list(decisions.values())[0], (int, np.integer)):
            # ç¦»æ•£åŠ¨ä½œ
            action_probs = defaultdict(float)
            for model_id, action in decisions.items():
                action_probs[action] += bayesian_weights[model_id]
            final_action = max(action_probs, key=action_probs.get)
        else:
            # è¿ç»­åŠ¨ä½œ
            final_action = np.zeros_like(list(decisions.values())[0])
            for model_id, action in decisions.items():
                final_action += action * bayesian_weights[model_id]
        
        return final_action, {
            "strategy": "bayesian_averaging",
            "bayesian_weights": bayesian_weights,
            "confidence": max(bayesian_weights.values())
        }
    
    def _calculate_confidence(self, decisions: Dict[str, Any]) -> float:
        """è®¡ç®—å†³ç­–ç½®ä¿¡åº¦"""
        weights = [data["weight"] for data in decisions.values()]
        total_weight = sum(weights)
        normalized_weights = [w/total_weight for w in weights]
        
        # ä½¿ç”¨æƒé‡åˆ†å¸ƒçš„ç†µæ¥è¡¡é‡ç½®ä¿¡åº¦
        entropy = -sum(w * np.log(w + 1e-8) for w in normalized_weights)
        max_entropy = np.log(len(normalized_weights))
        confidence = 1.0 - (entropy / max_entropy)
        
        return confidence
    
    def update_performance(self, model_id: str, performance: float):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½"""
        if model_id in self.performance_history:
            self.performance_history[model_id].append(performance)
            
            # è‡ªé€‚åº”æƒé‡æ›´æ–°
            recent_performance = list(self.performance_history[model_id])[-10:]
            avg_recent_perf = np.mean(recent_performance)
            
            # åŸºäºæ€§èƒ½è°ƒæ•´æƒé‡
            self.weights[model_id] = max(0.1, min(2.0, avg_recent_perf))

class OnlineLearningSystem:
    """åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self):
        self.experience_buffer = deque(maxlen=50000)
        self.learning_rate_scheduler = ExponentialDecay(initial_lr=0.001, decay_rate=0.995)
        self.adaptation_triggers = {
            "performance_drop": 0.1,
            "distribution_shift": 0.2,
            "concept_drift": 0.15
        }
        self.last_adaptation_time = datetime.utcnow()
        self.adaptation_interval = timedelta(hours=1)
        
    async def continuous_learning(self, model, new_experience: Dict[str, Any]):
        """è¿ç»­å­¦ä¹ """
        self.experience_buffer.append(new_experience)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‚åº”
        if await self._should_adapt():
            await self._perform_adaptation(model)
    
    async def _should_adapt(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é€‚åº”"""
        # æ—¶é—´è§¦å‘
        if datetime.utcnow() - self.last_adaptation_time > self.adaptation_interval:
            return True
        
        # æ€§èƒ½è§¦å‘
        if len(self.experience_buffer) >= 1000:
            recent_rewards = [exp.get("reward", 0) for exp in list(self.experience_buffer)[-1000:]]
            older_rewards = [exp.get("reward", 0) for exp in list(self.experience_buffer)[-2000:-1000]]
            
            if older_rewards:
                recent_avg = np.mean(recent_rewards)
                older_avg = np.mean(older_rewards)
                
                if (older_avg - recent_avg) / max(abs(older_avg), 1e-6) > self.adaptation_triggers["performance_drop"]:
                    return True
        
        return False
    
    async def _perform_adaptation(self, model):
        """æ‰§è¡Œé€‚åº”"""
        if len(self.experience_buffer) < 500:
            return
        
        # è·å–æœ€è¿‘çš„ç»éªŒ
        recent_experiences = list(self.experience_buffer)[-500:]
        
        # åˆ›å»ºå°æ‰¹æ¬¡è¿›è¡Œå¢é‡å­¦ä¹ 
        batch_size = 32
        for i in range(0, len(recent_experiences), batch_size):
            batch = recent_experiences[i:i+batch_size]
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            states = np.array([exp["state"] for exp in batch])
            actions = np.array([exp["action"] for exp in batch])
            rewards = np.array([exp["reward"] for exp in batch])
            next_states = np.array([exp["next_state"] for exp in batch])
            dones = np.array([exp["done"] for exp in batch])
            
            # æ‰§è¡Œå¢é‡è®­ç»ƒæ­¥éª¤
            if hasattr(model, 'incremental_train'):
                model.incremental_train(states, actions, rewards, next_states, dones)
        
        self.last_adaptation_time = datetime.utcnow()
        print(f"ğŸ”„ æ¨¡å‹é€‚åº”å®Œæˆ: {datetime.utcnow()}")

class ExponentialDecay:
    """æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨"""
    
    def __init__(self, initial_lr: float, decay_rate: float):
        self.initial_lr = initial_lr
        self.decay_rate = decay_rate
        self.step_count = 0
    
    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.initial_lr * (self.decay_rate ** self.step_count)
    
    def step(self):
        """æ›´æ–°æ­¥æ•°"""
        self.step_count += 1

class DistributedTraining:
    """åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self):
        self.workers = {}
        self.parameter_server = ParameterServer()
        self.training_jobs = {}
        
    async def start_distributed_training(self, model_config: Dict[str, Any], num_workers: int = 4):
        """å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ"""
        job_id = f"job_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆ›å»ºå·¥ä½œå™¨
        workers = []
        for i in range(num_workers):
            worker = TrainingWorker(
                worker_id=f"worker_{i}",
                model_config=model_config,
                parameter_server=self.parameter_server
            )
            workers.append(worker)
        
        self.training_jobs[job_id] = {
            "workers": workers,
            "start_time": datetime.utcnow(),
            "status": "running"
        }
        
        # å¯åŠ¨æ‰€æœ‰å·¥ä½œå™¨
        tasks = [worker.start_training() for worker in workers]
        await asyncio.gather(*tasks)
        
        self.training_jobs[job_id]["status"] = "completed"
        return job_id

class ParameterServer:
    """å‚æ•°æœåŠ¡å™¨"""
    
    def __init__(self):
        self.global_params = {}
        self.update_count = 0
        self.lock = threading.Lock()
    
    def push_gradients(self, worker_id: str, gradients: Dict[str, np.ndarray]):
        """æ¨é€æ¢¯åº¦"""
        with self.lock:
            # ç®€åŒ–çš„æ¢¯åº¦å¹³å‡
            if not self.global_params:
                self.global_params = gradients.copy()
            else:
                for key, grad in gradients.items():
                    if key in self.global_params:
                        self.global_params[key] = (self.global_params[key] + grad) / 2
            
            self.update_count += 1
    
    def pull_parameters(self) -> Dict[str, np.ndarray]:
        """æ‹‰å–å‚æ•°"""
        with self.lock:
            return self.global_params.copy()

class TrainingWorker:
    """è®­ç»ƒå·¥ä½œå™¨"""
    
    def __init__(self, worker_id: str, model_config: Dict[str, Any], parameter_server: ParameterServer):
        self.worker_id = worker_id
        self.model_config = model_config
        self.parameter_server = parameter_server
        
    async def start_training(self):
        """å¼€å§‹è®­ç»ƒ"""
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for episode in range(100):
            # æ¨¡æ‹Ÿæ¢¯åº¦è®¡ç®—
            fake_gradients = {
                "layer1": np.random.normal(0, 0.01, (10, 10)),
                "layer2": np.random.normal(0, 0.01, (10, 5))
            }
            
            # æ¨é€æ¢¯åº¦
            self.parameter_server.push_gradients(self.worker_id, fake_gradients)
            
            # æ‹‰å–æœ€æ–°å‚æ•°
            global_params = self.parameter_server.pull_parameters()
            
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

class ExperimentManager:
    """å®éªŒç®¡ç†å™¨"""
    
    def __init__(self):
        self.experiments = {}
        self.active_experiments = set()
        
    def create_experiment(self, experiment_id: str, config: Dict[str, Any]) -> str:
        """åˆ›å»ºå®éªŒ"""
        self.experiments[experiment_id] = {
            "id": experiment_id,
            "config": config,
            "models": [],
            "results": [],
            "status": "created",
            "created_at": datetime.utcnow(),
            "metrics_history": []
        }
        
        return experiment_id
    
    def add_model_to_experiment(self, experiment_id: str, model_id: str, model_config: Dict[str, Any]):
        """æ·»åŠ æ¨¡å‹åˆ°å®éªŒ"""
        if experiment_id in self.experiments:
            self.experiments[experiment_id]["models"].append({
                "model_id": model_id,
                "config": model_config,
                "added_at": datetime.utcnow()
            })
    
    def record_experiment_result(self, experiment_id: str, model_id: str, metrics: Dict[str, Any]):
        """è®°å½•å®éªŒç»“æœ"""
        if experiment_id in self.experiments:
            result = {
                "model_id": model_id,
                "metrics": metrics,
                "timestamp": datetime.utcnow()
            }
            self.experiments[experiment_id]["results"].append(result)
            self.experiments[experiment_id]["metrics_history"].append(metrics)
    
    def get_experiment_summary(self, experiment_id: str) -> Dict[str, Any]:
        """è·å–å®éªŒæ‘˜è¦"""
        if experiment_id not in self.experiments:
            return {}
        
        experiment = self.experiments[experiment_id]
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        if experiment["results"]:
            all_scores = [result["metrics"].get("evaluation_score", 0) for result in experiment["results"]]
            best_score = max(all_scores)
            avg_score = np.mean(all_scores)
            best_model = max(experiment["results"], key=lambda x: x["metrics"].get("evaluation_score", 0))["model_id"]
        else:
            best_score = avg_score = 0
            best_model = None
        
        return {
            "experiment_id": experiment_id,
            "num_models": len(experiment["models"]),
            "num_results": len(experiment["results"]),
            "best_score": best_score,
            "avg_score": avg_score,
            "best_model": best_model,
            "duration": (datetime.utcnow() - experiment["created_at"]).total_seconds() / 3600,
            "status": experiment["status"]
        }

class EnhancedRLManager(UnifiedRLManager):
    """å¢å¼ºç‰ˆå¼ºåŒ–å­¦ä¹ ç®¡ç†å™¨ - 100%å®Œæ•´åº¦"""
    
    def __init__(self, base_model_dir: str = "models/rl_models"):
        super().__init__(base_model_dir)
        
        # === å¢å¼ºåŠŸèƒ½ç»„ä»¶ ===
        self.real_time_monitor = RealTimeMonitor()
        self.intelligent_ensemble = IntelligentEnsemble()
        self.online_learning_system = OnlineLearningSystem()
        self.distributed_training = DistributedTraining()
        self.experiment_manager = ExperimentManager()
        
        # === é«˜çº§é…ç½® ===
        self.config.update({
            "real_time_monitoring": True,
            "ensemble_decision": True,
            "online_learning": True,
            "distributed_training": False,
            "auto_model_selection": True,
            "performance_threshold": 0.8,
            "adaptation_sensitivity": 0.1
        })
        
        # === çŠ¶æ€ç®¡ç† ===
        self.is_enhanced_mode = False
        self.decision_cache = {}
        self.performance_tracker = {}
        
        self.logger.info("ğŸš€ å¢å¼ºç‰ˆRLç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def start_enhanced_mode(self):
        """å¯åŠ¨å¢å¼ºæ¨¡å¼"""
        if self.is_enhanced_mode:
            return
        
        self.is_enhanced_mode = True
        
        # å¯åŠ¨å®æ—¶ç›‘æ§
        if self.config.get("real_time_monitoring"):
            asyncio.create_task(self.real_time_monitor.start_monitoring())
        
        self.logger.info("âœ… å¢å¼ºæ¨¡å¼å·²å¯åŠ¨")
    
    async def make_intelligent_decision(self, state: np.ndarray, model_selection_strategy: str = "auto") -> Tuple[Any, Dict[str, Any]]:
        """æ™ºèƒ½å†³ç­–åˆ¶å®š"""
        if not self.is_enhanced_mode:
            # å›é€€åˆ°æ ‡å‡†å†³ç­–
            if self.loaded_models:
                model = next(iter(self.loaded_models.values()))
                action = model.select_action(state, training=False)
                return action, {"strategy": "single_model"}
            else:
                return 0, {"strategy": "fallback", "error": "no_models_loaded"}
        
        # æ£€æŸ¥ç¼“å­˜
        state_hash = hash(state.tobytes())
        if state_hash in self.decision_cache:
            cache_entry = self.decision_cache[state_hash]
            if (datetime.utcnow() - cache_entry["timestamp"]).seconds < 60:  # 1åˆ†é’Ÿç¼“å­˜
                return cache_entry["decision"], cache_entry["metadata"]
        
        # ä½¿ç”¨é›†æˆå†³ç­–
        if len(self.loaded_models) > 1 and self.config.get("ensemble_decision"):
            # å°†æ¨¡å‹æ·»åŠ åˆ°é›†æˆ
            for model_id, model in self.loaded_models.items():
                if model_id not in self.intelligent_ensemble.models:
                    self.intelligent_ensemble.add_model(model_id, model)
            
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
            if model_selection_strategy == "auto":
                # æ ¹æ®å†å²æ€§èƒ½é€‰æ‹©ç­–ç•¥
                if len(self.intelligent_ensemble.performance_history) > 0:
                    avg_confidence = np.mean([
                        np.mean(list(history)[-10:]) 
                        for history in self.intelligent_ensemble.performance_history.values() 
                        if history
                    ])
                    
                    if avg_confidence > 0.8:
                        strategy = "weighted_voting"
                    elif avg_confidence > 0.6:
                        strategy = "dynamic_selection"
                    else:
                        strategy = "bayesian_averaging"
                else:
                    strategy = "weighted_voting"
            else:
                strategy = model_selection_strategy
            
            decision, metadata = self.intelligent_ensemble.make_ensemble_decision(state, strategy)
        else:
            # å•æ¨¡å‹å†³ç­–
            if self.loaded_models:
                model_id = next(iter(self.loaded_models.keys()))
                model = self.loaded_models[model_id]
                decision = model.select_action(state, training=False)
                metadata = {"strategy": "single_model", "model_id": model_id}
            else:
                decision = 0
                metadata = {"strategy": "fallback", "error": "no_models_loaded"}
        
        # ç¼“å­˜å†³ç­–
        self.decision_cache[state_hash] = {
            "decision": decision,
            "metadata": metadata,
            "timestamp": datetime.utcnow()
        }
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_decision_cache()
        
        return decision, metadata
    
    def _cleanup_decision_cache(self):
        """æ¸…ç†å†³ç­–ç¼“å­˜"""
        current_time = datetime.utcnow()
        expired_keys = [
            key for key, value in self.decision_cache.items()
            if (current_time - value["timestamp"]).seconds > 300  # 5åˆ†é’Ÿè¿‡æœŸ
        ]
        
        for key in expired_keys:
            del self.decision_cache[key]
    
    async def adapt_to_new_data(self, new_experience: Dict[str, Any]):
        """é€‚åº”æ–°æ•°æ®"""
        if not self.config.get("online_learning"):
            return
        
        # åœ¨çº¿å­¦ä¹ 
        for model_id, model in self.loaded_models.items():
            await self.online_learning_system.continuous_learning(model, new_experience)
        
        # æ›´æ–°æ€§èƒ½è·Ÿè¸ª
        if "reward" in new_experience:
            self._update_performance_tracking(new_experience["reward"])
    
    def _update_performance_tracking(self, reward: float):
        """æ›´æ–°æ€§èƒ½è·Ÿè¸ª"""
        current_time = datetime.utcnow()
        
        if "rewards" not in self.performance_tracker:
            self.performance_tracker["rewards"] = deque(maxlen=1000)
            self.performance_tracker["timestamps"] = deque(maxlen=1000)
        
        self.performance_tracker["rewards"].append(reward)
        self.performance_tracker["timestamps"].append(current_time)
        
        # æ›´æ–°é›†æˆç³»ç»Ÿçš„æ€§èƒ½
        for model_id in self.loaded_models.keys():
            if model_id in self.intelligent_ensemble.models:
                self.intelligent_ensemble.update_performance(model_id, reward)
    
    async def start_distributed_training_job(self, model_config: Dict[str, Any], num_workers: int = 4) -> str:
        """å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡"""
        if not self.config.get("distributed_training"):
            raise ValueError("åˆ†å¸ƒå¼è®­ç»ƒæœªå¯ç”¨")
        
        job_id = await self.distributed_training.start_distributed_training(model_config, num_workers)
        self.logger.info(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨: {job_id}")
        return job_id
    
    def create_experiment(self, experiment_name: str, models: List[str], config: Dict[str, Any]) -> str:
        """åˆ›å»ºå¯¹æ¯”å®éªŒ"""
        experiment_id = f"{experiment_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        exp_id = self.experiment_manager.create_experiment(experiment_id, config)
        
        # æ·»åŠ æ¨¡å‹åˆ°å®éªŒ
        for model_name in models:
            model_config = self.registry.get_model_config(model_name)
            if model_config:
                self.experiment_manager.add_model_to_experiment(
                    exp_id, model_name, model_config.__dict__
                )
        
        self.logger.info(f"ğŸ“Š å®éªŒå·²åˆ›å»º: {experiment_id}")
        return exp_id
    
    async def run_ab_test(self, experiment_id: str, test_data: np.ndarray, num_episodes: int = 100) -> Dict[str, Any]:
        """è¿è¡ŒA/Bæµ‹è¯•"""
        if experiment_id not in self.experiment_manager.experiments:
            raise ValueError(f"å®éªŒä¸å­˜åœ¨: {experiment_id}")
        
        experiment = self.experiment_manager.experiments[experiment_id]
        results = {}
        
        # å¯¹æ¯ä¸ªæ¨¡å‹è¿è¡Œæµ‹è¯•
        for model_info in experiment["models"]:
            model_id = model_info["model_id"]
            
            if model_id not in self.loaded_models:
                self.load_model(model_id)
            
            model = self.loaded_models[model_id]
            
            # è¿è¡Œæµ‹è¯•
            episode_rewards = []
            for episode in range(num_episodes):
                # ç®€åŒ–çš„æµ‹è¯•é€»è¾‘
                state = test_data[episode % len(test_data)]
                action = model.select_action(state, training=False)
                # æ¨¡æ‹Ÿå¥–åŠ±
                reward = np.random.normal(0, 1)
                episode_rewards.append(reward)
            
            # è®°å½•ç»“æœ
            avg_reward = np.mean(episode_rewards)
            metrics = {
                "avg_reward": avg_reward,
                "std_reward": np.std(episode_rewards),
                "num_episodes": num_episodes,
                "evaluation_score": avg_reward
            }
            
            self.experiment_manager.record_experiment_result(experiment_id, model_id, metrics)
            results[model_id] = metrics
        
        return results
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        base_status = super().list_available_models() if hasattr(super(), 'list_available_models') else []
        
        status = {
            "enhanced_mode": self.is_enhanced_mode,
            "loaded_models": len(self.loaded_models),
            "ensemble_models": len(self.intelligent_ensemble.models),
            "active_experiments": len(self.experiment_manager.active_experiments),
            "total_experiments": len(self.experiment_manager.experiments),
            "decision_cache_size": len(self.decision_cache),
            "monitoring_active": self.real_time_monitor.is_monitoring,
            "online_learning_buffer": len(self.online_learning_system.experience_buffer),
            "distributed_jobs": len(self.distributed_training.training_jobs)
        }
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        if self.performance_tracker:
            recent_rewards = list(self.performance_tracker.get("rewards", []))
            if recent_rewards:
                status.update({
                    "avg_recent_performance": np.mean(recent_rewards[-100:]),
                    "performance_trend": "improving" if len(recent_rewards) > 50 and 
                        np.mean(recent_rewards[-25:]) > np.mean(recent_rewards[-50:-25]) else "stable"
                })
        
        return status
    
    def get_performance_dashboard(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ä»ªè¡¨æ¿æ•°æ®"""
        dashboard = {
            "system_metrics": {
                "uptime": "N/A",  # å¯ä»¥æ·»åŠ å¯åŠ¨æ—¶é—´è·Ÿè¸ª
                "total_decisions": len(self.decision_cache),
                "cache_hit_rate": 0.0,  # å¯ä»¥æ·»åŠ ç¼“å­˜å‘½ä¸­ç‡è·Ÿè¸ª
                "avg_response_time": 0.0  # å¯ä»¥æ·»åŠ å“åº”æ—¶é—´è·Ÿè¸ª
            },
            "model_performance": {},
            "ensemble_stats": {
                "active_ensembles": len(self.intelligent_ensemble.models),
                "avg_confidence": 0.0
            },
            "learning_stats": {
                "online_adaptations": 0,  # å¯ä»¥æ·»åŠ é€‚åº”æ¬¡æ•°è·Ÿè¸ª
                "buffer_utilization": len(self.online_learning_system.experience_buffer) / 50000
            }
        }
        
        # æ·»åŠ æ¨¡å‹æ€§èƒ½æ•°æ®
        for model_id in self.loaded_models.keys():
            if model_id in self.intelligent_ensemble.performance_history:
                history = list(self.intelligent_ensemble.performance_history[model_id])
                if history:
                    dashboard["model_performance"][model_id] = {
                        "recent_performance": np.mean(history[-10:]),
                        "performance_trend": np.mean(history[-5:]) - np.mean(history[-10:-5]) if len(history) >= 10 else 0,
                        "weight": self.intelligent_ensemble.weights.get(model_id, 1.0)
                    }
        
        return dashboard

# åˆ›å»ºå…¨å±€å¢å¼ºç‰ˆå®ä¾‹
enhanced_rl_manager = EnhancedRLManager()