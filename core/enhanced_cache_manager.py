"""
å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨
å®ç°å¤šçº§ç¼“å­˜ã€æ™ºèƒ½é¢„çƒ­ã€åˆ†å¸ƒå¼ç¼“å­˜åŒæ­¥
"""

import asyncio
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import redis.asyncio as aioredis
import pickle
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.unified_logger import get_logger, LogCategory

logger = get_logger()

class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«"""
    L1_MEMORY = "l1_memory"      # å†…å­˜ç¼“å­˜ (æœ€å¿«)
    L2_REDIS = "l2_redis"        # Redisç¼“å­˜ (å¿«é€Ÿ)
    L3_DATABASE = "l3_database"  # æ•°æ®åº“ç¼“å­˜ (è¾ƒæ…¢)

class CachePolicy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "lru"                  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"                  # æœ€ä¸å¸¸ç”¨
    TTL = "ttl"                  # åŸºäºæ—¶é—´è¿‡æœŸ
    WRITE_THROUGH = "write_through"    # å†™ç©¿é€
    WRITE_BACK = "write_back"          # å†™å›
    WRITE_AROUND = "write_around"      # å†™ç»•è¿‡

@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    hits: int = 0
    misses: int = 0
    writes: int = 0
    deletes: int = 0
    evictions: int = 0
    l1_hits: int = 0
    l2_hits: int = 0
    l3_hits: int = 0
    total_requests: int = 0
    avg_response_time_ms: float = 0.0
    
    def get_hit_rate(self) -> float:
        """è·å–å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return (self.hits / max(total, 1)) * 100
    
    def get_l1_hit_rate(self) -> float:
        """è·å–L1ç¼“å­˜å‘½ä¸­ç‡"""
        return (self.l1_hits / max(self.total_requests, 1)) * 100
    
    def get_l2_hit_rate(self) -> float:
        """è·å–L2ç¼“å­˜å‘½ä¸­ç‡"""
        return (self.l2_hits / max(self.total_requests, 1)) * 100

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    ttl: Optional[int] = None
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_accessed: datetime = field(default_factory=datetime.utcnow)
    access_count: int = 0
    level: CacheLevel = CacheLevel.L1_MEMORY
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if not self.ttl:
            return False
        return (datetime.utcnow() - self.created_at).total_seconds() > self.ttl
    
    def access(self):
        """è®°å½•è®¿é—®"""
        self.last_accessed = datetime.utcnow()
        self.access_count += 1

class L1MemoryCache:
    """L1å†…å­˜ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000, policy: CachePolicy = CachePolicy.LRU):
        self.max_size = max_size
        self.policy = policy
        self.cache: Dict[str, CacheEntry] = {}
        self.access_order: List[str] = []  # LRUé˜Ÿåˆ—
        self.stats = CacheStats()
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        start_time = time.time()
        
        try:
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥è¿‡æœŸ
                if entry.is_expired():
                    await self.delete(key)
                    self.stats.misses += 1
                    return None
                
                # æ›´æ–°è®¿é—®è®°å½•
                entry.access()
                if key in self.access_order:
                    self.access_order.remove(key)
                self.access_order.append(key)
                
                self.stats.hits += 1
                self.stats.l1_hits += 1
                return entry.value
            
            self.stats.misses += 1
            return None
            
        finally:
            response_time = (time.time() - start_time) * 1000
            self.stats.avg_response_time_ms = (
                (self.stats.avg_response_time_ms * self.stats.total_requests + response_time) /
                (self.stats.total_requests + 1)
            )
            self.stats.total_requests += 1
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            # æ£€æŸ¥å®¹é‡é™åˆ¶
            if len(self.cache) >= self.max_size and key not in self.cache:
                await self._evict()
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=key,
                value=value,
                ttl=ttl,
                level=CacheLevel.L1_MEMORY
            )
            
            self.cache[key] = entry
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)
            
            self.stats.writes += 1
            return True
            
        except Exception as e:
            logger.error(f"L1ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        try:
            if key in self.cache:
                del self.cache[key]
                if key in self.access_order:
                    self.access_order.remove(key)
                self.stats.deletes += 1
                return True
            return False
            
        except Exception as e:
            logger.error(f"L1ç¼“å­˜åˆ é™¤å¤±è´¥: {e}")
            return False
    
    async def _evict(self):
        """ç¼“å­˜æ·˜æ±°"""
        if not self.access_order:
            return
        
        try:
            if self.policy == CachePolicy.LRU:
                # æ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„
                evict_key = self.access_order[0]
            elif self.policy == CachePolicy.LFU:
                # æ·˜æ±°æœ€ä¸å¸¸ç”¨çš„
                evict_key = min(self.cache.keys(), 
                              key=lambda k: self.cache[k].access_count)
            else:
                # é»˜è®¤LRU
                evict_key = self.access_order[0]
            
            await self.delete(evict_key)
            self.stats.evictions += 1
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ·˜æ±°å¤±è´¥: {e}")
    
    def get_size(self) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        return len(self.cache)
    
    async def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_order.clear()

class SmartRedisCache:
    """æ™ºèƒ½Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: aioredis.Redis):
        self.redis = redis_client
        self.stats = CacheStats()
        
        # ç¼“å­˜é…ç½®
        self.default_ttl = 300  # é»˜è®¤5åˆ†é’Ÿ
        self.market_data_ttl = 60    # å¸‚åœºæ•°æ®1åˆ†é’Ÿ
        self.ai_analysis_ttl = 1800  # AIåˆ†æ30åˆ†é’Ÿ
        self.static_data_ttl = 3600  # é™æ€æ•°æ®1å°æ—¶
        
        # é”®å‰ç¼€
        self.prefixes = {
            "market": "market:",
            "ai": "ai:",
            "config": "config:",
            "cache_meta": "meta:",
            "stats": "stats:"
        }
    
    def _get_key_info(self, key: str) -> Tuple[str, int]:
        """è·å–é”®ä¿¡æ¯å’ŒTTL"""
        if key.startswith(self.prefixes["market"]):
            return key, self.market_data_ttl
        elif key.startswith(self.prefixes["ai"]):
            return key, self.ai_analysis_ttl
        elif key.startswith(self.prefixes["config"]):
            return key, self.static_data_ttl
        else:
            return key, self.default_ttl
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        start_time = time.time()
        
        try:
            # å°è¯•è·å–JSONæ ¼å¼æ•°æ®
            value = await self.redis.get(key)
            
            if value is None:
                self.stats.misses += 1
                return None
            
            # å°è¯•JSONè§£ç 
            try:
                data = json.loads(value.decode() if isinstance(value, bytes) else value)
            except (json.JSONDecodeError, UnicodeDecodeError):
                # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•pickle
                try:
                    data = pickle.loads(value)
                except:
                    data = value.decode() if isinstance(value, bytes) else value
            
            self.stats.hits += 1
            self.stats.l2_hits += 1
            return data
            
        except Exception as e:
            logger.error(f"Redisè·å–å¤±è´¥ {key}: {e}")
            self.stats.misses += 1
            return None
            
        finally:
            response_time = (time.time() - start_time) * 1000
            self.stats.avg_response_time_ms = (
                (self.stats.avg_response_time_ms * self.stats.total_requests + response_time) /
                (self.stats.total_requests + 1)
            )
            self.stats.total_requests += 1
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            _, default_ttl = self._get_key_info(key)
            expire_time = ttl or default_ttl
            
            # æ™ºèƒ½åºåˆ—åŒ–
            if isinstance(value, (dict, list)):
                serialized = json.dumps(value, default=str)
            elif isinstance(value, (str, int, float, bool)):
                serialized = json.dumps(value)
            else:
                # å¤æ‚å¯¹è±¡ä½¿ç”¨pickle
                serialized = pickle.dumps(value)
            
            # è®¾ç½®ç¼“å­˜
            success = await self.redis.set(key, serialized, ex=expire_time)
            
            if success:
                self.stats.writes += 1
                
                # æ›´æ–°å…ƒæ•°æ®
                await self._update_cache_metadata(key, expire_time)
            
            return bool(success)
            
        except Exception as e:
            logger.error(f"Redisè®¾ç½®å¤±è´¥ {key}: {e}")
            return False
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        try:
            result = await self.redis.delete(key)
            if result > 0:
                self.stats.deletes += 1
                await self._remove_cache_metadata(key)
                return True
            return False
            
        except Exception as e:
            logger.error(f"Redisåˆ é™¤å¤±è´¥ {key}: {e}")
            return False
    
    async def mget(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–"""
        try:
            if not keys:
                return {}
            
            values = await self.redis.mget(keys)
            result = {}
            
            for i, (key, value) in enumerate(zip(keys, values)):
                if value is not None:
                    try:
                        result[key] = json.loads(value.decode() if isinstance(value, bytes) else value)
                        self.stats.hits += 1
                    except:
                        try:
                            result[key] = pickle.loads(value)
                            self.stats.hits += 1
                        except:
                            result[key] = value.decode() if isinstance(value, bytes) else value
                            self.stats.hits += 1
                else:
                    self.stats.misses += 1
            
            self.stats.total_requests += len(keys)
            return result
            
        except Exception as e:
            logger.error(f"Redisæ‰¹é‡è·å–å¤±è´¥: {e}")
            self.stats.misses += len(keys)
            return {}
    
    async def mset(self, data: Dict[str, Any], ttl: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½®"""
        try:
            if not data:
                return True
            
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            pipe = self.redis.pipeline()
            
            for key, value in data.items():
                _, default_ttl = self._get_key_info(key)
                expire_time = ttl or default_ttl
                
                # åºåˆ—åŒ–
                if isinstance(value, (dict, list)):
                    serialized = json.dumps(value, default=str)
                elif isinstance(value, (str, int, float, bool)):
                    serialized = json.dumps(value)
                else:
                    serialized = pickle.dumps(value)
                
                pipe.set(key, serialized, ex=expire_time)
            
            results = await pipe.execute()
            success_count = sum(1 for r in results if r)
            
            self.stats.writes += success_count
            return success_count == len(data)
            
        except Exception as e:
            logger.error(f"Redisæ‰¹é‡è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def _update_cache_metadata(self, key: str, ttl: int):
        """æ›´æ–°ç¼“å­˜å…ƒæ•°æ®"""
        try:
            metadata = {
                "created_at": datetime.utcnow().isoformat(),
                "ttl": ttl,
                "access_count": 0
            }
            
            meta_key = f"{self.prefixes['cache_meta']}{key}"
            await self.redis.hset(meta_key, mapping=metadata)
            await self.redis.expire(meta_key, ttl + 60)  # å…ƒæ•°æ®å­˜æ´»æ—¶é—´æ¯”æ•°æ®é•¿ä¸€ç‚¹
            
        except Exception as e:
            logger.debug(f"æ›´æ–°ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    async def _remove_cache_metadata(self, key: str):
        """ç§»é™¤ç¼“å­˜å…ƒæ•°æ®"""
        try:
            meta_key = f"{self.prefixes['cache_meta']}{key}"
            await self.redis.delete(meta_key)
        except Exception as e:
            logger.debug(f"ç§»é™¤ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    async def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        try:
            info = await self.redis.info()
            
            return {
                "redis_version": info.get("redis_version"),
                "used_memory": info.get("used_memory_human"),
                "connected_clients": info.get("connected_clients"),
                "keyspace_hits": info.get("keyspace_hits", 0),
                "keyspace_misses": info.get("keyspace_misses", 0),
                "hit_rate": self.stats.get_hit_rate(),
                "total_keys": await self._count_keys(),
                "avg_response_time_ms": self.stats.avg_response_time_ms
            }
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def _count_keys(self) -> int:
        """ç»Ÿè®¡é”®æ•°é‡"""
        try:
            # åˆ†åˆ«ç»Ÿè®¡å„ç±»å‹é”®çš„æ•°é‡
            total = 0
            for prefix in self.prefixes.values():
                keys = await self.redis.keys(f"{prefix}*")
                total += len(keys)
            return total
        except:
            return 0

class EnhancedCacheManager:
    """å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: aioredis.Redis):
        self.l1_cache = L1MemoryCache(max_size=5000)  # å¢åŠ L1ç¼“å­˜å¤§å°
        self.l2_cache = SmartRedisCache(redis_client)
        self.stats = CacheStats()
        self.cache = self  # ä¸ºäº†ä¿æŒå‘åå…¼å®¹
        
        # ç¼“å­˜é¢„çƒ­é…ç½®
        self.preload_enabled = True
        self.preload_patterns = [
            "market:*",
            "ai:sentiment:*",
            "config:*"
        ]
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–å¢å¼ºç¼“å­˜ç®¡ç†å™¨"""
        if self._initialized:
            return
            
        try:
            # åˆå§‹åŒ–L2ç¼“å­˜è¿æ¥æµ‹è¯•
            await self.l2_cache.redis.ping()
            
            # å¯åŠ¨ç›‘æ§ä»»åŠ¡
            self._monitoring_task = None  # ç›‘æ§ä»»åŠ¡å ä½ç¬¦
            
            self._initialized = True
            logger.info("âœ… å¢å¼ºç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def start_monitoring(self):
        """å¯åŠ¨ç¼“å­˜ç›‘æ§"""
        try:
            # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            if not hasattr(self, '_monitoring_task') or not self._monitoring_task:
                logger.info("ğŸ“Š ç¼“å­˜ç›‘æ§ä»»åŠ¡å·²å‡†å¤‡å°±ç»ª")
                # è¿™é‡Œå¯ä»¥æ·»åŠ åå°ç›‘æ§é€»è¾‘ï¼Œæš‚æ—¶ç®€åŒ–
                self._monitoring_task = True
        except Exception as e:
            logger.warning(f"âš ï¸ ç›‘æ§å¯åŠ¨å¤±è´¥: {e}")
    
    async def get(self, key: str) -> Optional[Any]:
        """å¤šçº§ç¼“å­˜è·å–"""
        start_time = time.time()
        
        try:
            # L1å†…å­˜ç¼“å­˜
            value = await self.l1_cache.get(key)
            if value is not None:
                return value
            
            # L2 Redisç¼“å­˜
            value = await self.l2_cache.get(key)
            if value is not None:
                # å›å¡«åˆ°L1ç¼“å­˜
                await self.l1_cache.set(key, value, ttl=60)  # L1ç¼“å­˜1åˆ†é’Ÿ
                return value
            
            return None
            
        finally:
            self._update_global_stats(start_time)
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """å¤šçº§ç¼“å­˜è®¾ç½®"""
        start_time = time.time()
        
        try:
            # åŒæ—¶å†™å…¥L1å’ŒL2
            l1_success = await self.l1_cache.set(key, value, min(ttl or 300, 300))
            l2_success = await self.l2_cache.set(key, value, ttl)
            
            return l1_success and l2_success
            
        finally:
            self._update_global_stats(start_time)
    
    async def delete(self, key: str) -> bool:
        """å¤šçº§ç¼“å­˜åˆ é™¤"""
        l1_success = await self.l1_cache.delete(key)
        l2_success = await self.l2_cache.delete(key)
        return l1_success or l2_success
    
    async def mget(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–"""
        if not keys:
            return {}
        
        result = {}
        missing_keys = []
        
        # å…ˆä»L1ç¼“å­˜è·å–
        for key in keys:
            value = await self.l1_cache.get(key)
            if value is not None:
                result[key] = value
            else:
                missing_keys.append(key)
        
        # ä»L2ç¼“å­˜è·å–å‰©ä½™çš„é”®
        if missing_keys:
            l2_results = await self.l2_cache.mget(missing_keys)
            
            # å›å¡«åˆ°L1ç¼“å­˜
            for key, value in l2_results.items():
                result[key] = value
                await self.l1_cache.set(key, value, ttl=60)
        
        return result
    
    async def mset(self, data: Dict[str, Any], ttl: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½®"""
        if not data:
            return True
        
        # åŒæ—¶å†™å…¥L1å’ŒL2
        l1_tasks = [self.l1_cache.set(k, v, min(ttl or 300, 300)) for k, v in data.items()]
        l2_success = await self.l2_cache.mset(data, ttl)
        
        l1_results = await asyncio.gather(*l1_tasks, return_exceptions=True)
        l1_success = all(r is True for r in l1_results if not isinstance(r, Exception))
        
        return l1_success and l2_success
    
    async def preload_cache(self):
        """ç¼“å­˜é¢„çƒ­"""
        if not self.preload_enabled:
            return
        
        logger.info("ğŸ”¥ å¼€å§‹ç¼“å­˜é¢„çƒ­...")
        
        try:
            # é¢„çƒ­å¸‚åœºæ•°æ®
            await self._preload_market_data()
            
            # é¢„çƒ­é…ç½®æ•°æ®
            await self._preload_config_data()
            
            # é¢„çƒ­AIåˆ†ææ•°æ®
            await self._preload_ai_data()
            
            logger.info("âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")
    
    async def _preload_market_data(self):
        """é¢„çƒ­å¸‚åœºæ•°æ®"""
        try:
            # é¢„è®¾ä¸€äº›å¸¸ç”¨çš„å¸‚åœºæ•°æ®é”®
            common_symbols = ["BTC/USDT", "ETH/USDT"]
            exchanges = ["OKX", "Binance"]
            
            for symbol in common_symbols:
                for exchange in exchanges:
                    key = f"market:{exchange}:{symbol.replace('/', '')}"
                    # å¦‚æœRedisä¸­æœ‰æ•°æ®ä½†L1ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œè¿›è¡Œé¢„çƒ­
                    if await self.l2_cache.get(key) is not None:
                        value = await self.l2_cache.get(key)
                        await self.l1_cache.set(key, value, ttl=60)
                        
        except Exception as e:
            logger.debug(f"å¸‚åœºæ•°æ®é¢„çƒ­å¤±è´¥: {e}")
    
    async def _preload_config_data(self):
        """é¢„çƒ­é…ç½®æ•°æ®"""
        try:
            config_keys = [
                "config:trading_pairs",
                "config:api_settings",
                "config:risk_limits"
            ]
            
            for key in config_keys:
                value = await self.l2_cache.get(key)
                if value is not None:
                    await self.l1_cache.set(key, value, ttl=300)
                    
        except Exception as e:
            logger.debug(f"é…ç½®æ•°æ®é¢„çƒ­å¤±è´¥: {e}")
    
    async def _preload_ai_data(self):
        """é¢„çƒ­AIæ•°æ®"""
        try:
            ai_keys = [
                "ai:sentiment:latest",
                "ai:composite_signal:latest",
                "ai:market_regime:latest"
            ]
            
            for key in ai_keys:
                value = await self.l2_cache.get(key)
                if value is not None:
                    await self.l1_cache.set(key, value, ttl=180)
                    
        except Exception as e:
            logger.debug(f"AIæ•°æ®é¢„çƒ­å¤±è´¥: {e}")
    
    def _update_global_stats(self, start_time: float):
        """æ›´æ–°å…¨å±€ç»Ÿè®¡"""
        response_time = (time.time() - start_time) * 1000
        self.stats.avg_response_time_ms = (
            (self.stats.avg_response_time_ms * self.stats.total_requests + response_time) /
            (self.stats.total_requests + 1)
        )
        self.stats.total_requests += 1
        
        # åˆå¹¶L1å’ŒL2çš„ç»Ÿè®¡
        self.stats.l1_hits = self.l1_cache.stats.l1_hits
        self.stats.l2_hits = self.l2_cache.stats.l2_hits
        self.stats.hits = self.l1_cache.stats.hits + self.l2_cache.stats.hits
        self.stats.misses = self.l1_cache.stats.misses + self.l2_cache.stats.misses
    
    async def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç¼“å­˜ç»Ÿè®¡"""
        cache_info = await self.l2_cache.get_cache_info()
        
        return {
            "overall": {
                "total_requests": self.stats.total_requests,
                "hit_rate": self.stats.get_hit_rate(),
                "avg_response_time_ms": self.stats.avg_response_time_ms
            },
            "l1_memory": {
                "size": self.l1_cache.get_size(),
                "max_size": self.l1_cache.max_size,
                "hit_rate": self.stats.get_l1_hit_rate(),
                "evictions": self.l1_cache.stats.evictions
            },
            "l2_redis": {
                "hit_rate": self.stats.get_l2_hit_rate(),
                "used_memory": cache_info.get("used_memory", "N/A"),
                "connected_clients": cache_info.get("connected_clients", 0),
                "total_keys": cache_info.get("total_keys", 0)
            },
            "performance": {
                "l1_response_time_ms": self.l1_cache.stats.avg_response_time_ms,
                "l2_response_time_ms": self.l2_cache.stats.avg_response_time_ms
            }
        }
    
    async def clear_all_caches(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        await self.l1_cache.clear()
        # Redisæ¸…ç©ºéœ€è¦è°¨æ…ï¼Œè¿™é‡Œåªæ¸…ç©ºæˆ‘ä»¬çš„é”®
        for prefix in self.l2_cache.prefixes.values():
            keys = await self.l2_cache.redis.keys(f"{prefix}*")
            if keys:
                await self.l2_cache.redis.delete(*keys)

# å¸‚åœºæ•°æ®ä¸“ç”¨ç¼“å­˜æ¥å£
class MarketDataCache:
    """å¸‚åœºæ•°æ®ä¸“ç”¨ç¼“å­˜æ¥å£"""
    
    def __init__(self, cache_manager: EnhancedCacheManager):
        self.cache = cache_manager
    
    async def set_market_data(self, exchange: str, symbol: str, data: Dict[str, Any]):
        """è®¾ç½®å¸‚åœºæ•°æ®"""
        key = f"market:{exchange}:{symbol.replace('/', '')}"
        return await self.cache.set(key, data, ttl=60)  # 1åˆ†é’Ÿè¿‡æœŸ
    
    async def get_market_data(self, exchange: str, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å–å¸‚åœºæ•°æ®"""
        key = f"market:{exchange}:{symbol.replace('/', '')}"
        return await self.cache.get(key)
    
    async def get_latest_prices(self, symbols: List[str] = None) -> Dict[str, Dict[str, Any]]:
        """è·å–æœ€æ–°ä»·æ ¼"""
        symbols = symbols or ["BTC/USDT", "ETH/USDT"]
        exchanges = ["OKX", "Binance"]
        
        keys = []
        for symbol in symbols:
            for exchange in exchanges:
                key = f"market:{exchange}:{symbol.replace('/', '')}"
                keys.append(key)
        
        cached_data = await self.cache.mget(keys)
        
        # é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„
        result = {}
        for symbol in symbols:
            result[symbol] = {}
            for exchange in exchanges:
                key = f"market:{exchange}:{symbol.replace('/', '')}"
                result[symbol][exchange.lower()] = cached_data.get(key)
        
        return result

# åˆ›å»ºå…¨å±€å¢å¼ºç¼“å­˜ç®¡ç†å™¨å®ä¾‹çš„å·¥å‚å‡½æ•°
async def create_enhanced_cache_manager(redis_client: aioredis.Redis) -> EnhancedCacheManager:
    """åˆ›å»ºå¢å¼ºç¼“å­˜ç®¡ç†å™¨"""
    cache_manager = EnhancedCacheManager(redis_client)
    await cache_manager.initialize()
    return cache_manager