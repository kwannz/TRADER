"""
çœŸå®æ•°æ®ç®¡ç†å™¨
é›†æˆOKXå’ŒBinanceçš„å®æ—¶æ•°æ®æµå’Œå†å²æ•°æ®
æ›¿ä»£æ¨¡æ‹Ÿæ•°æ®ç³»ç»Ÿ
"""

import asyncio
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from collections import defaultdict
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.unified_logger import get_logger, LogCategory
from .websocket_client import OKXWebSocketClient, BinanceWebSocketClient, WebSocketManager
from .enhanced_websocket_client import enhanced_binance_client
from .historical_data_fetcher import HistoricalDataManager
from .data_manager import data_manager
from .coinglass_collector import coinglass_collector_manager
from .coinglass_analyzer import coinglass_analyzer
from .data_validator import market_data_validator, ValidationLevel
from .optimized_data_pipeline import optimized_pipeline

logger = get_logger()

class RealDataManager:
    """çœŸå®æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.websocket_manager = WebSocketManager()
        self.historical_manager = HistoricalDataManager()
        
        # äº¤æ˜“å¯¹é…ç½®
        self.trading_pairs = {
            "BTC/USDT": {"okx": "BTC-USDT", "binance": "BTCUSDT"},
            "ETH/USDT": {"okx": "ETH-USDT", "binance": "ETHUSDT"}
        }
        
        # æ•°æ®å›è°ƒ
        self.tick_callbacks: List[Callable] = []
        self.candle_callbacks: List[Callable] = []
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        
        # Coinglassç›¸å…³
        self.coinglass_enabled = bool(os.getenv("COINGLASS_API_KEY", ""))
        self.coinglass_update_interval = int(os.getenv("COINGLASS_UPDATE_INTERVAL", "300"))
        
    async def initialize(self):
        """åˆå§‹åŒ–çœŸå®æ•°æ®ç®¡ç†å™¨"""
        try:
            logger.info("åˆå§‹åŒ–çœŸå®æ•°æ®ç®¡ç†å™¨...")
            
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if not data_manager._initialized:
                await data_manager.initialize()
            
            # åˆå§‹åŒ–ä¼˜åŒ–æ•°æ®æµæ°´çº¿
            await optimized_pipeline.start()
            logger.info("âœ… ä¼˜åŒ–æ•°æ®æµæ°´çº¿å·²å¯åŠ¨")
            
            # æ³¨å†Œæ•°æ®æµæ°´çº¿å›è°ƒ
            optimized_pipeline.register_callback("market_data", self._save_market_data_batch)
            optimized_pipeline.register_callback("candle_data", self._save_candle_data_batch)
            optimized_pipeline.register_callback("coinglass_data", self._save_coinglass_data_batch)
            
            # åˆ›å»ºWebSocketå®¢æˆ·ç«¯
            okx_client = OKXWebSocketClient()
            
            # ä½¿ç”¨å¢å¼ºç‰ˆBinanceå®¢æˆ·ç«¯
            binance_client = enhanced_binance_client
            
            # è®¾ç½®ä¼˜åŒ–å›è°ƒï¼ˆä½¿ç”¨æµæ°´çº¿ï¼‰
            okx_client.add_callback("ticker", self._handle_ticker_data_optimized)
            okx_client.add_callback("candle", self._handle_candle_data_optimized)
            binance_client.add_callback("ticker", self._handle_ticker_data_optimized)
            binance_client.add_callback("candle", self._handle_candle_data_optimized)
            
            # æ·»åŠ åˆ°ç®¡ç†å™¨
            self.websocket_manager.add_client("okx", okx_client)
            self.websocket_manager.add_client("binance", binance_client)
            
            # åˆå§‹åŒ–Coinglassåˆ†æå™¨
            if self.coinglass_enabled:
                await coinglass_analyzer.initialize()
                logger.info("Coinglassåˆ†æå™¨å·²åˆå§‹åŒ–")
            
            logger.info("çœŸå®æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"çœŸå®æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def start_real_data_stream(self):
        """å¯åŠ¨çœŸå®æ•°æ®æµ"""
        if self.is_running:
            logger.warning("çœŸå®æ•°æ®æµå·²åœ¨è¿è¡Œ")
            return
            
        try:
            self.is_running = True
            logger.info("å¯åŠ¨çœŸå®æ•°æ®æµ...")
            
            # è¿æ¥åˆ°WebSocket
            okx_client = self.websocket_manager.get_client("okx")
            binance_client = self.websocket_manager.get_client("binance")
            
            if okx_client:
                # è®¢é˜…OKXæ•°æ®
                okx_symbols = [pair["okx"] for pair in self.trading_pairs.values()]
                await okx_client.connect()
                await okx_client.subscribe_tickers(okx_symbols)
                await okx_client.subscribe_candles(okx_symbols, "1m")
                logger.info(f"å·²è®¢é˜…OKXæ•°æ®æµ: {okx_symbols}")
            
            if binance_client:
                # è®¢é˜…Binanceæ•°æ®
                binance_symbols = [pair["binance"] for pair in self.trading_pairs.values()]
                await binance_client.connect()
                await binance_client.subscribe_tickers(binance_symbols)
                await binance_client.subscribe_candles(binance_symbols, "1m")
                logger.info(f"å·²è®¢é˜…Binanceæ•°æ®æµ: {binance_symbols}")
            
            # å¯åŠ¨WebSocketç›‘å¬
            await self.websocket_manager.start_all()
            
            # å¯åŠ¨Coinglassæ•°æ®æ”¶é›†
            if self.coinglass_enabled:
                await coinglass_collector_manager.start_collection()
                logger.info("Coinglassæ•°æ®æ”¶é›†å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨çœŸå®æ•°æ®æµå¤±è´¥: {e}")
            self.is_running = False
            raise
    
    async def stop_real_data_stream(self):
        """åœæ­¢çœŸå®æ•°æ®æµ"""
        try:
            self.is_running = False
            await self.websocket_manager.stop_all()
            
            # åœæ­¢Coinglassæ•°æ®æ”¶é›†
            if self.coinglass_enabled:
                await coinglass_collector_manager.stop_collection()
                logger.info("Coinglassæ•°æ®æ”¶é›†å·²åœæ­¢")
            
            logger.info("çœŸå®æ•°æ®æµå·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢çœŸå®æ•°æ®æµå¤±è´¥: {e}")
    
    async def fetch_historical_training_data(self, days_back: int = 30):
        """è·å–å†å²è®­ç»ƒæ•°æ®"""
        try:
            logger.info(f"å¼€å§‹è·å–{days_back}å¤©å†å²æ•°æ®ç”¨äºAIè®­ç»ƒ...")
            
            # è·å–ä¸»è¦äº¤æ˜“å¯¹çš„å†å²æ•°æ®
            symbols = list(self.trading_pairs.keys())
            exchanges = ["okx", "binance"]
            timeframes = ["1m", "5m", "15m", "1h", "4h", "1d"]
            
            training_data = await self.historical_manager.fetch_training_data(
                symbols=symbols,
                exchanges=exchanges,
                timeframes=timeframes,
                days_back=days_back
            )
            
            # åˆ›å»ºè®­ç»ƒæ•°æ®é›†æ–‡ä»¶
            dataset_file = await self.historical_manager.create_training_dataset(
                symbols=symbols,
                target_file="data/btc_eth_training_dataset.csv"
            )
            
            logger.info(f"å†å²æ•°æ®è·å–å®Œæˆï¼Œè®­ç»ƒæ•°æ®é›†ä¿å­˜è‡³: {dataset_file}")
            return training_data
            
        except Exception as e:
            logger.error(f"è·å–å†å²è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            raise
    
    async def _handle_ticker_data(self, ticker_data: Dict):
        """å¤„ç†è¡Œæƒ…æ•°æ®"""
        try:
            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            standardized_data = {
                "symbol": ticker_data.get("symbol", ""),
                "exchange": ticker_data.get("exchange", ""),
                "price": ticker_data.get("price", 0),
                "volume_24h": ticker_data.get("volume_24h", 0),
                "change_24h": ticker_data.get("change_24h", 0),
                "change_24h_pct": ticker_data.get("change_24h_pct", 0),
                "high_24h": ticker_data.get("high_24h", 0),
                "low_24h": ticker_data.get("low_24h", 0),
                "timestamp": ticker_data.get("timestamp", int(datetime.utcnow().timestamp() * 1000)),
                "source": "real_data"
            }
            
            # æ•°æ®éªŒè¯
            validation_result = await market_data_validator.validate_market_data(
                standardized_data, 
                data_type="ticker", 
                level=ValidationLevel.STANDARD
            )
            
            # è®°å½•éªŒè¯ç»“æœ
            if not validation_result.is_valid:
                logger.warning(f"âš ï¸ è¡Œæƒ…æ•°æ®éªŒè¯å¤±è´¥ {standardized_data.get('symbol', 'UNKNOWN')}: {validation_result.issues}")
                return
            elif validation_result.warnings:
                logger.debug(f"ğŸ“Š è¡Œæƒ…æ•°æ®è´¨é‡è­¦å‘Š {standardized_data.get('symbol', 'UNKNOWN')}: {validation_result.warnings}")
            
            # æ·»åŠ è´¨é‡åˆ†æ•°åˆ°æ•°æ®ä¸­
            standardized_data["data_quality_score"] = validation_result.quality_score
            standardized_data["data_quality_level"] = validation_result.quality_level.value
            
            # é€šçŸ¥å›è°ƒå‡½æ•°
            for callback in self.tick_callbacks:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(standardized_data)
                    else:
                        callback(standardized_data)
                except Exception as e:
                    logger.error(f"Tickerå›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"å¤„ç†è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
    
    async def _handle_candle_data(self, symbol: str, candle_data: List[Dict]):
        """å¤„ç†Kçº¿æ•°æ®"""
        try:
            # æ•°æ®éªŒè¯
            validation_result = await market_data_validator.validate_market_data(
                candle_data, 
                data_type="candle", 
                level=ValidationLevel.STANDARD
            )
            
            # è®°å½•éªŒè¯ç»“æœ
            if not validation_result.is_valid:
                logger.warning(f"âš ï¸ Kçº¿æ•°æ®éªŒè¯å¤±è´¥ {symbol}: {validation_result.issues}")
                return
            elif validation_result.warnings:
                logger.debug(f"ğŸ“Š Kçº¿æ•°æ®è´¨é‡è­¦å‘Š {symbol}: {validation_result.warnings}")
            
            # ä¸ºæ¯ä¸ªKçº¿æ·»åŠ è´¨é‡åˆ†æ•°
            validated_candles = []
            for i, candle in enumerate(candle_data):
                enhanced_candle = candle.copy()
                enhanced_candle["data_quality_score"] = validation_result.quality_score
                enhanced_candle["data_quality_level"] = validation_result.quality_level.value
                enhanced_candle["symbol"] = symbol
                validated_candles.append(enhanced_candle)
            
            # é€šçŸ¥å›è°ƒå‡½æ•°
            for callback in self.candle_callbacks:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(symbol, validated_candles)
                    else:
                        callback(symbol, validated_candles)
                except Exception as e:
                    logger.error(f"Kçº¿å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"å¤„ç†Kçº¿æ•°æ®å¤±è´¥: {e}")
    
    def add_tick_callback(self, callback: Callable):
        """æ·»åŠ è¡Œæƒ…æ•°æ®å›è°ƒ"""
        self.tick_callbacks.append(callback)
        
    def add_candle_callback(self, callback: Callable):
        """æ·»åŠ Kçº¿æ•°æ®å›è°ƒ"""
        self.candle_callbacks.append(callback)
    
    def get_connection_status(self) -> Dict[str, bool]:
        """è·å–è¿æ¥çŠ¶æ€"""
        return self.websocket_manager.get_connection_status()
    
    async def get_latest_prices(self) -> Dict[str, Any]:
        """è·å–æœ€æ–°ä»·æ ¼"""
        try:
            prices = {}
            
            for symbol in self.trading_pairs:
                # ä»å¢å¼ºç¼“å­˜ç®¡ç†å™¨è·å–æœ€æ–°ä»·æ ¼
                if data_manager.cache_manager:
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºç¼“å­˜ç®¡ç†å™¨
                    if hasattr(data_manager.cache_manager, 'cache') and hasattr(data_manager.cache_manager.cache, 'l1_cache'):
                        # ä½¿ç”¨å¢å¼ºç¼“å­˜ç®¡ç†å™¨çš„MarketDataCacheæ¥å£
                        from .enhanced_cache_manager import MarketDataCache
                        market_cache = MarketDataCache(data_manager.cache_manager)
                        
                        okx_data = await market_cache.get_market_data("OKX", self.trading_pairs[symbol]['okx'])
                        binance_data = await market_cache.get_market_data("Binance", self.trading_pairs[symbol]['binance'])
                    else:
                        # ä½¿ç”¨ä¼ ç»Ÿç¼“å­˜æ–¹æ³•
                        okx_data = await data_manager.cache_manager.get_market_data(f"OKX:{self.trading_pairs[symbol]['okx']}")
                        binance_data = await data_manager.cache_manager.get_market_data(f"Binance:{self.trading_pairs[symbol]['binance']}")
                    
                    prices[symbol] = {
                        "okx": okx_data,
                        "binance": binance_data
                    }
            
            return prices
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°ä»·æ ¼å¤±è´¥: {e}")
            return {}
    
    async def _handle_ticker_data_optimized(self, ticker_data: Dict):
        """ä¼˜åŒ–ç‰ˆè¡Œæƒ…æ•°æ®å¤„ç†å™¨"""
        try:
            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            standardized_data = {
                "symbol": ticker_data.get("symbol", ""),
                "exchange": ticker_data.get("exchange", ""),
                "price": ticker_data.get("price", 0),
                "volume_24h": ticker_data.get("volume_24h", 0),
                "change_24h": ticker_data.get("change_24h", 0),
                "change_24h_pct": ticker_data.get("change_24h_pct", 0),
                "high_24h": ticker_data.get("high_24h", 0),
                "low_24h": ticker_data.get("low_24h", 0),
                "timestamp": ticker_data.get("timestamp", int(datetime.utcnow().timestamp() * 1000)),
                "source": "real_data"
            }
            
            # ç›´æ¥å‘é€åˆ°ä¼˜åŒ–æµæ°´çº¿ï¼ˆå†…éƒ¨åŒ…å«éªŒè¯é€»è¾‘ï¼‰
            await optimized_pipeline.add_data("market_data", standardized_data)
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–è¡Œæƒ…æ•°æ®å¤„ç†å¤±è´¥: {e}")
    
    async def _handle_candle_data_optimized(self, candle_data: Dict):
        """ä¼˜åŒ–ç‰ˆKçº¿æ•°æ®å¤„ç†å™¨"""
        try:
            # æ ‡å‡†åŒ–Kçº¿æ•°æ®æ ¼å¼
            standardized_data = {
                "symbol": candle_data.get("symbol", ""),
                "exchange": candle_data.get("exchange", ""),
                "timeframe": candle_data.get("timeframe", "1m"),
                "timestamp": candle_data.get("timestamp", int(datetime.utcnow().timestamp() * 1000)),
                "open": candle_data.get("open", 0),
                "high": candle_data.get("high", 0),
                "low": candle_data.get("low", 0),
                "close": candle_data.get("close", 0),
                "volume": candle_data.get("volume", 0),
                "source": "real_data"
            }
            
            # å‘é€åˆ°ä¼˜åŒ–æµæ°´çº¿
            await optimized_pipeline.add_data("candle_data", standardized_data)
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–Kçº¿æ•°æ®å¤„ç†å¤±è´¥: {e}")
    
    async def _save_market_data_batch(self, processed_data: List[Dict]):
        """æ‰¹é‡ä¿å­˜å¸‚åœºæ•°æ®"""
        try:
            # åˆ†ç±»æ•°æ®å¹¶æ‰¹é‡å­˜å‚¨åˆ°ç¼“å­˜
            if data_manager.cache_manager:
                # æ‰¹é‡ç¼“å­˜æ“ä½œ
                cache_data = {}
                for data in processed_data:
                    symbol = data.get("symbol", "")
                    exchange = data.get("exchange", "")
                    if symbol and exchange:
                        cache_key = f"market:{exchange}:{symbol}"
                        cache_data[cache_key] = {
                            "price": data.get("price", 0),
                            "volume_24h": data.get("volume_24h", 0),
                            "change_24h_pct": data.get("change_24h_pct", 0),
                            "timestamp": data.get("timestamp", 0)
                        }
                
                if cache_data:
                    await data_manager.cache_manager.mset(cache_data, ttl=60)
                    logger.debug(f"æ‰¹é‡ç¼“å­˜å¸‚åœºæ•°æ®: {len(cache_data)}æ¡")
            
            # æ‰§è¡ŒåŸæœ‰å›è°ƒ
            for callback in self.tick_callbacks:
                try:
                    for data in processed_data:
                        await callback(data)
                except Exception as e:
                    logger.error(f"å¸‚åœºæ•°æ®å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜å¸‚åœºæ•°æ®å¤±è´¥: {e}")
    
    async def _save_candle_data_batch(self, processed_data: List[Dict]):
        """æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®"""
        try:
            # æŒ‰äº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶åˆ†ç»„
            grouped_data = defaultdict(list)
            for data in processed_data:
                symbol = data.get("symbol", "")
                timeframe = data.get("timeframe", "1m")
                if symbol and timeframe:
                    grouped_data[(symbol, timeframe)].append(data)
            
            # æ‰¹é‡æ’å…¥æ•°æ®åº“
            for (symbol, timeframe), candles in grouped_data.items():
                try:
                    await data_manager.time_series_manager.insert_kline_data(
                        symbol=symbol,
                        timeframe=timeframe,
                        data=candles
                    )
                except Exception as e:
                    logger.error(f"ä¿å­˜Kçº¿æ•°æ®å¤±è´¥ {symbol} {timeframe}: {e}")
            
            # æ‰§è¡ŒåŸæœ‰å›è°ƒ
            for callback in self.candle_callbacks:
                try:
                    for data in processed_data:
                        await callback(data)
                except Exception as e:
                    logger.error(f"Kçº¿æ•°æ®å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
            
            logger.debug(f"æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®: {len(processed_data)}æ¡")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®å¤±è´¥: {e}")
    
    async def _save_coinglass_data_batch(self, processed_data: List[Dict]):
        """æ‰¹é‡ä¿å­˜Coinglassæ•°æ®"""
        try:
            # æŒ‰æ•°æ®ç±»å‹åˆ†ç»„ä¿å­˜
            fear_greed_data = []
            funding_rate_data = []
            open_interest_data = []
            
            for data in processed_data:
                data_type = data.get("data_type", "")
                if data_type == "fear_greed":
                    fear_greed_data.append(data)
                elif data_type == "funding_rate":
                    funding_rate_data.append(data)
                elif data_type == "open_interest":
                    open_interest_data.append(data)
            
            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            if fear_greed_data:
                await data_manager.db.fear_greed_index.insert_many(fear_greed_data)
            if funding_rate_data:
                await data_manager.db.funding_rates.insert_many(funding_rate_data)
            if open_interest_data:
                await data_manager.db.open_interest.insert_many(open_interest_data)
            
            logger.debug(f"æ‰¹é‡ä¿å­˜Coinglassæ•°æ®: FGI={len(fear_greed_data)}, FR={len(funding_rate_data)}, OI={len(open_interest_data)}")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜Coinglassæ•°æ®å¤±è´¥: {e}")
    
    def get_pipeline_metrics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æµæ°´çº¿æ€§èƒ½æŒ‡æ ‡"""
        return optimized_pipeline.get_metrics()
    
    async def get_coinglass_analysis(self) -> Dict[str, Any]:
        """è·å–Coinglassåˆ†ææ•°æ®"""
        if not self.coinglass_enabled:
            return {"enabled": False, "message": "Coinglassæœªå¯ç”¨"}
        
        try:
            # è·å–ç»¼åˆåˆ†æä¿¡å·
            composite_signal = await coinglass_analyzer.generate_composite_signal()
            
            return {
                "enabled": True,
                "composite_signal": {
                    "overall_score": composite_signal.overall_score,
                    "signal_strength": composite_signal.signal_strength,
                    "market_regime": composite_signal.market_regime,
                    "risk_assessment": composite_signal.risk_assessment,
                    "confidence": composite_signal.confidence,
                    "key_factors": composite_signal.key_factors,
                    "timestamp": composite_signal.timestamp.isoformat()
                },
                "sentiment": {
                    "score": composite_signal.sentiment_signal.sentiment_score,
                    "trend": composite_signal.sentiment_signal.trend,
                    "strength": composite_signal.sentiment_signal.strength,
                    "confidence": composite_signal.sentiment_signal.confidence
                },
                "funding_rates": {
                    "overall_rate": composite_signal.funding_signal.overall_rate,
                    "trend": composite_signal.funding_signal.rate_trend,
                    "market_heat": composite_signal.funding_signal.market_heat,
                    "divergence": composite_signal.funding_signal.divergence_score
                },
                "open_interest": {
                    "total": composite_signal.oi_signal.total_oi,
                    "trend": composite_signal.oi_signal.oi_trend,
                    "change_24h": composite_signal.oi_signal.oi_change_24h,
                    "risk_level": composite_signal.oi_signal.risk_level
                },
                "etf_flows": {
                    "btc_flow": composite_signal.etf_signal.btc_flow,
                    "eth_flow": composite_signal.etf_signal.eth_flow,
                    "trend": composite_signal.etf_signal.flow_trend,
                    "institutional_sentiment": composite_signal.etf_signal.institutional_sentiment
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–Coinglassåˆ†æå¤±è´¥: {e}")
            return {"enabled": True, "error": str(e)}
    
    async def get_data_quality_report(self, hours: int = 1) -> Dict[str, Any]:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Š"""
        try:
            return market_data_validator.get_quality_report(hours)
        except Exception as e:
            logger.error(f"è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def migrate_coinglass_historical_data(self, source_path: str) -> Dict[str, Any]:
        """è¿ç§»Coinglasså†å²æ•°æ®"""
        if not self.coinglass_enabled:
            return {"success": False, "enabled": False, "message": "Coinglassæœªå¯ç”¨"}
        
        try:
            from .coinglass_data_migrator import migrate_coinglass_data
            result = await migrate_coinglass_data(source_path)
            logger.info(f"Coinglassæ•°æ®è¿ç§»å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Coinglassæ•°æ®è¿ç§»å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ•°æ®åº“è¿æ¥çŠ¶æ€
            db_status = await data_manager.health_check()
            
            # WebSocketè¿æ¥çŠ¶æ€
            ws_status = self.get_connection_status()
            
            # CoinglassçŠ¶æ€
            coinglass_status = {}
            if self.coinglass_enabled:
                coinglass_status = coinglass_collector_manager.get_status()
            
            return {
                "database": db_status,
                "websockets": ws_status,
                "coinglass": {
                    "enabled": self.coinglass_enabled,
                    "status": coinglass_status
                },
                "running": self.is_running,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {"error": str(e)}

# å…¨å±€çœŸå®æ•°æ®ç®¡ç†å™¨å®ä¾‹
real_data_manager = RealDataManager()