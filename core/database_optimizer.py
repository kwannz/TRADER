"""
æ•°æ®åº“ä¼˜åŒ–å·¥å…·
MongoDBç´¢å¼•ã€æŸ¥è¯¢ä¼˜åŒ–å’Œåˆ†ç‰‡ç­–ç•¥
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pymongo
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.unified_logger import get_logger, LogCategory

logger = get_logger()

class MongoDBOptimizer:
    """MongoDBä¼˜åŒ–å™¨"""
    
    def __init__(self, database: AsyncIOMotorDatabase):
        self.db = database
        self.optimization_results = {}
        
    async def create_indexes(self):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        logger.info("ğŸš€ å¼€å§‹åˆ›å»ºMongoDBä¼˜åŒ–ç´¢å¼•...")
        
        try:
            # 1. æ—¶åºæ•°æ®é›†åˆç´¢å¼•
            await self._create_timeseries_indexes()
            
            # 2. å¸‚åœºæ•°æ®ç´¢å¼•
            await self._create_market_data_indexes()
            
            # 3. Coinglassæ•°æ®ç´¢å¼•
            await self._create_coinglass_indexes()
            
            # 4. ç³»ç»Ÿæ€§èƒ½ç´¢å¼•
            await self._create_system_indexes()
            
            logger.info("âœ… MongoDBç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    async def _create_timeseries_indexes(self):
        """åˆ›å»ºæ—¶åºæ•°æ®ç´¢å¼•"""
        logger.info("ğŸ“Š åˆ›å»ºæ—¶åºæ•°æ®ç´¢å¼•...")
        
        # tick_dataæ—¶åºé›†åˆ
        tick_collection = self.db["tick_data"]
        
        try:
            # å¤åˆç´¢å¼•ï¼šç¬¦å· + æ—¶é—´æˆ³ï¼ˆé™åºï¼‰
            await tick_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_timestamp_desc", background=True)
            
            # å¤åˆç´¢å¼•ï¼šäº¤æ˜“æ‰€ + ç¬¦å· + æ—¶é—´æˆ³
            await tick_collection.create_index([
                ("exchange", pymongo.ASCENDING),
                ("symbol", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="exchange_symbol_timestamp", background=True)
            
            # ä»·æ ¼èŒƒå›´æŸ¥è¯¢ç´¢å¼•
            await tick_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("price", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_price_timestamp", background=True)
            
            # æ•°æ®è´¨é‡ç´¢å¼•
            await tick_collection.create_index([
                ("data_quality_level", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="quality_timestamp", background=True)
            
            logger.info("âœ… tick_dataç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ tick_dataç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
        
        # kline_dataæ—¶åºé›†åˆ
        kline_collection = self.db["kline_data"]
        
        try:
            # å¤åˆç´¢å¼•ï¼šç¬¦å· + æ—¶é—´å‘¨æœŸ + æ—¶é—´æˆ³
            await kline_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("timeframe", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_timeframe_timestamp", background=True)
            
            # OHLCèŒƒå›´æŸ¥è¯¢ç´¢å¼•
            await kline_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("close", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_close_timestamp", background=True)
            
            # æˆäº¤é‡æ’åºç´¢å¼•
            await kline_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("volume", pymongo.DESCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_volume_desc", background=True)
            
            logger.info("âœ… kline_dataç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ kline_dataç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
    
    async def _create_market_data_indexes(self):
        """åˆ›å»ºå¸‚åœºæ•°æ®ç´¢å¼•"""
        logger.info("ğŸ“ˆ åˆ›å»ºå¸‚åœºæ•°æ®ç´¢å¼•...")
        
        # åˆ›å»ºå®æ—¶å¸‚åœºæ•°æ®è§†å›¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        try:
            await self.db.create_collection("market_data_view", viewOn="tick_data", pipeline=[
                {"$match": {"timestamp": {"$gte": datetime.utcnow() - timedelta(hours=24)}}},
                {"$sort": {"timestamp": -1}},
                {"$group": {
                    "_id": {"symbol": "$symbol", "exchange": "$exchange"},
                    "latest_price": {"$first": "$price"},
                    "latest_timestamp": {"$first": "$timestamp"},
                    "volume_24h": {"$sum": "$volume_24h"},
                    "high_24h": {"$max": "$high_24h"},
                    "low_24h": {"$min": "$low_24h"},
                    "data_quality_avg": {"$avg": "$data_quality_score"}
                }}
            ])
            logger.info("âœ… åˆ›å»ºmarket_data_viewè§†å›¾")
        except Exception as e:
            logger.debug(f"market_data_viewå¯èƒ½å·²å­˜åœ¨: {e}")
        
        # AIåˆ†æç»“æœé›†åˆ
        ai_collection = self.db["ai_analysis"]
        
        try:
            # AIåˆ†ææ—¶é—´ç´¢å¼•
            await ai_collection.create_index([
                ("analysis_type", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="analysis_type_timestamp", background=True)
            
            # ä¿¡å·å¼ºåº¦ç´¢å¼•
            await ai_collection.create_index([
                ("signal_strength", pymongo.ASCENDING),
                ("confidence", pymongo.DESCENDING)
            ], name="signal_confidence", background=True)
            
            logger.info("âœ… ai_analysisç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ai_analysisç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
    
    async def _create_coinglass_indexes(self):
        """åˆ›å»ºCoinglassæ•°æ®ç´¢å¼•"""
        logger.info("ğŸ§  åˆ›å»ºCoinglassæ•°æ®ç´¢å¼•...")
        
        # æè´ªæŒ‡æ•°é›†åˆ
        fear_greed_collection = self.db["fear_greed_index"]
        
        try:
            # æ—¶é—´ç´¢å¼•ï¼ˆé™åºï¼Œç”¨äºè·å–æœ€æ–°æ•°æ®ï¼‰
            await fear_greed_collection.create_index([
                ("timestamp", pymongo.DESCENDING)
            ], name="timestamp_desc", background=True)
            
            # æ—¥æœŸç´¢å¼•ï¼ˆç”¨äºæŒ‰æ—¥æœŸæŸ¥è¯¢ï¼‰
            await fear_greed_collection.create_index([
                ("date", pymongo.ASCENDING)
            ], name="date_asc", background=True)
            
            # å€¼èŒƒå›´ç´¢å¼•
            await fear_greed_collection.create_index([
                ("value", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="value_timestamp", background=True)
            
            # åˆ†ç±»ç´¢å¼•
            await fear_greed_collection.create_index([
                ("classification", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="classification_timestamp", background=True)
            
            logger.info("âœ… fear_greed_indexç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ fear_greed_indexç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
        
        # èµ„é‡‘è´¹ç‡é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        funding_collection = self.db["funding_rates"]
        
        try:
            await funding_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("exchange", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_exchange_timestamp", background=True)
            
            logger.info("âœ… funding_ratesç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.debug(f"funding_ratesé›†åˆå¯èƒ½ä¸å­˜åœ¨: {e}")
        
        # æŒä»“æ•°æ®é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        oi_collection = self.db["open_interest"]
        
        try:
            await oi_collection.create_index([
                ("symbol", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="symbol_timestamp_desc", background=True)
            
            logger.info("âœ… open_interestç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.debug(f"open_interesté›†åˆå¯èƒ½ä¸å­˜åœ¨: {e}")
    
    async def _create_system_indexes(self):
        """åˆ›å»ºç³»ç»Ÿæ€§èƒ½ç´¢å¼•"""
        logger.info("âš™ï¸ åˆ›å»ºç³»ç»Ÿæ€§èƒ½ç´¢å¼•...")
        
        # ç³»ç»Ÿæ—¥å¿—é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        logs_collection = self.db["system_logs"]
        
        try:
            # æ—¶é—´ + çº§åˆ«ç´¢å¼•
            await logs_collection.create_index([
                ("level", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="level_timestamp", background=True)
            
            # TTLç´¢å¼•ï¼ˆè‡ªåŠ¨åˆ é™¤30å¤©å‰çš„æ—¥å¿—ï¼‰
            await logs_collection.create_index([
                ("timestamp", pymongo.ASCENDING)
            ], name="log_ttl", background=True, expireAfterSeconds=30*24*3600)
            
            logger.info("âœ… system_logsç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.debug(f"system_logsé›†åˆå¯èƒ½ä¸å­˜åœ¨: {e}")
        
        # æ€§èƒ½æŒ‡æ ‡é›†åˆ
        metrics_collection = self.db["performance_metrics"]
        
        try:
            # æŒ‡æ ‡ç±»å‹ + æ—¶é—´ç´¢å¼•
            await metrics_collection.create_index([
                ("metric_type", pymongo.ASCENDING),
                ("timestamp", pymongo.DESCENDING)
            ], name="metric_timestamp", background=True)
            
            # TTLç´¢å¼•ï¼ˆè‡ªåŠ¨åˆ é™¤7å¤©å‰çš„æ€§èƒ½æŒ‡æ ‡ï¼‰
            await metrics_collection.create_index([
                ("timestamp", pymongo.ASCENDING)
            ], name="metrics_ttl", background=True, expireAfterSeconds=7*24*3600)
            
            logger.info("âœ… performance_metricsç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.debug(f"performance_metricsé›†åˆå¯èƒ½ä¸å­˜åœ¨: {e}")
    
    async def optimize_queries(self):
        """ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        logger.info("ğŸ” å¼€å§‹ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½...")
        
        try:
            # 1. åˆ†ææ…¢æŸ¥è¯¢
            await self._analyze_slow_queries()
            
            # 2. ä¼˜åŒ–èšåˆç®¡é“
            await self._optimize_aggregation_pipelines()
            
            # 3. é…ç½®æ•°æ®åº“å‚æ•°
            await self._configure_database_settings()
            
            logger.info("âœ… æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _analyze_slow_queries(self):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        try:
            # è·å–æ…¢æŸ¥è¯¢åˆ†æ
            db_stats = await self.db.command("dbStats")
            logger.info(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: {db_stats.get('collections')}ä¸ªé›†åˆ, {db_stats.get('dataSize', 0)/1024/1024:.2f}MBæ•°æ®")
            
            # æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
            collections = await self.db.list_collection_names()
            for collection_name in collections:
                if not collection_name.startswith("system."):
                    try:
                        collection = self.db[collection_name]
                        index_stats = await collection.index_stats().to_list(None)
                        
                        unused_indexes = [idx for idx in index_stats if idx.get("accesses", {}).get("ops", 0) == 0]
                        if unused_indexes:
                            logger.info(f"âš ï¸ {collection_name}é›†åˆæœ‰{len(unused_indexes)}ä¸ªæœªä½¿ç”¨çš„ç´¢å¼•")
                    except Exception as e:
                        logger.debug(f"æ— æ³•è·å–{collection_name}çš„ç´¢å¼•ç»Ÿè®¡: {e}")
                        
        except Exception as e:
            logger.error(f"æ…¢æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
    
    async def _optimize_aggregation_pipelines(self):
        """ä¼˜åŒ–èšåˆç®¡é“"""
        logger.info("âš¡ ä¼˜åŒ–èšåˆæŸ¥è¯¢ç®¡é“...")
        
        # åˆ›å»ºé¢„è®¡ç®—è§†å›¾ç”¨äºå¸¸ç”¨æŸ¥è¯¢
        try:
            # 24å°æ—¶ä»·æ ¼ç»Ÿè®¡è§†å›¾
            await self.db.create_collection("price_stats_24h", viewOn="tick_data", pipeline=[
                {"$match": {"timestamp": {"$gte": datetime.utcnow() - timedelta(hours=24)}}},
                {"$group": {
                    "_id": "$symbol",
                    "avg_price": {"$avg": "$price"},
                    "max_price": {"$max": "$price"},
                    "min_price": {"$min": "$price"},
                    "total_volume": {"$sum": "$volume_24h"},
                    "count": {"$sum": 1},
                    "last_update": {"$max": "$timestamp"}
                }},
                {"$sort": {"total_volume": -1}}
            ])
            logger.info("âœ… åˆ›å»ºprice_stats_24hè§†å›¾")
        except Exception as e:
            logger.debug(f"price_stats_24hè§†å›¾å¯èƒ½å·²å­˜åœ¨: {e}")
        
        # æ•°æ®è´¨é‡ç»Ÿè®¡è§†å›¾
        try:
            await self.db.create_collection("quality_stats", viewOn="tick_data", pipeline=[
                {"$match": {"timestamp": {"$gte": datetime.utcnow() - timedelta(hours=1)}}},
                {"$group": {
                    "_id": {
                        "exchange": "$exchange",
                        "quality_level": "$data_quality_level"
                    },
                    "count": {"$sum": 1},
                    "avg_score": {"$avg": "$data_quality_score"},
                    "min_score": {"$min": "$data_quality_score"},
                    "max_score": {"$max": "$data_quality_score"}
                }},
                {"$sort": {"_id.exchange": 1, "_id.quality_level": 1}}
            ])
            logger.info("âœ… åˆ›å»ºquality_statsè§†å›¾")
        except Exception as e:
            logger.debug(f"quality_statsè§†å›¾å¯èƒ½å·²å­˜åœ¨: {e}")
    
    async def _configure_database_settings(self):
        """é…ç½®æ•°æ®åº“è®¾ç½®"""
        logger.info("âš™ï¸ ä¼˜åŒ–æ•°æ®åº“é…ç½®...")
        
        try:
            # è®¾ç½®åˆ†æå™¨æ”¶é›†æŸ¥è¯¢ç»Ÿè®¡ï¼ˆä»…åœ¨å¼€å‘ç¯å¢ƒï¼‰
            if os.getenv("ENVIRONMENT", "production") == "development":
                await self.db.command("profile", 2, slowOpThresholdMs=100)
                logger.info("âœ… å¯ç”¨æŸ¥è¯¢åˆ†æå™¨")
            
            # ä¼˜åŒ–è¿æ¥æ± è®¾ç½®å·²åœ¨è¿æ¥å­—ç¬¦ä¸²ä¸­é…ç½®
            logger.info("âœ… æ•°æ®åº“é…ç½®ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
    
    async def create_sharding_strategy(self):
        """åˆ›å»ºåˆ†ç‰‡ç­–ç•¥ï¼ˆä¼ä¸šçº§åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ—‚ï¸ è®¾è®¡åˆ†ç‰‡ç­–ç•¥...")
        
        # è¿™é‡Œå®šä¹‰åˆ†ç‰‡ç­–ç•¥ï¼Œå®é™…åˆ†ç‰‡éœ€è¦MongoDBé›†ç¾¤
        sharding_recommendations = {
            "tick_data": {
                "shard_key": {"symbol": 1, "timestamp": 1},
                "reason": "æŒ‰äº¤æ˜“å¯¹å’Œæ—¶é—´åˆ†ç‰‡ï¼Œæ”¯æŒé«˜å¹¶å‘å†™å…¥",
                "zones": [
                    {"range": {"symbol": "BTC/USDT"}, "zone": "crypto_major"},
                    {"range": {"symbol": "ETH/USDT"}, "zone": "crypto_major"},
                ]
            },
            "kline_data": {
                "shard_key": {"symbol": 1, "timeframe": 1, "timestamp": 1},
                "reason": "æŒ‰äº¤æ˜“å¯¹ã€æ—¶é—´å‘¨æœŸå’Œæ—¶é—´åˆ†ç‰‡",
                "zones": [
                    {"range": {"timeframe": "1m"}, "zone": "high_freq"},
                    {"range": {"timeframe": "1d"}, "zone": "low_freq"},
                ]
            },
            "fear_greed_index": {
                "shard_key": {"date": 1},
                "reason": "æŒ‰æ—¥æœŸåˆ†ç‰‡ï¼Œå†å²æ•°æ®æŸ¥è¯¢ä¼˜åŒ–",
                "zones": []
            }
        }
        
        logger.info("ğŸ“‹ åˆ†ç‰‡ç­–ç•¥è®¾è®¡å®Œæˆ:")
        for collection, strategy in sharding_recommendations.items():
            logger.info(f"  {collection}: {strategy['reason']}")
        
        return sharding_recommendations
    
    async def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š...")
        
        try:
            report = {
                "optimization_time": datetime.utcnow().isoformat(),
                "database_info": {},
                "collections": {},
                "indexes": {},
                "performance": {}
            }
            
            # æ•°æ®åº“åŸºæœ¬ä¿¡æ¯
            db_stats = await self.db.command("dbStats")
            report["database_info"] = {
                "collections": db_stats.get("collections", 0),
                "data_size_mb": db_stats.get("dataSize", 0) / 1024 / 1024,
                "index_size_mb": db_stats.get("indexSize", 0) / 1024 / 1024,
                "total_size_mb": db_stats.get("storageSize", 0) / 1024 / 1024
            }
            
            # å„é›†åˆç»Ÿè®¡
            collections = await self.db.list_collection_names()
            for collection_name in collections:
                if not collection_name.startswith("system."):
                    try:
                        collection = self.db[collection_name]
                        stats = await self.db.command("collStats", collection_name)
                        
                        report["collections"][collection_name] = {
                            "count": stats.get("count", 0),
                            "size_mb": stats.get("size", 0) / 1024 / 1024,
                            "avg_obj_size": stats.get("avgObjSize", 0),
                            "total_index_size_mb": stats.get("totalIndexSize", 0) / 1024 / 1024,
                            "indexes": stats.get("nindexes", 0)
                        }
                        
                        # ç´¢å¼•ä¿¡æ¯
                        indexes = await collection.list_indexes().to_list(None)
                        report["indexes"][collection_name] = [
                            {
                                "name": idx.get("name"),
                                "key": idx.get("key"),
                                "unique": idx.get("unique", False),
                                "background": idx.get("background", False)
                            }
                            for idx in indexes
                        ]
                        
                    except Exception as e:
                        logger.debug(f"æ— æ³•è·å–{collection_name}çš„ç»Ÿè®¡: {e}")
            
            logger.info("âœ… æ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}

class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    def __init__(self, database: AsyncIOMotorDatabase):
        self.db = database
    
    async def get_optimized_market_data_pipeline(self, symbol: str = None, 
                                                exchange: str = None, 
                                                hours: int = 24) -> List[Dict]:
        """è·å–ä¼˜åŒ–çš„å¸‚åœºæ•°æ®èšåˆç®¡é“"""
        pipeline = []
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
        match_stage = {"timestamp": {"$gte": datetime.utcnow() - timedelta(hours=hours)}}
        
        if symbol:
            match_stage["symbol"] = symbol
        if exchange:
            match_stage["exchange"] = exchange
            
        pipeline.append({"$match": match_stage})
        
        # æ’åºï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
        pipeline.append({"$sort": {"timestamp": -1}})
        
        # èšåˆç»Ÿè®¡
        pipeline.extend([
            {"$group": {
                "_id": {"symbol": "$symbol", "exchange": "$exchange"},
                "latest_price": {"$first": "$price"},
                "latest_timestamp": {"$first": "$timestamp"},
                "high": {"$max": "$price"},
                "low": {"$min": "$price"},
                "volume": {"$sum": "$volume_24h"},
                "count": {"$sum": 1},
                "avg_quality": {"$avg": "$data_quality_score"}
            }},
            {"$sort": {"volume": -1}}
        ])
        
        return pipeline
    
    async def get_optimized_candle_pipeline(self, symbol: str, timeframe: str, 
                                           limit: int = 100) -> List[Dict]:
        """è·å–ä¼˜åŒ–çš„Kçº¿æ•°æ®ç®¡é“"""
        pipeline = [
            # ç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨å¤åˆç´¢å¼•ï¼‰
            {"$match": {
                "symbol": symbol,
                "timeframe": timeframe
            }},
            # æ’åºï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
            {"$sort": {"timestamp": -1}},
            # é™åˆ¶ç»“æœæ•°é‡
            {"$limit": limit},
            # åªè¿”å›éœ€è¦çš„å­—æ®µ
            {"$project": {
                "timestamp": 1,
                "open": 1,
                "high": 1,
                "low": 1,
                "close": 1,
                "volume": 1,
                "data_quality_score": 1
            }}
        ]
        
        return pipeline

# åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
def create_database_optimizer(database: AsyncIOMotorDatabase) -> MongoDBOptimizer:
    """åˆ›å»ºæ•°æ®åº“ä¼˜åŒ–å™¨å®ä¾‹"""
    return MongoDBOptimizer(database)