"""
CLIæ¨¡å—éªŒè¯æµ‹è¯•å™¨

æä¾›å®Œæ•´çš„æ¨¡å—éªŒè¯ã€æµ‹è¯•å’Œè°ƒè¯•åŠŸèƒ½
æ”¯æŒå®æ—¶ç»“æœæ˜¾ç¤ºå’Œè¯¦ç»†æŠ¥å‘Šç”Ÿæˆ
"""

import asyncio
import time
import sys
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import json
import subprocess

# Richç»„ä»¶ç”¨äºç¾è§‚çš„CLIè¾“å‡º
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import (
    Progress, 
    SpinnerColumn, 
    TextColumn, 
    BarColumn, 
    TimeElapsedColumn,
    TimeRemainingColumn
)
from rich.layout import Layout
from rich.live import Live
from rich.text import Text
from rich.tree import Tree
from rich.align import Align
from rich.rule import Rule

# éªŒè¯å™¨æ¨¡å—å¯¼å…¥ - ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜
import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'validators'))

from rust_engine_test import RustEngineValidator
from python_layer_test import PythonLayerValidator  
from fastapi_test import FastAPIValidator
from database_test import DatabaseValidator
from integration_test import IntegrationValidator

console = Console()

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœæ•°æ®ç»“æ„"""
    module_name: str
    test_name: str
    status: str  # "PASS", "FAIL", "SKIP", "ERROR"
    duration_ms: float
    message: str
    details: Dict[str, Any]
    metrics: Dict[str, float]
    timestamp: datetime
    error_trace: Optional[str] = None

@dataclass
class ModuleTestReport:
    """æ¨¡å—æµ‹è¯•æŠ¥å‘Š"""
    module_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    error_tests: int
    total_duration_ms: float
    results: List[ValidationResult]
    status: str  # "PASS", "FAIL", "PARTIAL"

class CLITester:
    """
    CLIæ¨¡å—æµ‹è¯•å™¨ä¸»ç±»
    
    è´Ÿè´£åè°ƒæ‰€æœ‰æ¨¡å—çš„éªŒè¯æµ‹è¯•
    """
    
    def __init__(self):
        self.console = Console(record=True)
        self.validators: Dict[str, Any] = {}
        self.test_results: Dict[str, ModuleTestReport] = {}
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        
        # é…ç½®é€‰é¡¹
        self.verbose = False
        self.parallel = True
        self.timeout_seconds = 300  # 5åˆ†é’Ÿæ€»è¶…æ—¶
        self.module_timeout = 60    # å•æ¨¡å—60ç§’è¶…æ—¶
        
        # æ€§èƒ½åŸºå‡†
        self.performance_thresholds = {
            "rust_engine": {
                "data_processing_time": 100,  # ms
                "strategy_execution_time": 200,  # ms
                "risk_check_time": 50,  # ms
            },
            "python_layer": {
                "ai_response_time": 2000,  # ms
                "data_query_time": 500,  # ms
                "model_load_time": 1000,  # ms
            },
            "fastapi": {
                "api_response_time": 100,  # ms
                "websocket_latency": 50,  # ms
                "auth_time": 200,  # ms
            },
            "database": {
                "connection_time": 1000,  # ms
                "query_time": 100,  # ms
                "write_time": 200,  # ms
            }
        }
        
    def add_validator(self, validator_instance) -> None:
        """æ·»åŠ éªŒè¯å™¨å®ä¾‹"""
        module_name = validator_instance.__class__.__name__.replace('Validator', '').lower()
        self.validators[module_name] = validator_instance
        
    async def run_all_validations(self, modules: Optional[List[str]] = None) -> Dict[str, ModuleTestReport]:
        """è¿è¡Œæ‰€æœ‰æ¨¡å—éªŒè¯"""
        self.start_time = datetime.utcnow()
        
        try:
            # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å—
            test_modules = modules or list(self.validators.keys())
            
            with self.console.status("[bold green]åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...") as status:
                await asyncio.sleep(0.5)
                status.update("[bold blue]å¼€å§‹æ¨¡å—éªŒè¯...")
                
                # æ˜¾ç¤ºæµ‹è¯•æ¦‚è§ˆ
                self._show_test_overview(test_modules)
                
                # å¹¶è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œæµ‹è¯•
                if self.parallel and len(test_modules) > 1:
                    await self._run_parallel_tests(test_modules)
                else:
                    await self._run_sequential_tests(test_modules)
            
            self.end_time = datetime.utcnow()
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            self._show_final_results()
            
            return self.test_results
            
        except KeyboardInterrupt:
            self.console.print("\n[yellow]æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
            return self.test_results
        except Exception as e:
            self.console.print(f"\n[red]æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}[/red]")
            return self.test_results

    async def _run_parallel_tests(self, modules: List[str]) -> None:
        """å¹¶è¡Œæ‰§è¡Œæµ‹è¯•"""
        tasks = []
        
        # åˆ›å»ºè¿›åº¦æ˜¾ç¤º
        progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=self.console,
            transient=True
        )
        
        with progress:
            # ä¸ºæ¯ä¸ªæ¨¡å—åˆ›å»ºè¿›åº¦ä»»åŠ¡
            progress_tasks = {}
            for module in modules:
                task_id = progress.add_task(f"[cyan]æµ‹è¯• {module}...", total=100)
                progress_tasks[module] = task_id
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            for module in modules:
                task = asyncio.create_task(
                    self._run_module_test_with_progress(module, progress, progress_tasks[module])
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _run_sequential_tests(self, modules: List[str]) -> None:
        """é¡ºåºæ‰§è¡Œæµ‹è¯•"""
        for i, module in enumerate(modules, 1):
            self.console.print(f"\n[bold cyan]â•â•â• æµ‹è¯•æ¨¡å— {i}/{len(modules)}: {module} â•â•â•[/bold cyan]")
            await self._run_module_test(module)

    async def _run_module_test_with_progress(self, module: str, progress: Progress, task_id) -> None:
        """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ¨¡å—æµ‹è¯•"""
        try:
            # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹æµ‹è¯•
            progress.update(task_id, completed=10, description=f"[cyan]åˆå§‹åŒ– {module}...")
            
            validator = self.validators.get(module)
            if not validator:
                raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å—éªŒè¯å™¨: {module}")
            
            # æ›´æ–°è¿›åº¦ï¼šè¿è¡Œæµ‹è¯•
            progress.update(task_id, completed=30, description=f"[yellow]è¿è¡Œ {module} æµ‹è¯•...")
            
            module_start_time = time.time()
            test_results = []
            
            # è·å–æµ‹è¯•æ–¹æ³•
            test_methods = [method for method in dir(validator) if method.startswith('test_')]
            total_methods = len(test_methods)
            
            for i, method_name in enumerate(test_methods):
                method = getattr(validator, method_name)
                
                # æ›´æ–°å­è¿›åº¦
                sub_progress = 30 + (i / total_methods) * 60
                progress.update(task_id, completed=sub_progress, 
                               description=f"[yellow]{module}: {method_name}...")
                
                # æ‰§è¡Œæµ‹è¯•æ–¹æ³•
                result = await self._run_single_test(validator, method_name, method)
                test_results.append(result)
            
            # æ›´æ–°è¿›åº¦ï¼šå®Œæˆ
            progress.update(task_id, completed=100, description=f"[green]âœ… {module} å®Œæˆ")
            
            # ç”Ÿæˆæ¨¡å—æŠ¥å‘Š
            module_duration = (time.time() - module_start_time) * 1000
            self.test_results[module] = self._generate_module_report(module, test_results, module_duration)
            
        except Exception as e:
            progress.update(task_id, completed=100, description=f"[red]âŒ {module} å¤±è´¥")
            
            # åˆ›å»ºé”™è¯¯æŠ¥å‘Š
            error_result = ValidationResult(
                module_name=module,
                test_name="module_initialization",
                status="ERROR",
                duration_ms=0,
                message=str(e),
                details={"error_type": type(e).__name__},
                metrics={},
                timestamp=datetime.utcnow(),
                error_trace=traceback.format_exc()
            )
            
            self.test_results[module] = ModuleTestReport(
                module_name=module,
                total_tests=1,
                passed_tests=0,
                failed_tests=0,
                skipped_tests=0,
                error_tests=1,
                total_duration_ms=0,
                results=[error_result],
                status="FAIL"
            )

    async def _run_module_test(self, module: str) -> None:
        """è¿è¡Œå•ä¸ªæ¨¡å—æµ‹è¯•"""
        try:
            validator = self.validators.get(module)
            if not validator:
                self.console.print(f"[red]âŒ æœªæ‰¾åˆ°æ¨¡å—éªŒè¯å™¨: {module}[/red]")
                return
            
            module_start_time = time.time()
            test_results = []
            
            # è·å–æ‰€æœ‰æµ‹è¯•æ–¹æ³•
            test_methods = [method for method in dir(validator) if method.startswith('test_')]
            
            self.console.print(f"[dim]æ‰¾åˆ° {len(test_methods)} ä¸ªæµ‹è¯•ç”¨ä¾‹[/dim]")
            
            # æ‰§è¡Œæ¯ä¸ªæµ‹è¯•æ–¹æ³•
            for method_name in test_methods:
                method = getattr(validator, method_name)
                result = await self._run_single_test(validator, method_name, method)
                test_results.append(result)
                
                # å®æ—¶æ˜¾ç¤ºç»“æœ
                self._show_test_result(result)
            
            # ç”Ÿæˆæ¨¡å—æŠ¥å‘Š
            module_duration = (time.time() - module_start_time) * 1000
            self.test_results[module] = self._generate_module_report(module, test_results, module_duration)
            
        except Exception as e:
            self.console.print(f"[red]âŒ æ¨¡å—æµ‹è¯•å¤±è´¥: {e}[/red]")

    async def _run_single_test(self, validator, method_name: str, method: Callable) -> ValidationResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•æ–¹æ³•"""
        test_start_time = time.time()
        
        try:
            # æ‰§è¡Œæµ‹è¯•æ–¹æ³•ï¼ˆæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ï¼‰
            if asyncio.iscoroutinefunction(method):
                result = await asyncio.wait_for(method(), timeout=self.module_timeout)
            else:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ–¹æ³•
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor() as executor:
                    result = await loop.run_in_executor(executor, method)
            
            test_duration = (time.time() - test_start_time) * 1000
            
            # è§£ææµ‹è¯•ç»“æœ
            if isinstance(result, dict):
                status = result.get("status", "PASS")
                message = result.get("message", "æµ‹è¯•é€šè¿‡")
                details = result.get("details", {})
                metrics = result.get("metrics", {})
            else:
                status = "PASS" if result else "FAIL"
                message = "æµ‹è¯•é€šè¿‡" if result else "æµ‹è¯•å¤±è´¥"
                details = {}
                metrics = {}
            
            return ValidationResult(
                module_name=validator.__class__.__name__.replace('Validator', '').lower(),
                test_name=method_name,
                status=status,
                duration_ms=test_duration,
                message=message,
                details=details,
                metrics=metrics,
                timestamp=datetime.utcnow()
            )
            
        except asyncio.TimeoutError:
            test_duration = (time.time() - test_start_time) * 1000
            return ValidationResult(
                module_name=validator.__class__.__name__.replace('Validator', '').lower(),
                test_name=method_name,
                status="FAIL",
                duration_ms=test_duration,
                message=f"æµ‹è¯•è¶…æ—¶ï¼ˆ>{self.module_timeout}ç§’ï¼‰",
                details={"timeout": self.module_timeout},
                metrics={},
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            test_duration = (time.time() - test_start_time) * 1000
            return ValidationResult(
                module_name=validator.__class__.__name__.replace('Validator', '').lower(),
                test_name=method_name,
                status="ERROR",
                duration_ms=test_duration,
                message=str(e),
                details={"error_type": type(e).__name__},
                metrics={},
                timestamp=datetime.utcnow(),
                error_trace=traceback.format_exc()
            )

    def _generate_module_report(self, module: str, results: List[ValidationResult], duration_ms: float) -> ModuleTestReport:
        """ç”Ÿæˆæ¨¡å—æµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == "PASS")
        failed_tests = sum(1 for r in results if r.status == "FAIL")
        skipped_tests = sum(1 for r in results if r.status == "SKIP")
        error_tests = sum(1 for r in results if r.status == "ERROR")
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if error_tests > 0 or failed_tests > 0:
            status = "FAIL"
        elif passed_tests == total_tests:
            status = "PASS"
        else:
            status = "PARTIAL"
        
        return ModuleTestReport(
            module_name=module,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            error_tests=error_tests,
            total_duration_ms=duration_ms,
            results=results,
            status=status
        )

    def _show_test_overview(self, modules: List[str]) -> None:
        """æ˜¾ç¤ºæµ‹è¯•æ¦‚è§ˆ"""
        table = Table(title="[bold cyan]CLIæ¨¡å—éªŒè¯æµ‹è¯•æ¦‚è§ˆ[/bold cyan]", show_header=True)
        table.add_column("æ¨¡å—", style="cyan", no_wrap=True)
        table.add_column("éªŒè¯å™¨", style="magenta")
        table.add_column("æµ‹è¯•æ•°é‡", justify="center", style="green")
        table.add_column("é¢„è®¡è€—æ—¶", justify="center", style="yellow")
        
        for module in modules:
            validator = self.validators.get(module)
            if validator:
                validator_name = validator.__class__.__name__
                test_count = len([m for m in dir(validator) if m.startswith('test_')])
                estimated_time = f"{test_count * 2}s"
            else:
                validator_name = "âŒ æœªæ‰¾åˆ°"
                test_count = 0
                estimated_time = "0s"
            
            table.add_row(module, validator_name, str(test_count), estimated_time)
        
        self.console.print(table)
        self.console.print()

    def _show_test_result(self, result: ValidationResult) -> None:
        """æ˜¾ç¤ºå•ä¸ªæµ‹è¯•ç»“æœ"""
        status_icon = {
            "PASS": "âœ…",
            "FAIL": "âŒ", 
            "SKIP": "â­ï¸",
            "ERROR": "ğŸ’¥"
        }.get(result.status, "â“")
        
        status_color = {
            "PASS": "green",
            "FAIL": "red",
            "SKIP": "yellow",
            "ERROR": "magenta"
        }.get(result.status, "white")
        
        duration_color = "green" if result.duration_ms < 1000 else "yellow" if result.duration_ms < 5000 else "red"
        
        self.console.print(
            f"{status_icon} [{status_color}]{result.test_name}[/{status_color}] "
            f"[{duration_color}]{result.duration_ms:.1f}ms[/{duration_color}] "
            f"[dim]{result.message}[/dim]"
        )
        
        # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        if result.metrics and self.verbose:
            for key, value in result.metrics.items():
                self.console.print(f"    ğŸ“Š {key}: {value}")

    def _show_final_results(self) -> None:
        """æ˜¾ç¤ºæœ€ç»ˆæµ‹è¯•ç»“æœ"""
        if not self.test_results:
            self.console.print("[red]æ²¡æœ‰æµ‹è¯•ç»“æœ[/red]")
            return
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_modules = len(self.test_results)
        passed_modules = sum(1 for report in self.test_results.values() if report.status == "PASS")
        failed_modules = sum(1 for report in self.test_results.values() if report.status == "FAIL")
        partial_modules = sum(1 for report in self.test_results.values() if report.status == "PARTIAL")
        
        total_tests = sum(report.total_tests for report in self.test_results.values())
        total_passed = sum(report.passed_tests for report in self.test_results.values())
        total_failed = sum(report.failed_tests for report in self.test_results.values())
        total_errors = sum(report.error_tests for report in self.test_results.values())
        
        total_duration = (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0
        
        # åˆ›å»ºç»“æœé¢æ¿
        self.console.print("\n" + "="*80)
        
        # æ€»ä½“çŠ¶æ€
        if failed_modules == 0 and total_errors == 0:
            overall_status = "[bold green]âœ… å…¨éƒ¨é€šè¿‡[/bold green]"
        elif passed_modules > 0:
            overall_status = "[bold yellow]âš ï¸ éƒ¨åˆ†é€šè¿‡[/bold yellow]"
        else:
            overall_status = "[bold red]âŒ æµ‹è¯•å¤±è´¥[/bold red]"
        
        summary_panel = Panel(
            f"""
{overall_status}

ğŸ“Š [bold]æ¨¡å—ç»Ÿè®¡[/bold]:
   â€¢ æ€»æ¨¡å—æ•°: {total_modules}
   â€¢ é€šè¿‡: [green]{passed_modules}[/green]
   â€¢ å¤±è´¥: [red]{failed_modules}[/red]
   â€¢ éƒ¨åˆ†é€šè¿‡: [yellow]{partial_modules}[/yellow]

ğŸ§ª [bold]æµ‹è¯•ç»Ÿè®¡[/bold]:
   â€¢ æ€»æµ‹è¯•æ•°: {total_tests}
   â€¢ é€šè¿‡: [green]{total_passed}[/green]
   â€¢ å¤±è´¥: [red]{total_failed}[/red]
   â€¢ é”™è¯¯: [magenta]{total_errors}[/magenta]

â±ï¸ [bold]æ€§èƒ½ç»Ÿè®¡[/bold]:
   â€¢ æ€»è€—æ—¶: {total_duration:.2f}s
   â€¢ å¹³å‡æ¯æµ‹è¯•: {(total_duration*1000/max(1,total_tests)):.1f}ms
   â€¢ æµ‹è¯•é€Ÿç‡: {(total_tests/max(0.1,total_duration)):.1f} tests/s
""".strip(),
            title="[bold cyan]CLIéªŒè¯æµ‹è¯•æŠ¥å‘Š[/bold cyan]",
            border_style="cyan" if failed_modules == 0 else "red",
            padding=(1, 2)
        )
        
        self.console.print(summary_panel)
        
        # è¯¦ç»†æ¨¡å—ç»“æœè¡¨æ ¼
        self._show_detailed_results_table()
        
        # æ€§èƒ½åˆ†æ
        self._show_performance_analysis()
        
        # å¤±è´¥æµ‹è¯•è¯¦æƒ…
        if total_failed > 0 or total_errors > 0:
            self._show_failure_details()

    def _show_detailed_results_table(self) -> None:
        """æ˜¾ç¤ºè¯¦ç»†ç»“æœè¡¨æ ¼"""
        table = Table(title="[bold]æ¨¡å—è¯¦ç»†ç»“æœ[/bold]", show_header=True)
        table.add_column("æ¨¡å—", style="cyan", no_wrap=True)
        table.add_column("çŠ¶æ€", justify="center")
        table.add_column("é€šè¿‡/æ€»è®¡", justify="center", style="green")
        table.add_column("è€—æ—¶", justify="center", style="yellow") 
        table.add_column("æˆåŠŸç‡", justify="center", style="blue")
        table.add_column("å¹³å‡å“åº”", justify="center", style="magenta")
        
        for module, report in self.test_results.items():
            # çŠ¶æ€å›¾æ ‡å’Œé¢œè‰²
            if report.status == "PASS":
                status = "[green]âœ… é€šè¿‡[/green]"
            elif report.status == "FAIL":
                status = "[red]âŒ å¤±è´¥[/red]"
            else:
                status = "[yellow]âš ï¸ éƒ¨åˆ†[/yellow]"
            
            # é€šè¿‡ç‡
            pass_rate = f"{report.passed_tests}/{report.total_tests}"
            
            # æˆåŠŸç‡ç™¾åˆ†æ¯”
            success_rate = f"{(report.passed_tests/max(1,report.total_tests)*100):.1f}%"
            
            # å¹³å‡å“åº”æ—¶é—´
            avg_response = f"{(report.total_duration_ms/max(1,report.total_tests)):.1f}ms"
            
            # æ€»è€—æ—¶
            duration = f"{report.total_duration_ms:.0f}ms"
            
            table.add_row(module, status, pass_rate, duration, success_rate, avg_response)
        
        self.console.print("\n")
        self.console.print(table)

    def _show_performance_analysis(self) -> None:
        """æ˜¾ç¤ºæ€§èƒ½åˆ†æ"""
        self.console.print("\n[bold cyan]ğŸ“ˆ æ€§èƒ½åˆ†æ[/bold cyan]")
        
        perf_issues = []
        perf_good = []
        
        for module, report in self.test_results.items():
            module_thresholds = self.performance_thresholds.get(module, {})
            
            for result in report.results:
                for metric_name, metric_value in result.metrics.items():
                    threshold = module_thresholds.get(metric_name)
                    if threshold:
                        if metric_value > threshold:
                            perf_issues.append(f"âš ï¸ {module}.{result.test_name}.{metric_name}: {metric_value:.1f}ms (é˜ˆå€¼: {threshold}ms)")
                        else:
                            perf_good.append(f"âœ… {module}.{result.test_name}.{metric_name}: {metric_value:.1f}ms")
        
        if perf_issues:
            self.console.print("[yellow]æ€§èƒ½è­¦å‘Š:[/yellow]")
            for issue in perf_issues:
                self.console.print(f"  {issue}")
        
        if perf_good and self.verbose:
            self.console.print("[green]æ€§èƒ½è‰¯å¥½:[/green]")
            for good in perf_good[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                self.console.print(f"  {good}")

    def _show_failure_details(self) -> None:
        """æ˜¾ç¤ºå¤±è´¥æµ‹è¯•è¯¦æƒ…"""
        self.console.print("\n[bold red]âŒ å¤±è´¥æµ‹è¯•è¯¦æƒ…[/bold red]")
        
        failure_count = 0
        for module, report in self.test_results.items():
            for result in report.results:
                if result.status in ["FAIL", "ERROR"]:
                    failure_count += 1
                    
                    # åˆ›å»ºå¤±è´¥è¯¦æƒ…é¢æ¿
                    details_content = f"""
[red]æ¨¡å—:[/red] {result.module_name}
[red]æµ‹è¯•:[/red] {result.test_name}
[red]çŠ¶æ€:[/red] {result.status}
[red]è€—æ—¶:[/red] {result.duration_ms:.1f}ms
[red]é”™è¯¯ä¿¡æ¯:[/red] {result.message}
"""
                    
                    if result.details:
                        details_content += f"[red]è¯¦ç»†ä¿¡æ¯:[/red] {json.dumps(result.details, indent=2)}\n"
                    
                    if result.error_trace and self.verbose:
                        details_content += f"[red]é”™è¯¯å †æ ˆ:[/red]\n{result.error_trace}"
                    
                    panel = Panel(
                        details_content.strip(),
                        title=f"[red]å¤±è´¥ #{failure_count}[/red]",
                        border_style="red",
                        padding=(0, 1)
                    )
                    
                    self.console.print(panel)
                    
                    if failure_count >= 5:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                        remaining = sum(
                            len([r for r in rep.results if r.status in ["FAIL", "ERROR"]])
                            for rep in self.test_results.values()
                        ) - 5
                        if remaining > 0:
                            self.console.print(f"[dim]... è¿˜æœ‰ {remaining} ä¸ªå¤±è´¥æµ‹è¯•ï¼ˆä½¿ç”¨ --verbose æŸ¥çœ‹å…¨éƒ¨ï¼‰[/dim]")
                        break

    def generate_report(self, output_file: Optional[str] = None) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report_data = {
            "summary": {
                "start_time": self.start_time.isoformat() if self.start_time else None,
                "end_time": self.end_time.isoformat() if self.end_time else None,
                "total_duration_seconds": (
                    (self.end_time - self.start_time).total_seconds() 
                    if self.end_time and self.start_time else 0
                ),
                "total_modules": len(self.test_results),
                "passed_modules": sum(1 for r in self.test_results.values() if r.status == "PASS"),
                "failed_modules": sum(1 for r in self.test_results.values() if r.status == "FAIL"),
                "total_tests": sum(r.total_tests for r in self.test_results.values()),
                "passed_tests": sum(r.passed_tests for r in self.test_results.values()),
                "failed_tests": sum(r.failed_tests for r in self.test_results.values()),
                "error_tests": sum(r.error_tests for r in self.test_results.values()),
            },
            "modules": {}
        }
        
        # æ·»åŠ æ¨¡å—è¯¦ç»†ä¿¡æ¯
        for module, report in self.test_results.items():
            report_data["modules"][module] = {
                "status": report.status,
                "total_tests": report.total_tests,
                "passed_tests": report.passed_tests,
                "failed_tests": report.failed_tests,
                "error_tests": report.error_tests,
                "duration_ms": report.total_duration_ms,
                "results": [asdict(result) for result in report.results]
            }
        
        report_json = json.dumps(report_data, ensure_ascii=False, indent=2, default=str)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_json)
            self.console.print(f"[green]âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}[/green]")
        
        return report_json

    async def run_continuous_validation(self, interval_seconds: int = 60) -> None:
        """è¿ç»­éªŒè¯æ¨¡å¼"""
        self.console.print(f"[cyan]å¼€å§‹è¿ç»­éªŒè¯æ¨¡å¼ï¼Œé—´éš”: {interval_seconds}ç§’[/cyan]")
        
        try:
            while True:
                self.console.print(f"\n[yellow]{'='*20} {datetime.now().strftime('%H:%M:%S')} {'='*20}[/yellow]")
                
                await self.run_all_validations()
                
                self.console.print(f"[dim]ç­‰å¾… {interval_seconds} ç§’åç»§ç»­...[/dim]")
                await asyncio.sleep(interval_seconds)
                
        except KeyboardInterrupt:
            self.console.print("\n[yellow]è¿ç»­éªŒè¯å·²åœæ­¢[/yellow]")

# ============ å‘½ä»¤è¡Œæ¥å£ ============

async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CLIæ¨¡å—éªŒè¯æµ‹è¯•å™¨")
    parser.add_argument("--modules", nargs="+", help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å—")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--parallel", action="store_true", default=True, help="å¹¶è¡Œæ‰§è¡Œæµ‹è¯•")
    parser.add_argument("--sequential", action="store_true", help="é¡ºåºæ‰§è¡Œæµ‹è¯•")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--continuous", "-c", type=int, metavar="SECONDS", help="è¿ç»­éªŒè¯æ¨¡å¼")
    parser.add_argument("--timeout", type=int, default=300, help="æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = CLITester()
    tester.verbose = args.verbose
    tester.parallel = args.parallel and not args.sequential
    tester.timeout_seconds = args.timeout
    
    # æ·»åŠ éªŒè¯å™¨
    tester.add_validator(RustEngineValidator())
    tester.add_validator(PythonLayerValidator())
    tester.add_validator(FastAPIValidator())
    tester.add_validator(DatabaseValidator())
    tester.add_validator(IntegrationValidator())
    
    # æ‰§è¡Œæµ‹è¯•
    if args.continuous:
        await tester.run_continuous_validation(args.continuous)
    else:
        await tester.run_all_validations(args.modules)
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.output:
            tester.generate_report(args.output)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[yellow]ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
    except Exception as e:
        console.print(f"\n[red]ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}[/red]")
        sys.exit(1)