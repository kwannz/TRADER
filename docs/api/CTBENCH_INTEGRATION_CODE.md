# CTBenché›†æˆä»£ç å®ç°

## 1. FastAPIç«¯ç‚¹é›†æˆ

```python
# backend/src/api/v1/ctbench.py
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import asyncio
import numpy as np
from datetime import datetime

from ...ctbench.data_generation.synthetic_data_manager import SyntheticDataManager
from ...ctbench.models.model_registry import ModelRegistry
from ...ctbench.evaluation.metrics import MetricsCalculator
from ...core.auth import get_current_user

router = APIRouter(prefix="/ctbench", tags=["CTBench"])

class DataGenerationRequest(BaseModel):
    """æ•°æ®ç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    model_name: str  # 'timevae', 'quantgan', 'diffusion_ts', 'fourier_flow'
    n_samples: int = 1000
    seq_length: int = 500
    scenario_type: str = 'normal'  # 'normal', 'stress', 'black_swan'
    strategy_context: Optional[str] = None
    augmentation_ratio: float = 0.3
    
class ModelTrainingRequest(BaseModel):
    """æ¨¡å‹è®­ç»ƒè¯·æ±‚æ¨¡å‹"""
    model_name: str
    training_data_symbols: List[str]
    training_period_days: int = 365
    hyperparameters: Dict[str, Any] = {}
    use_gpu: bool = True
    distributed_training: bool = False

class StressTestRequest(BaseModel):
    """å‹åŠ›æµ‹è¯•è¯·æ±‚æ¨¡å‹"""
    portfolio_config: Dict[str, float]  # èµ„äº§æƒé‡
    test_types: List[str] = ['black_swan', 'flash_crash', 'correlation_breakdown']
    n_scenarios: int = 100
    confidence_levels: List[float] = [0.95, 0.99]

# å…¨å±€æœåŠ¡å®ä¾‹
synthetic_data_manager = SyntheticDataManager()
model_registry = ModelRegistry()
metrics_calculator = MetricsCalculator()

@router.post("/models/train")
async def train_model(
    request: ModelTrainingRequest,
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_user)
):
    """
    è®­ç»ƒTSGæ¨¡å‹
    æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’ŒGPUåŠ é€Ÿ
    """
    try:
        # éªŒè¯æ¨¡å‹åç§°
        if request.model_name not in model_registry.available_models:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {request.model_name}"
            )
        
        # è·å–è®­ç»ƒæ•°æ®
        training_data = await synthetic_data_manager.prepare_training_data(
            symbols=request.training_data_symbols,
            period_days=request.training_period_days
        )
        
        if len(training_data) < 1000:
            raise HTTPException(
                status_code=400,
                detail="è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦1000ä¸ªæ ·æœ¬"
            )
        
        # åˆ›å»ºè®­ç»ƒä»»åŠ¡
        training_task_id = f"train_{request.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å¼‚æ­¥å¯åŠ¨è®­ç»ƒä»»åŠ¡
        background_tasks.add_task(
            _execute_model_training,
            task_id=training_task_id,
            model_name=request.model_name,
            training_data=training_data,
            hyperparameters=request.hyperparameters,
            use_gpu=request.use_gpu,
            distributed=request.distributed_training,
            user_id=current_user.id
        )
        
        return {
            "status": "training_started",
            "task_id": training_task_id,
            "model_name": request.model_name,
            "estimated_duration_minutes": _estimate_training_time(
                request.model_name, 
                len(training_data),
                request.use_gpu
            ),
            "monitoring_endpoint": f"/ctbench/training/status/{training_task_id}"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®­ç»ƒå¯åŠ¨å¤±è´¥: {str(e)}")

@router.get("/training/status/{task_id}")
async def get_training_status(
    task_id: str,
    current_user = Depends(get_current_user)
):
    """
    è·å–æ¨¡å‹è®­ç»ƒçŠ¶æ€
    å®æ—¶è¿”å›è®­ç»ƒè¿›åº¦å’ŒæŒ‡æ ‡
    """
    try:
        training_status = await model_registry.get_training_status(task_id)
        
        if not training_status:
            raise HTTPException(status_code=404, detail="è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨")
        
        # æ£€æŸ¥ç”¨æˆ·æƒé™
        if training_status['user_id'] != current_user.id and not current_user.is_admin:
            raise HTTPException(status_code=403, detail="æ— æƒé™è®¿é—®è¯¥è®­ç»ƒä»»åŠ¡")
        
        return {
            "task_id": task_id,
            "status": training_status['status'],  # 'running', 'completed', 'failed', 'paused'
            "progress": {
                "current_epoch": training_status.get('current_epoch', 0),
                "total_epochs": training_status.get('total_epochs', 0),
                "progress_percentage": training_status.get('progress_percentage', 0.0)
            },
            "metrics": {
                "training_loss": training_status.get('training_loss', []),
                "validation_loss": training_status.get('validation_loss', []),
                "quality_score": training_status.get('quality_score', 0.0)
            },
            "resource_usage": {
                "gpu_usage": training_status.get('gpu_usage', 0.0),
                "memory_usage": training_status.get('memory_usage', 0.0),
                "training_time_elapsed": training_status.get('training_time_elapsed', 0)
            },
            "checkpoints": training_status.get('saved_checkpoints', []),
            "logs": training_status.get('recent_logs', [])[-10:]  # æœ€è¿‘10æ¡æ—¥å¿—
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}")

@router.post("/generate/synthetic-data")
async def generate_synthetic_data(
    request: DataGenerationRequest,
    current_user = Depends(get_current_user)
):
    """
    ç”Ÿæˆåˆæˆæ•°æ®
    æ”¯æŒç­–ç•¥ç‰¹å®šçš„æ•°æ®å¢å¼º
    """
    try:
        # éªŒè¯æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ
        model_status = await model_registry.get_model_status(request.model_name)
        if model_status['status'] != 'trained':
            raise HTTPException(
                status_code=400,
                detail=f"æ¨¡å‹{request.model_name}å°šæœªè®­ç»ƒå®Œæˆ"
            )
        
        # æ ¹æ®åœºæ™¯ç±»å‹ç”Ÿæˆæ•°æ®
        if request.strategy_context:
            # ç­–ç•¥ç‰¹å®šçš„æ•°æ®å¢å¼º
            base_data = await synthetic_data_manager.get_strategy_base_data(
                request.strategy_context
            )
            
            generation_results = await synthetic_data_manager.generate_augmented_training_data(
                strategy_type=request.strategy_context,
                base_data=base_data,
                augmentation_ratio=request.augmentation_ratio
            )
        else:
            # é€šç”¨åœºæ™¯ç”Ÿæˆ
            model = model_registry.get_model(request.model_name)
            generation_results = await model.generate_trading_scenarios(
                scenario_type=request.scenario_type,
                n_scenarios=request.n_samples
            )
        
        # è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡
        quality_metrics = await metrics_calculator.calculate_generation_quality(
            synthetic_data=generation_results['scenarios'],
            model_name=request.model_name
        )
        
        # ä¿å­˜ç”Ÿæˆç»“æœ
        generation_id = await synthetic_data_manager.save_generation_results(
            user_id=current_user.id,
            model_name=request.model_name,
            request_params=request.dict(),
            generated_data=generation_results,
            quality_metrics=quality_metrics
        )
        
        return {
            "status": "success",
            "generation_id": generation_id,
            "data_summary": {
                "n_samples": len(generation_results.get('scenarios', [])),
                "seq_length": request.seq_length,
                "scenario_type": request.scenario_type,
                "file_size_mb": generation_results.get('file_size_mb', 0)
            },
            "quality_metrics": quality_metrics,
            "download_endpoint": f"/ctbench/download/{generation_id}",
            "usage_recommendations": generation_results.get('usage_recommendations', [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")

@router.post("/stress-test/scenarios")
async def generate_stress_test_scenarios(
    request: StressTestRequest,
    current_user = Depends(get_current_user)
):
    """
    ç”Ÿæˆå‹åŠ›æµ‹è¯•åœºæ™¯
    ç”¨äºæŠ•èµ„ç»„åˆé£é™©è¯„ä¼°
    """
    try:
        # éªŒè¯æŠ•èµ„ç»„åˆé…ç½®
        portfolio_weights = request.portfolio_config
        if abs(sum(portfolio_weights.values()) - 1.0) > 0.01:
            raise HTTPException(
                status_code=400,
                detail="æŠ•èµ„ç»„åˆæƒé‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0"
            )
        
        # ç”Ÿæˆå‹åŠ›æµ‹è¯•åœºæ™¯
        stress_test_results = await synthetic_data_manager.generate_stress_test_scenarios(
            current_portfolio=portfolio_weights,
            test_types=request.test_types
        )
        
        # è®¡ç®—é£é™©æŒ‡æ ‡
        risk_analysis = {}
        for confidence_level in request.confidence_levels:
            risk_metrics = await metrics_calculator.calculate_portfolio_risk_metrics(
                portfolio=portfolio_weights,
                scenarios=stress_test_results['scenarios'],
                confidence_level=confidence_level
            )
            risk_analysis[f"confidence_{int(confidence_level*100)}"] = risk_metrics
        
        # ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š
        test_report = {
            "test_id": f"stress_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "portfolio_config": portfolio_weights,
            "test_scenarios": {
                test_type: {
                    "n_scenarios": len(scenarios['scenario_data']['scenarios']),
                    "worst_case_loss": scenarios['risk_metrics']['max_loss'],
                    "expected_loss": scenarios['risk_metrics']['expected_loss'],
                    "recovery_time_days": scenarios.get('recovery_time_days', 'unknown')
                }
                for test_type, scenarios in stress_test_results['scenarios'].items()
            },
            "risk_analysis": risk_analysis,
            "overall_recommendations": stress_test_results['recommendations'],
            "generated_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜å‹åŠ›æµ‹è¯•ç»“æœ
        await synthetic_data_manager.save_stress_test_report(
            user_id=current_user.id,
            report=test_report
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é£é™©è­¦æŠ¥
        critical_alerts = []
        for test_type, scenarios in stress_test_results['scenarios'].items():
            max_loss = scenarios['risk_metrics']['max_loss']
            if max_loss > 0.2:  # è¶…è¿‡20%æŸå¤±
                critical_alerts.append({
                    "test_type": test_type,
                    "max_potential_loss": max_loss,
                    "severity": "critical" if max_loss > 0.5 else "high"
                })
        
        return {
            "status": "completed",
            "test_report": test_report,
            "critical_alerts": critical_alerts,
            "visualization_data": {
                "risk_distribution_charts": stress_test_results.get('charts', {}),
                "scenario_timelines": stress_test_results.get('timelines', {})
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å‹åŠ›æµ‹è¯•å¤±è´¥: {str(e)}")

@router.get("/models/available")
async def list_available_models(current_user = Depends(get_current_user)):
    """
    è·å–å¯ç”¨çš„TSGæ¨¡å‹åˆ—è¡¨
    """
    try:
        models = await model_registry.list_available_models()
        
        model_info = []
        for model_name, model_data in models.items():
            # è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            model_status = await model_registry.get_model_status(model_name)
            
            model_info.append({
                "name": model_name,
                "display_name": model_data['display_name'],
                "type": model_data['model_type'],  # 'VAE', 'GAN', 'Diffusion', 'Flow'
                "description": model_data['description'],
                "status": model_status['status'],
                "training_progress": model_status.get('training_progress', 0),
                "quality_score": model_status.get('quality_score', 0.0),
                "last_trained": model_status.get('last_trained'),
                "supported_scenarios": model_data['supported_scenarios'],
                "computational_requirements": {
                    "gpu_required": model_data['gpu_required'],
                    "min_memory_gb": model_data['min_memory_gb'],
                    "estimated_training_time_hours": model_data['estimated_training_time_hours']
                }
            })
        
        return {
            "available_models": model_info,
            "total_count": len(model_info),
            "trained_count": len([m for m in model_info if m['status'] == 'trained']),
            "training_count": len([m for m in model_info if m['status'] == 'training'])
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/benchmark/performance")
async def get_benchmark_performance(
    model_names: Optional[List[str]] = None,
    current_user = Depends(get_current_user)
):
    """
    è·å–æ¨¡å‹åŸºå‡†æµ‹è¯•æ€§èƒ½å¯¹æ¯”
    """
    try:
        if not model_names:
            model_names = list(model_registry.available_models.keys())
        
        benchmark_results = {}
        
        for model_name in model_names:
            model_status = await model_registry.get_model_status(model_name)
            
            if model_status['status'] == 'trained':
                # è·å–åŸºå‡†æµ‹è¯•ç»“æœ
                performance_metrics = await metrics_calculator.get_model_benchmark_metrics(model_name)
                
                benchmark_results[model_name] = {
                    "generation_quality": performance_metrics['quality_metrics'],
                    "computational_performance": {
                        "training_time_minutes": performance_metrics['training_time_minutes'],
                        "inference_time_ms": performance_metrics['inference_time_ms'],
                        "memory_usage_gb": performance_metrics['peak_memory_usage_gb'],
                        "samples_per_second": performance_metrics['generation_throughput']
                    },
                    "trading_utility": performance_metrics['trading_utility_metrics'],
                    "overall_ranking": performance_metrics['overall_ranking']
                }
        
        # ç”Ÿæˆæ’åå¯¹æ¯”
        ranking_comparison = _generate_model_ranking_comparison(benchmark_results)
        
        return {
            "benchmark_results": benchmark_results,
            "performance_ranking": ranking_comparison,
            "evaluation_criteria": {
                "data_quality_weight": 0.4,
                "computational_efficiency_weight": 0.3,
                "trading_utility_weight": 0.3
            },
            "last_updated": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–åŸºå‡†æ€§èƒ½å¤±è´¥: {str(e)}")

# åå°ä»»åŠ¡å‡½æ•°
async def _execute_model_training(
    task_id: str,
    model_name: str,
    training_data: np.ndarray,
    hyperparameters: Dict[str, Any],
    use_gpu: bool,
    distributed: bool,
    user_id: str
):
    """
    æ‰§è¡Œæ¨¡å‹è®­ç»ƒçš„åå°ä»»åŠ¡
    """
    try:
        # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
        await model_registry.initialize_training_task(
            task_id=task_id,
            model_name=model_name,
            user_id=user_id,
            config={
                'use_gpu': use_gpu,
                'distributed': distributed,
                'hyperparameters': hyperparameters
            }
        )
        
        # è·å–æ¨¡å‹å®ä¾‹
        model = model_registry.create_model_instance(model_name, hyperparameters)
        
        # å‡†å¤‡è®­ç»ƒé…ç½®
        training_config = {
            'epochs': hyperparameters.get('epochs', 100),
            'batch_size': hyperparameters.get('batch_size', 32),
            'learning_rate': hyperparameters.get('learning_rate', 0.001),
            'validation_split': 0.2,
            'early_stopping_patience': 10,
            'save_checkpoints': True,
            'checkpoint_frequency': 10
        }
        
        # åˆ›å»ºè®­ç»ƒå›è°ƒ
        callbacks = [
            ModelCheckpointCallback(task_id),
            ProgressTrackingCallback(task_id),
            EarlyStoppingCallback(patience=training_config['early_stopping_patience']),
            MetricsLoggingCallback(task_id)
        ]
        
        # æ‰§è¡Œè®­ç»ƒ
        training_history = await model.train(
            train_data=training_data,
            config=training_config,
            callbacks=callbacks
        )
        
        # è®­ç»ƒå®Œæˆåçš„è´¨é‡è¯„ä¼°
        quality_assessment = await metrics_calculator.assess_trained_model_quality(
            model=model,
            validation_data=training_data[-1000:],  # ä½¿ç”¨æœ€å1000ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯
            model_name=model_name
        )
        
        # æ›´æ–°æ¨¡å‹æ³¨å†Œè¡¨
        await model_registry.complete_training_task(
            task_id=task_id,
            model=model,
            training_history=training_history,
            quality_metrics=quality_assessment
        )
        
        print(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ (ä»»åŠ¡ID: {task_id})")
        
    except Exception as e:
        # è®­ç»ƒå¤±è´¥å¤„ç†
        await model_registry.fail_training_task(
            task_id=task_id,
            error_message=str(e)
        )
        print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")

def _estimate_training_time(model_name: str, data_size: int, use_gpu: bool) -> int:
    """ä¼°ç®—è®­ç»ƒæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
    base_times = {
        'timevae': 30,
        'quantgan': 60,
        'diffusion_ts': 90,
        'fourier_flow': 45
    }
    
    base_time = base_times.get(model_name, 60)
    
    # æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´
    size_factor = max(1.0, data_size / 10000)
    
    # GPUåŠ é€Ÿ
    gpu_factor = 0.3 if use_gpu else 1.0
    
    return int(base_time * size_factor * gpu_factor)

def _generate_model_ranking_comparison(benchmark_results: Dict) -> List[Dict]:
    """ç”Ÿæˆæ¨¡å‹æ’åå¯¹æ¯”"""
    rankings = []
    
    for model_name, results in benchmark_results.items():
        overall_score = (
            results['generation_quality']['overall_quality'] * 0.4 +
            (1.0 - results['computational_performance']['training_time_minutes'] / 1000) * 0.3 +
            results['trading_utility']['sharpe_ratio'] * 0.3
        )
        
        rankings.append({
            'model_name': model_name,
            'overall_score': overall_score,
            'quality_rank': 0,  # å°†åœ¨æ’åºåå¡«å……
            'speed_rank': 0,
            'utility_rank': 0
        })
    
    # æ’åºå¹¶åˆ†é…æ’å
    rankings.sort(key=lambda x: x['overall_score'], reverse=True)
    for i, ranking in enumerate(rankings):
        ranking['overall_rank'] = i + 1
    
    return rankings
```

## 2. æ ¸å¿ƒä¸šåŠ¡é€»è¾‘é›†æˆ

```python
# backend/src/ctbench/data_generation/synthetic_data_manager.py
import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import torch

from ..models.model_registry import ModelRegistry
from ..evaluation.metrics import MetricsCalculator
from ...database.mongodb import get_mongo_client
from ...database.redis import get_redis_client
from ...services.exchange_connectors.okx_connector import OKXConnector
from ...core.enhanced_risk_manager import EnhancedRiskManager

class SyntheticDataManager:
    """
    åˆæˆæ•°æ®ç®¡ç†å™¨
    å°†CTBenchæ—¶é—´åºåˆ—ç”Ÿæˆèƒ½åŠ›é›†æˆåˆ°äº¤æ˜“ç³»ç»Ÿ
    """
    
    def __init__(self):
        self.model_registry = ModelRegistry()
        self.metrics_calculator = MetricsCalculator()
        self.mongo_client = None
        self.redis_client = None
        self.okx_connector = None
        self.risk_manager = None
        
        # æ•°æ®è´¨é‡æ§åˆ¶å‚æ•°
        self.quality_thresholds = {
            'min_overall_quality': 0.7,
            'min_price_similarity': 0.6,
            'min_volatility_clustering': 0.65,
            'min_correlation_preservation': 0.6
        }
        
        # ç¼“å­˜é…ç½®
        self.cache_ttl = 3600  # 1å°æ—¶
        self.max_cache_size_mb = 1000  # 1GB
        
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥å’ŒæœåŠ¡"""
        self.mongo_client = await get_mongo_client()
        self.redis_client = await get_redis_client()
        self.okx_connector = OKXConnector()
        self.risk_manager = EnhancedRiskManager()
        await self.model_registry.initialize()
        
    async def generate_augmented_training_data(self,
                                             strategy_type: str,
                                             base_data: np.ndarray,
                                             augmentation_ratio: float = 0.3,
                                             quality_override: bool = False) -> Dict:
        """
        ä¸ºç­–ç•¥è®­ç»ƒç”Ÿæˆå¢å¼ºæ•°æ®
        
        å‚æ•°:
            strategy_type: ç­–ç•¥ç±»å‹ ('grid', 'dca', 'momentum', 'mean_reversion', 'arbitrage')
            base_data: åŸºç¡€å†å²æ•°æ® (n_samples, n_assets, seq_length, n_features)
            augmentation_ratio: å¢å¼ºæ¯”ä¾‹ï¼Œåˆæˆæ•°æ®å æ€»æ•°æ®çš„æ¯”ä¾‹
            quality_override: æ˜¯å¦å¿½ç•¥è´¨é‡æ£€æŸ¥å¼ºåˆ¶ç”Ÿæˆ
            
        è¿”å›:
            Dict: {
                'status': 'success'|'low_quality'|'failed',
                'augmented_data': å¢å¼ºåçš„æ•°æ®é›†,
                'synthetic_data': åˆæˆæ•°æ®éƒ¨åˆ†,
                'quality_metrics': è´¨é‡è¯„ä¼°æŒ‡æ ‡,
                'model_used': ä½¿ç”¨çš„TSGæ¨¡å‹,
                'usage_recommendations': ä½¿ç”¨å»ºè®®
            }
        """
        try:
            print(f"ğŸ”¬ å¼€å§‹ä¸º {strategy_type} ç­–ç•¥ç”Ÿæˆå¢å¼ºæ•°æ®...")
            
            # 1. æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            validated_data = await self._validate_and_preprocess_data(base_data)
            if validated_data is None:
                return {'status': 'failed', 'error': 'åŸºç¡€æ•°æ®éªŒè¯å¤±è´¥'}
            
            # 2. æ ¹æ®ç­–ç•¥ç±»å‹é€‰æ‹©æœ€ä¼˜TSGæ¨¡å‹
            optimal_model = await self._select_optimal_model_for_strategy(
                strategy_type, validated_data
            )
            print(f"ğŸ“Š é€‰æ‹©æ¨¡å‹: {optimal_model}")
            
            # 3. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            model_status = await self.model_registry.get_model_status(optimal_model)
            if model_status['status'] != 'trained':
                # å°è¯•è‡ªåŠ¨è®­ç»ƒæ¨¡å‹
                print(f"ğŸ‹ï¸ æ¨¡å‹ {optimal_model} å°šæœªè®­ç»ƒï¼Œå¼€å§‹è‡ªåŠ¨è®­ç»ƒ...")
                await self._auto_train_model(optimal_model, validated_data)
            
            # 4. ç”Ÿæˆåˆæˆæ•°æ®
            n_synthetic_samples = int(len(validated_data) * augmentation_ratio)
            
            generation_config = self._get_strategy_specific_generation_config(
                strategy_type, validated_data
            )
            
            model = self.model_registry.get_model(optimal_model)
            synthetic_data = await model.generate(
                n_samples=n_synthetic_samples,
                conditioning_data=validated_data[-200:],  # ä½¿ç”¨æœ€è¿‘200ä¸ªæ ·æœ¬ä½œä¸ºæ¡ä»¶
                **generation_config
            )
            
            print(f"ğŸ“¡ ç”Ÿæˆäº† {len(synthetic_data)} ä¸ªåˆæˆæ ·æœ¬")
            
            # 5. æ•°æ®è´¨é‡è¯„ä¼°
            quality_metrics = await self.metrics_calculator.calculate_generation_quality(
                synthetic_data=synthetic_data,
                real_data=validated_data,
                strategy_context=strategy_type
            )
            
            print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†: {quality_metrics['overall_quality']:.3f}")
            
            # 6. è´¨é‡æ£€æŸ¥
            if not quality_override and not self._passes_quality_check(quality_metrics):
                return {
                    'status': 'low_quality',
                    'quality_metrics': quality_metrics,
                    'recommendations': self._get_quality_improvement_suggestions(quality_metrics),
                    'error': f"æ•°æ®è´¨é‡ä¸è¾¾æ ‡ (å¾—åˆ†: {quality_metrics['overall_quality']:.3f})"
                }
            
            # 7. æ•°æ®æ··åˆå’Œåå¤„ç†
            augmented_data = await self._blend_and_postprocess_data(
                real_data=validated_data,
                synthetic_data=synthetic_data,
                augmentation_ratio=augmentation_ratio,
                strategy_type=strategy_type
            )
            
            # 8. ç¼“å­˜ç»“æœ
            cache_key = f"augmented_data:{strategy_type}:{hash(str(base_data.tobytes()))}"
            await self._cache_generation_results(
                cache_key, {
                    'augmented_data': augmented_data,
                    'synthetic_data': synthetic_data,
                    'quality_metrics': quality_metrics,
                    'model_used': optimal_model,
                    'generation_timestamp': datetime.now().isoformat()
                }
            )
            
            # 9. ç”Ÿæˆä½¿ç”¨å»ºè®®
            usage_recommendations = self._generate_usage_recommendations(
                quality_metrics, strategy_type, optimal_model
            )
            
            return {
                'status': 'success',
                'augmented_data': augmented_data,
                'synthetic_data': synthetic_data,
                'original_size': len(validated_data),
                'synthetic_size': len(synthetic_data),
                'quality_metrics': quality_metrics,
                'model_used': optimal_model,
                'generation_config': generation_config,
                'usage_recommendations': usage_recommendations,
                'cache_key': cache_key
            }
            
        except Exception as e:
            print(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e),
                'error_type': type(e).__name__
            }
    
    async def generate_stress_test_scenarios(self,
                                           current_portfolio: Dict[str, float],
                                           test_types: List[str],
                                           n_scenarios: int = 100) -> Dict:
        """
        ç”Ÿæˆå‹åŠ›æµ‹è¯•åœºæ™¯æ•°æ®
        
        å‚æ•°:
            current_portfolio: å½“å‰æŠ•èµ„ç»„åˆ {symbol: weight}
            test_types: æµ‹è¯•ç±»å‹ ['black_swan', 'flash_crash', 'correlation_breakdown', 'liquidity_crisis']
            n_scenarios: æ¯ç§ç±»å‹ç”Ÿæˆçš„åœºæ™¯æ•°é‡
            
        è¿”å›:
            Dict: åŒ…å«å„ç±»å‹åŠ›æµ‹è¯•åœºæ™¯çš„æ•°æ®å’Œé£é™©åˆ†æ
        """
        try:
            print(f"ğŸ§ª ç”Ÿæˆ {len(test_types)} ç§å‹åŠ›æµ‹è¯•åœºæ™¯...")
            
            scenarios = {}
            portfolio_symbols = list(current_portfolio.keys())
            
            # è·å–å½“å‰å¸‚åœºæ•°æ®ä½œä¸ºåŸºå‡†
            base_market_data = await self._get_current_market_data(portfolio_symbols)
            
            for test_type in test_types:
                print(f"  ğŸ¯ ç”Ÿæˆ {test_type} åœºæ™¯...")
                
                # æ ¹æ®æµ‹è¯•ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
                optimal_model = self._select_model_for_stress_test(test_type)
                model = self.model_registry.get_model(optimal_model)
                
                # ç”Ÿæˆç‰¹å®šç±»å‹çš„åœºæ™¯æ•°æ®
                scenario_config = self._get_stress_test_config(test_type, current_portfolio)
                
                scenario_data = await model.generate_trading_scenarios(
                    scenario_type=test_type,
                    n_scenarios=n_scenarios,
                    base_data=base_market_data,
                    portfolio_context=current_portfolio,
                    **scenario_config
                )
                
                # è®¡ç®—è¯¥åœºæ™¯ä¸‹çš„æŠ•èµ„ç»„åˆé£é™©
                portfolio_risk_metrics = await self._calculate_portfolio_risk_under_scenario(
                    portfolio=current_portfolio,
                    scenario_data=scenario_data['scenarios'],
                    scenario_type=test_type
                )
                
                # å½±å“åˆ†æ
                impact_analysis = await self._analyze_scenario_impact(
                    current_portfolio, scenario_data['scenarios'], test_type
                )
                
                scenarios[test_type] = {
                    'scenario_data': scenario_data,
                    'risk_metrics': portfolio_risk_metrics,
                    'impact_analysis': impact_analysis,
                    'model_used': optimal_model,
                    'scenario_config': scenario_config
                }
            
            # ç”Ÿæˆç»¼åˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š
            comprehensive_report = await self._generate_comprehensive_stress_report(
                scenarios, current_portfolio
            )
            
            # ç”Ÿæˆé£é™©ç®¡ç†å»ºè®®
            risk_recommendations = await self._generate_risk_management_recommendations(
                scenarios, current_portfolio
            )
            
            # ä¿å­˜å‹åŠ›æµ‹è¯•ç»“æœ
            test_report_id = await self._save_stress_test_results({
                'portfolio': current_portfolio,
                'scenarios': scenarios,
                'report': comprehensive_report,
                'recommendations': risk_recommendations,
                'test_timestamp': datetime.now().isoformat()
            })
            
            return {
                'status': 'success',
                'test_report_id': test_report_id,
                'scenarios': scenarios,
                'comprehensive_report': comprehensive_report,
                'risk_recommendations': risk_recommendations,
                'critical_findings': comprehensive_report['critical_findings'],
                'next_steps': risk_recommendations['immediate_actions']
            }
            
        except Exception as e:
            print(f"âŒ å‹åŠ›æµ‹è¯•ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e)
            }
    
    async def enhance_factor_discovery(self,
                                     base_factors: pd.DataFrame,
                                     target_strategy: str,
                                     n_synthetic_factors: int = 50) -> Dict:
        """
        ä½¿ç”¨CTBenchå¢å¼ºAlphaå› å­å‘ç°
        
        å‚æ•°:
            base_factors: åŸºç¡€å› å­æ•°æ®
            target_strategy: ç›®æ ‡ç­–ç•¥ç±»å‹
            n_synthetic_factors: è¦ç”Ÿæˆçš„åˆæˆå› å­æ•°é‡
            
        è¿”å›:
            Dict: å¢å¼ºåçš„å› å­é›†åˆå’Œè¯„ä¼°ç»“æœ
        """
        try:
            print(f"ğŸ”¬ ä¸º {target_strategy} ç­–ç•¥å¢å¼ºå› å­å‘ç°...")
            
            # 1. åˆ†æç°æœ‰å› å­ç‰¹æ€§
            factor_characteristics = await self._analyze_factor_characteristics(base_factors)
            
            # 2. é€‰æ‹©é€‚åˆå› å­ç”Ÿæˆçš„æ¨¡å‹ (é€šå¸¸ä½¿ç”¨VAE)
            model = self.model_registry.get_model('timevae')
            
            # 3. åŸºäºç°æœ‰å› å­ç”Ÿæˆæ–°çš„åˆæˆå› å­
            synthetic_factors = await self._generate_synthetic_factors(
                model=model,
                base_factors=base_factors,
                n_factors=n_synthetic_factors,
                strategy_context=target_strategy
            )
            
            # 4. å› å­è´¨é‡è¯„ä¼°
            factor_quality = await self._evaluate_factor_quality(
                synthetic_factors, base_factors, target_strategy
            )
            
            # 5. å› å­é€‰æ‹©å’Œä¼˜åŒ–
            optimized_factors = await self._optimize_factor_combination(
                base_factors, synthetic_factors, factor_quality
            )
            
            # 6. ç”Ÿæˆå› å­ä½¿ç”¨å»ºè®®
            factor_recommendations = self._generate_factor_recommendations(
                optimized_factors, factor_quality, target_strategy
            )
            
            return {
                'status': 'success',
                'enhanced_factors': optimized_factors,
                'synthetic_factors': synthetic_factors,
                'factor_quality_metrics': factor_quality,
                'recommendations': factor_recommendations,
                'improvement_over_base': factor_quality['improvement_metrics']
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    # ç§æœ‰æ–¹æ³•å®ç°
    async def _validate_and_preprocess_data(self, data: np.ndarray) -> Optional[np.ndarray]:
        """éªŒè¯å’Œé¢„å¤„ç†è¾“å…¥æ•°æ®"""
        try:
            # åŸºæœ¬å½¢çŠ¶æ£€æŸ¥
            if len(data.shape) != 4:  # (n_samples, n_assets, seq_length, n_features)
                print(f"âŒ æ•°æ®å½¢çŠ¶é”™è¯¯: {data.shape}, æœŸæœ›: (n_samples, n_assets, seq_length, n_features)")
                return None
            
            n_samples, n_assets, seq_length, n_features = data.shape
            
            # æ£€æŸ¥æ•°æ®å¤§å°é™åˆ¶
            if n_samples < 100:
                print(f"âŒ æ ·æœ¬æ•°é‡è¿‡å°‘: {n_samples}, æœ€å°‘éœ€è¦100ä¸ªæ ·æœ¬")
                return None
            
            if seq_length < 24:  # è‡³å°‘24å°æ—¶æ•°æ®
                print(f"âŒ æ—¶é—´åºåˆ—é•¿åº¦è¿‡çŸ­: {seq_length}, æœ€å°‘éœ€è¦24ä¸ªæ—¶é—´ç‚¹")
                return None
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_ratio = np.isnan(data).sum() / data.size
            if missing_ratio > 0.05:  # è¶…è¿‡5%ç¼ºå¤±
                print(f"âš ï¸ æ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜: {missing_ratio:.2%}, è¿›è¡Œæ’å€¼å¤„ç†...")
                data = self._interpolate_missing_values(data)
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            data = self._detect_and_handle_outliers(data)
            
            # æ•°æ®æ ‡å‡†åŒ–
            data = self._normalize_data(data)
            
            print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {data.shape}")
            return data
            
        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return None
    
    async def _select_optimal_model_for_strategy(self, 
                                               strategy_type: str, 
                                               data: np.ndarray) -> str:
        """æ ¹æ®ç­–ç•¥ç±»å‹å’Œæ•°æ®ç‰¹æ€§é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        
        # åˆ†ææ•°æ®ç‰¹æ€§
        volatility = np.std(np.diff(data[:, :, :, 0], axis=2))  # ä»·æ ¼æ³¢åŠ¨ç‡
        trend_strength = self._calculate_trend_strength(data)
        mean_reversion_strength = self._calculate_mean_reversion_strength(data)
        
        # æ¨¡å‹é€‰æ‹©é€»è¾‘
        if strategy_type == 'grid':
            # ç½‘æ ¼ç­–ç•¥éœ€è¦æ¨ªç›˜æ•´ç†çš„å¸‚åœºï¼ŒVAEæ›´é€‚åˆ
            return 'timevae'
        elif strategy_type == 'momentum':
            # åŠ¨é‡ç­–ç•¥éœ€è¦è¶‹åŠ¿å»¶ç»­ï¼ŒFlowæ¨¡å‹æ›´é€‚åˆ
            return 'fourier_flow'
        elif strategy_type == 'mean_reversion':
            # å‡å€¼å›å½’ç­–ç•¥ï¼ŒGANæ¨¡å‹æ›´é€‚åˆç”Ÿæˆå›å½’æ¨¡å¼
            return 'quantgan'
        elif strategy_type == 'arbitrage':
            # å¥—åˆ©ç­–ç•¥éœ€è¦ç²¾ç¡®çš„ä»·æ ¼å…³ç³»ï¼ŒDiffusionæ¨¡å‹æ›´é€‚åˆ
            return 'diffusion_ts'
        elif volatility > 0.1:  # é«˜æ³¢åŠ¨å¸‚åœº
            return 'quantgan'  # GANæ›´é€‚åˆæç«¯æƒ…å†µ
        elif trend_strength > 0.7:  # å¼ºè¶‹åŠ¿å¸‚åœº
            return 'fourier_flow'
        else:
            return 'timevae'  # é»˜è®¤é€‰æ‹©
    
    def _get_strategy_specific_generation_config(self, 
                                               strategy_type: str, 
                                               data: np.ndarray) -> Dict:
        """è·å–ç­–ç•¥ç‰¹å®šçš„ç”Ÿæˆé…ç½®"""
        
        base_config = {
            'temperature': 1.0,
            'top_p': 0.9,
            'diversity_penalty': 0.1
        }
        
        if strategy_type == 'grid':
            # ç½‘æ ¼ç­–ç•¥éœ€è¦æ›´ç¨³å®šçš„ä»·æ ¼æ³¢åŠ¨
            base_config.update({
                'volatility_scaling': 0.8,
                'trend_suppression': 0.3,
                'range_bound_emphasis': 1.2
            })
        elif strategy_type == 'momentum':
            # åŠ¨é‡ç­–ç•¥éœ€è¦æ›´æ˜æ˜¾çš„è¶‹åŠ¿
            base_config.update({
                'trend_amplification': 1.3,
                'momentum_persistence': 1.1,
                'reversal_suppression': 0.7
            })
        elif strategy_type == 'mean_reversion':
            # å‡å€¼å›å½’éœ€è¦æ›´å¤šçš„ä»·æ ¼å›å½’æ¨¡å¼
            base_config.update({
                'mean_reversion_strength': 1.2,
                'oscillation_amplification': 1.1,
                'trend_suppression': 0.6
            })
        
        return base_config
    
    def _passes_quality_check(self, quality_metrics: Dict[str, float]) -> bool:
        """æ£€æŸ¥æ•°æ®è´¨é‡æ˜¯å¦è¾¾æ ‡"""
        for metric, threshold in self.quality_thresholds.items():
            if quality_metrics.get(metric.replace('min_', ''), 0) < threshold:
                return False
        return True
    
    async def _blend_and_postprocess_data(self,
                                        real_data: np.ndarray,
                                        synthetic_data: np.ndarray,
                                        augmentation_ratio: float,
                                        strategy_type: str) -> np.ndarray:
        """æ··åˆçœŸå®æ•°æ®å’Œåˆæˆæ•°æ®"""
        
        # è®¡ç®—æ··åˆæ¯”ä¾‹
        n_real = len(real_data)
        n_synthetic = int(n_real * augmentation_ratio / (1 - augmentation_ratio))
        n_synthetic = min(n_synthetic, len(synthetic_data))
        
        # éšæœºé€‰æ‹©åˆæˆæ•°æ®æ ·æœ¬
        synthetic_indices = np.random.choice(
            len(synthetic_data), 
            size=n_synthetic, 
            replace=False
        )
        selected_synthetic = synthetic_data[synthetic_indices]
        
        # æ•°æ®æ··åˆ
        blended_data = np.concatenate([real_data, selected_synthetic], axis=0)
        
        # éšæœºæ‰“ä¹±é¡ºåº
        shuffle_indices = np.random.permutation(len(blended_data))
        blended_data = blended_data[shuffle_indices]
        
        # ç­–ç•¥ç‰¹å®šçš„åå¤„ç†
        if strategy_type == 'grid':
            # ç¡®ä¿ä»·æ ¼åœ¨åˆç†èŒƒå›´å†…
            blended_data = self._ensure_price_bounds(blended_data)
        elif strategy_type == 'momentum':
            # å¢å¼ºè¶‹åŠ¿è¿ç»­æ€§
            blended_data = self._enhance_trend_continuity(blended_data)
        
        return blended_data
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u5206\u6790CTBench\u4e0e\u73b0\u6709\u7cfb\u7edf\u7684\u96c6\u6210\u70b9", "status": "completed", "id": "14"}, {"content": "\u8bbe\u8ba1CTBench\u96c6\u6210\u67b6\u6784", "status": "completed", "id": "15"}, {"content": "\u66f4\u65b0\u5168\u6808\u67b6\u6784\u6587\u6863", "status": "completed", "id": "16"}, {"content": "\u751f\u6210CTBench\u96c6\u6210\u4ee3\u7801", "status": "completed", "id": "17"}]